{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e86e85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RECURRENT NEURAL NETWORKS (RNNs)\n",
      "======================================================================\n",
      "\n",
      "THE PROBLEM: Standard neural networks expect FIXED-SIZE inputs.\n",
      "But sequences (text, audio, time series) have VARIABLE length and ORDER matters.\n",
      "\n",
      "\"dog bites man\" ≠ \"man bites dog\"  (same words, different meaning!)\n",
      "\n",
      "THE SOLUTION: Process one element at a time, maintaining a \"hidden state\"\n",
      "that acts as MEMORY of what we've seen so far.\n",
      "\n",
      "At each time step t:\n",
      "  h_t = tanh(W_xh @ x_t + W_hh @ h_prev + b)\n",
      "\n",
      "  - x_t: current input (e.g., word embedding)\n",
      "  - h_prev: memory from previous steps  \n",
      "  - h_t: updated memory after seeing this input\n",
      "\n",
      "KEY INSIGHT: Same weights (W_xh, W_hh) are used at EVERY time step.\n",
      "This means:\n",
      "  1. Network can handle ANY sequence length\n",
      "  2. What it learns at position 1 applies to position 100\n",
      "  3. Parameters don't grow with sequence length\n",
      "\n",
      "Let's see it in action:\n",
      "\n",
      "RNN initialized:\n",
      "  Input size: 4\n",
      "  Hidden size: 3\n",
      "  W_xh shape: (3, 4) (input → hidden)\n",
      "  W_hh shape: (3, 3) (hidden → hidden)\n",
      "  Total parameters: 24\n",
      "\n",
      "Input sequence shape: (3, 4)\n",
      "(3 time steps, 4 features per step)\n",
      "\n",
      "Processing sequence of length 3\n",
      "==================================================\n",
      "\n",
      "Time step 0:\n",
      "\n",
      "  --- RNN Step ---\n",
      "  Input x_t shape: (4, 1)\n",
      "  Previous hidden h_prev shape: (3, 1)\n",
      "  W_xh @ x_t: shape (3, 1)\n",
      "  W_hh @ h_prev: shape (3, 1)\n",
      "  New hidden state h_t: shape (3, 1)\n",
      "  Hidden state range: [-0.148, 0.100]\n",
      "\n",
      "Time step 1:\n",
      "\n",
      "  --- RNN Step ---\n",
      "  Input x_t shape: (4, 1)\n",
      "  Previous hidden h_prev shape: (3, 1)\n",
      "  W_xh @ x_t: shape (3, 1)\n",
      "  W_hh @ h_prev: shape (3, 1)\n",
      "  New hidden state h_t: shape (3, 1)\n",
      "  Hidden state range: [-0.492, 0.463]\n",
      "\n",
      "Time step 2:\n",
      "\n",
      "  --- RNN Step ---\n",
      "  Input x_t shape: (4, 1)\n",
      "  Previous hidden h_prev shape: (3, 1)\n",
      "  W_xh @ x_t: shape (3, 1)\n",
      "  W_hh @ h_prev: shape (3, 1)\n",
      "  New hidden state h_t: shape (3, 1)\n",
      "  Hidden state range: [-0.189, -0.097]\n",
      "\n",
      "==================================================\n",
      "Sequence processing complete!\n",
      "Final hidden state encodes the entire sequence.\n",
      "\n",
      "Number of hidden states: 3\n",
      "Final hidden state (summary of entire sequence):\n",
      "[-0.18901923 -0.0972174  -0.11769798]\n",
      "\n",
      "KEY TAKEAWAY:\n",
      "The final hidden state h_3 is a fixed-size vector that \"summarizes\" the \n",
      "entire input sequence. This can be used for:\n",
      "  - Sentiment analysis: sequence → h_final → positive/negative\n",
      "  - Language modeling: at each step, h_t → predict next word\n",
      "  - Seq2Seq: encode input → h_final → decode to output\n",
      "\n",
      "LIMITATION (covered next): Gradients vanish over long sequences.\n",
      "After ~10-20 steps, early inputs barely affect learning.\n",
      "This is why LSTM/GRU were invented.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleRNN:\n",
    "    \"\"\"\n",
    "    A simple Recurrent Neural Network cell.\n",
    "    \n",
    "    The RNN processes sequences one element at a time, maintaining a \"hidden state\"\n",
    "    that serves as memory of what it has seen so far.\n",
    "    \n",
    "    Think of it like reading a book: as you read each word, you update your\n",
    "    understanding of the story. The hidden state is your \"mental model\" of\n",
    "    the story so far.\n",
    "    \n",
    "    Key insight: The SAME weights are used at every time step. This means:\n",
    "    1. The network can handle sequences of any length\n",
    "    2. What it learns about processing position 1 applies to position 100\n",
    "    3. The number of parameters doesn't grow with sequence length\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Initialize the RNN.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            Dimension of each input element.\n",
    "            For text: This would be the word embedding dimension (e.g., 256)\n",
    "            For time series: This might be 1 (single value) or more (multiple sensors)\n",
    "        \n",
    "        hidden_size : int\n",
    "            Dimension of the hidden state (the \"memory\").\n",
    "            Larger = more capacity to remember, but more computation.\n",
    "            Typical values: 128, 256, 512\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # =====================================================================\n",
    "        # Weight Initialization\n",
    "        # =====================================================================\n",
    "        # We use Xavier initialization (explained in Part 0A: it keeps \n",
    "        # gradients stable by scaling weights based on layer sizes)\n",
    "        \n",
    "        # Weights for transforming the input\n",
    "        # Shape: (hidden_size, input_size)\n",
    "        # These weights determine how the current input affects the hidden state\n",
    "        scale_xh = np.sqrt(1.0 / input_size)\n",
    "        self.W_xh = np.random.randn(hidden_size, input_size) * scale_xh\n",
    "        \n",
    "        # Weights for transforming the previous hidden state\n",
    "        # Shape: (hidden_size, hidden_size)  \n",
    "        # These weights determine how the previous memory affects the new memory\n",
    "        scale_hh = np.sqrt(1.0 / hidden_size)\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * scale_hh\n",
    "        \n",
    "        # Bias for the hidden state\n",
    "        # Shape: (hidden_size, 1)\n",
    "        self.b_h = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        print(f\"RNN initialized:\")\n",
    "        print(f\"  Input size: {input_size}\")\n",
    "        print(f\"  Hidden size: {hidden_size}\")\n",
    "        print(f\"  W_xh shape: {self.W_xh.shape} (input → hidden)\")\n",
    "        print(f\"  W_hh shape: {self.W_hh.shape} (hidden → hidden)\")\n",
    "        print(f\"  Total parameters: {self.W_xh.size + self.W_hh.size + self.b_h.size}\")\n",
    "    \n",
    "    def step(self, x_t, h_prev, verbose=False):\n",
    "        \"\"\"\n",
    "        Process ONE time step of the sequence.\n",
    "        \n",
    "        This is the core RNN computation:\n",
    "        h_t = tanh(W_xh @ x_t + W_hh @ h_prev + b_h)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x_t : numpy array, shape (input_size, 1)\n",
    "            The input at the current time step.\n",
    "            For text: The word embedding of the current word.\n",
    "        \n",
    "        h_prev : numpy array, shape (hidden_size, 1)\n",
    "            The hidden state from the previous time step.\n",
    "            This encodes everything the network \"remembers\" so far.\n",
    "        \n",
    "        verbose : bool\n",
    "            If True, print intermediate computations.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        h_t : numpy array, shape (hidden_size, 1)\n",
    "            The new hidden state after processing this input.\n",
    "        \"\"\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n  --- RNN Step ---\")\n",
    "            print(f\"  Input x_t shape: {x_t.shape}\")\n",
    "            print(f\"  Previous hidden h_prev shape: {h_prev.shape}\")\n",
    "        \n",
    "        # Step 1: Transform the current input\n",
    "        # This extracts features from the current input\n",
    "        input_contribution = np.dot(self.W_xh, x_t)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  W_xh @ x_t: shape {input_contribution.shape}\")\n",
    "        \n",
    "        # Step 2: Transform the previous hidden state  \n",
    "        # This carries forward the memory from previous steps\n",
    "        memory_contribution = np.dot(self.W_hh, h_prev)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  W_hh @ h_prev: shape {memory_contribution.shape}\")\n",
    "        \n",
    "        # Step 3: Combine and add bias\n",
    "        # The new hidden state is influenced by BOTH current input AND memory\n",
    "        combined = input_contribution + memory_contribution + self.b_h\n",
    "        \n",
    "        # Step 4: Apply tanh activation\n",
    "        # tanh squashes values to [-1, 1], which helps with:\n",
    "        # - Keeping hidden state bounded (doesn't explode)\n",
    "        # - Introducing non-linearity (can learn complex patterns)\n",
    "        h_t = np.tanh(combined)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  New hidden state h_t: shape {h_t.shape}\")\n",
    "            print(f\"  Hidden state range: [{h_t.min():.3f}, {h_t.max():.3f}]\")\n",
    "        \n",
    "        return h_t\n",
    "    \n",
    "    def forward(self, X, verbose=False):\n",
    "        \"\"\"\n",
    "        Process an entire sequence.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, shape (sequence_length, input_size)\n",
    "            The full input sequence.\n",
    "            Each row is one time step.\n",
    "        \n",
    "        verbose : bool\n",
    "            If True, print step-by-step details.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        hidden_states : list of numpy arrays\n",
    "            The hidden state after each time step.\n",
    "            hidden_states[-1] is the final \"summary\" of the entire sequence.\n",
    "        \"\"\"\n",
    "        sequence_length = X.shape[0]\n",
    "        \n",
    "        # Initialize hidden state to zeros\n",
    "        # This represents \"no memory yet\" at the start\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        hidden_states = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nProcessing sequence of length {sequence_length}\")\n",
    "            print(\"=\" * 50)\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            # Get input at time t\n",
    "            # Reshape to (input_size, 1) for matrix multiplication\n",
    "            x_t = X[t:t+1, :].T\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nTime step {t}:\")\n",
    "            \n",
    "            # Process this time step\n",
    "            h = self.step(x_t, h, verbose=verbose)\n",
    "            \n",
    "            # Store the hidden state\n",
    "            hidden_states.append(h.copy())\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\" * 50)\n",
    "            print(f\"Sequence processing complete!\")\n",
    "            print(f\"Final hidden state encodes the entire sequence.\")\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"RECURRENT NEURAL NETWORKS (RNNs)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "THE PROBLEM: Standard neural networks expect FIXED-SIZE inputs.\n",
    "But sequences (text, audio, time series) have VARIABLE length and ORDER matters.\n",
    "\n",
    "\"dog bites man\" ≠ \"man bites dog\"  (same words, different meaning!)\n",
    "\n",
    "THE SOLUTION: Process one element at a time, maintaining a \"hidden state\"\n",
    "that acts as MEMORY of what we've seen so far.\n",
    "\n",
    "At each time step t:\n",
    "  h_t = tanh(W_xh @ x_t + W_hh @ h_prev + b)\n",
    "  \n",
    "  - x_t: current input (e.g., word embedding)\n",
    "  - h_prev: memory from previous steps  \n",
    "  - h_t: updated memory after seeing this input\n",
    "\n",
    "KEY INSIGHT: Same weights (W_xh, W_hh) are used at EVERY time step.\n",
    "This means:\n",
    "  1. Network can handle ANY sequence length\n",
    "  2. What it learns at position 1 applies to position 100\n",
    "  3. Parameters don't grow with sequence length\n",
    "\n",
    "Let's see it in action:\n",
    "\"\"\")\n",
    "\n",
    "# Create a simple RNN\n",
    "rnn = SimpleRNN(input_size=4, hidden_size=3)\n",
    "\n",
    "# Create a dummy sequence (3 time steps, 4 features each)\n",
    "# In practice, these would be word embeddings or sensor readings\n",
    "sequence = np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],  # Time step 0\n",
    "    [0.5, 0.6, 0.7, 0.8],  # Time step 1\n",
    "    [0.2, 0.1, 0.4, 0.3],  # Time step 2\n",
    "])\n",
    "\n",
    "print(f\"\\nInput sequence shape: {sequence.shape}\")\n",
    "print(f\"(3 time steps, 4 features per step)\")\n",
    "\n",
    "# Process the sequence\n",
    "hidden_states = rnn.forward(sequence, verbose=True)\n",
    "\n",
    "print(f\"\\nNumber of hidden states: {len(hidden_states)}\")\n",
    "print(f\"Final hidden state (summary of entire sequence):\")\n",
    "print(hidden_states[-1].flatten())\n",
    "\n",
    "print(\"\"\"\n",
    "KEY TAKEAWAY:\n",
    "The final hidden state h_3 is a fixed-size vector that \"summarizes\" the \n",
    "entire input sequence. This can be used for:\n",
    "  - Sentiment analysis: sequence → h_final → positive/negative\n",
    "  - Language modeling: at each step, h_t → predict next word\n",
    "  - Seq2Seq: encode input → h_final → decode to output\n",
    "\n",
    "LIMITATION (covered next): Gradients vanish over long sequences.\n",
    "After ~10-20 steps, early inputs barely affect learning.\n",
    "This is why LSTM/GRU were invented.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f4f868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "THE VANISHING GRADIENT PROBLEM IN RNNs\n",
      "======================================================================\n",
      "\n",
      "WHY THIS MATTERS:\n",
      "\n",
      "Consider: \"The cat, which had been sleeping on the warm windowsill \n",
      "all afternoon while the rain poured outside, finally woke up.\"\n",
      "\n",
      "The verb \"woke\" must agree with \"cat\" (singular) — but they're 18 words apart!\n",
      "For the RNN to learn this, the gradient from \"woke\" must flow back to \"cat\".\n",
      "\n",
      "THE PROBLEM:\n",
      "\n",
      "During backpropagation, gradients flow backward through time.\n",
      "At each step, we multiply by:\n",
      "  - The derivative of tanh (max 1.0, typically ~0.5)\n",
      "  - The weight matrix W_hh (typically < 1.0 to avoid explosion)\n",
      "\n",
      "After N steps: gradient ≈ (0.5 × 0.9)^N = 0.45^N\n",
      "\n",
      "Let's see what happens:\n",
      "\n",
      "Gradient multiplier per time step: 0.45\n",
      "\n",
      "After   1 time steps: gradient magnitude = 4.50e-01\n",
      "After   5 time steps: gradient magnitude = 1.85e-02\n",
      "After  10 time steps: gradient magnitude = 3.41e-04\n",
      "After  20 time steps: gradient magnitude = 1.16e-07\n",
      "After  50 time steps: gradient magnitude = 4.58e-18\n",
      "After 100 time steps: gradient magnitude = 2.10e-35\n",
      "\n",
      "After 100 steps, gradient is essentially ZERO!\n",
      "Early words in a long sequence barely get any learning signal.\n",
      "\n",
      "This is catastrophic for language understanding. In our example sentence,\n",
      "the RNN can't connect \"cat\" to \"woke\" because the gradient vanishes \n",
      "over those 18 words.\n",
      "\n",
      "THE SOLUTION: LSTM (Long Short-Term Memory)\n",
      "\n",
      "LSTM introduces a \"cell state\" — a highway for information that uses \n",
      "ADDITION instead of multiplication. Gradients can flow through addition\n",
      "without vanishing. Plus, learnable \"gates\" control what to remember/forget.\n",
      "\n",
      "That's what we'll build next.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_rnn_vanishing_gradient():\n",
    "    \"\"\"\n",
    "    Show why gradients vanish in RNNs over long sequences.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"THE VANISHING GRADIENT PROBLEM IN RNNs\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\"\"\n",
    "WHY THIS MATTERS:\n",
    "\n",
    "Consider: \"The cat, which had been sleeping on the warm windowsill \n",
    "all afternoon while the rain poured outside, finally woke up.\"\n",
    "\n",
    "The verb \"woke\" must agree with \"cat\" (singular) — but they're 18 words apart!\n",
    "For the RNN to learn this, the gradient from \"woke\" must flow back to \"cat\".\n",
    "\n",
    "THE PROBLEM:\n",
    "\n",
    "During backpropagation, gradients flow backward through time.\n",
    "At each step, we multiply by:\n",
    "  - The derivative of tanh (max 1.0, typically ~0.5)\n",
    "  - The weight matrix W_hh (typically < 1.0 to avoid explosion)\n",
    "\n",
    "After N steps: gradient ≈ (0.5 × 0.9)^N = 0.45^N\n",
    "\n",
    "Let's see what happens:\n",
    "\"\"\")\n",
    "    \n",
    "    # tanh derivative: max value is 1.0 (at tanh(0) = 0)\n",
    "    # For typical values, it's much smaller\n",
    "    # d/dx tanh(x) = 1 - tanh(x)²\n",
    "    \n",
    "    # Simulate gradient flow through time\n",
    "    # Each time step multiplies gradient by tanh_derivative and W_hh\n",
    "    \n",
    "    # Assume typical tanh derivative of ~0.5 and well-initialized W_hh\n",
    "    tanh_deriv_typical = 0.5\n",
    "    w_hh_effect = 0.9  # Slightly less than 1\n",
    "    \n",
    "    gradient_multiplier_per_step = tanh_deriv_typical * w_hh_effect\n",
    "    \n",
    "    print(f\"Gradient multiplier per time step: {gradient_multiplier_per_step}\")\n",
    "    print()\n",
    "    \n",
    "    gradient = 1.0  # Start with gradient = 1 from the loss\n",
    "    \n",
    "    for t in [1, 5, 10, 20, 50, 100]:\n",
    "        gradient_at_t = gradient_multiplier_per_step ** t\n",
    "        print(f\"After {t:3d} time steps: gradient magnitude = {gradient_at_t:.2e}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"After 100 steps, gradient is essentially ZERO!\")\n",
    "    print(\"Early words in a long sequence barely get any learning signal.\")\n",
    "    print()\n",
    "    print(\"\"\"This is catastrophic for language understanding. In our example sentence,\n",
    "the RNN can't connect \"cat\" to \"woke\" because the gradient vanishes \n",
    "over those 18 words.\n",
    "\n",
    "THE SOLUTION: LSTM (Long Short-Term Memory)\n",
    "\n",
    "LSTM introduces a \"cell state\" — a highway for information that uses \n",
    "ADDITION instead of multiplication. Gradients can flow through addition\n",
    "without vanishing. Plus, learnable \"gates\" control what to remember/forget.\n",
    "\n",
    "That's what we'll build next.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "demonstrate_rnn_vanishing_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "234c8ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LSTM DEMONSTRATION\n",
      "======================================================================\n",
      "LSTM initialized:\n",
      "  Input size: 4\n",
      "  Hidden size: 3\n",
      "  Total parameters: 96\n",
      "  (Note: 4x more parameters than simple RNN due to 4 gates)\n",
      "\n",
      "Input sequence shape: (10, 4)\n",
      "(10 time steps, 4 features per step)\n",
      "\n",
      "Processing sequence of length 10\n",
      "============================================================\n",
      "\n",
      "Time step 0:\n",
      "  Forget gate (avg): 0.593 (0=forget all, 1=keep all)\n",
      "  Input gate (avg): 0.464 (0=ignore, 1=store)\n",
      "  Candidate values range: [-0.381, 0.621]\n",
      "  Cell state updated: range [-0.157, 0.389]\n",
      "  Output gate (avg): 0.447 (0=hide, 1=expose)\n",
      "  Hidden state range: [-0.057, 0.198]\n",
      "\n",
      "Time step 1:\n",
      "  Forget gate (avg): 0.523 (0=forget all, 1=keep all)\n",
      "  Input gate (avg): 0.517 (0=ignore, 1=store)\n",
      "  Candidate values range: [-0.453, 0.053]\n",
      "  Cell state updated: range [-0.071, -0.058]\n",
      "  Output gate (avg): 0.535 (0=hide, 1=expose)\n",
      "  Hidden state range: [-0.039, -0.031]\n",
      "\n",
      "Time step 2:\n",
      "  Forget gate (avg): 0.536 (0=forget all, 1=keep all)\n",
      "  Input gate (avg): 0.545 (0=ignore, 1=store)\n",
      "  Candidate values range: [-0.347, -0.028]\n",
      "  Cell state updated: range [-0.226, -0.044]\n",
      "  Output gate (avg): 0.545 (0=hide, 1=expose)\n",
      "  Hidden state range: [-0.123, -0.025]\n",
      "\n",
      "Time step 3:\n",
      "  Forget gate (avg): 0.453 (0=forget all, 1=keep all)\n",
      "  Input gate (avg): 0.510 (0=ignore, 1=store)\n",
      "  Candidate values range: [-0.218, 0.335]\n",
      "  Cell state updated: range [-0.154, 0.074]\n",
      "  Output gate (avg): 0.507 (0=hide, 1=expose)\n",
      "  Hidden state range: [-0.072, 0.040]\n",
      "\n",
      "Time step 4:\n",
      "  Forget gate (avg): 0.529 (0=forget all, 1=keep all)\n",
      "  Input gate (avg): 0.492 (0=ignore, 1=store)\n",
      "  Candidate values range: [-0.525, 0.430]\n",
      "  Cell state updated: range [-0.363, 0.254]\n",
      "  Output gate (avg): 0.498 (0=hide, 1=expose)\n",
      "  Hidden state range: [-0.167, 0.136]\n",
      "\n",
      "Time step 5:\n",
      "  Forget gate (avg): 0.538 (0=forget all, 1=keep all)\n",
      "  Input gate (avg): 0.537 (0=ignore, 1=store)\n",
      "  Candidate values range: [-0.210, -0.011]\n",
      "  Cell state updated: range [-0.303, 0.140]\n",
      "  Output gate (avg): 0.522 (0=hide, 1=expose)\n",
      "  Hidden state range: [-0.151, 0.077]\n",
      "\n",
      "Time step 6:\n",
      "  Forget gate (avg): 0.395 (0=forget all, 1=keep all)\n",
      "  Input gate (avg): 0.478 (0=ignore, 1=store)\n",
      "  Candidate values range: [-0.340, 0.562]\n",
      "  Cell state updated: range [-0.204, 0.235]\n",
      "  Output gate (avg): 0.532 (0=hide, 1=expose)\n",
      "  Hidden state range: [-0.094, 0.130]\n",
      "\n",
      "Time step 7:\n",
      "  Forget gate (avg): 0.549 (0=forget all, 1=keep all)\n",
      "  Input gate (avg): 0.452 (0=ignore, 1=store)\n",
      "  Candidate values range: [-0.693, 0.326]\n",
      "  Cell state updated: range [-0.505, 0.065]\n",
      "  Output gate (avg): 0.478 (0=hide, 1=expose)\n",
      "  Hidden state range: [-0.215, 0.033]\n",
      "\n",
      "Time step 8:\n",
      "  Forget gate (avg): 0.467 (0=forget all, 1=keep all)\n",
      "  Input gate (avg): 0.527 (0=ignore, 1=store)\n",
      "  Candidate values range: [-0.312, 0.244]\n",
      "  Cell state updated: range [-0.236, 0.159]\n",
      "  Output gate (avg): 0.523 (0=hide, 1=expose)\n",
      "  Hidden state range: [-0.115, 0.083]\n",
      "\n",
      "Time step 9:\n",
      "  Forget gate (avg): 0.389 (0=forget all, 1=keep all)\n",
      "  Input gate (avg): 0.549 (0=ignore, 1=store)\n",
      "  Candidate values range: [-0.100, 0.154]\n",
      "  Cell state updated: range [-0.130, 0.108]\n",
      "  Output gate (avg): 0.537 (0=hide, 1=expose)\n",
      "  Hidden state range: [-0.061, 0.059]\n",
      "\n",
      "======================================================================\n",
      "KEY INSIGHT: The cell state can carry information across many steps\n",
      "because it uses addition, not multiplication!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LSTM:\n",
    "    \"\"\"\n",
    "    Long Short-Term Memory network.\n",
    "    \n",
    "    LSTM solves the vanishing gradient problem of standard RNNs by introducing:\n",
    "    1. A separate \"cell state\" that carries information across time steps\n",
    "    2. Gates that control what information flows in and out\n",
    "    \n",
    "    The key insight: Instead of always mixing old and new information,\n",
    "    let the network LEARN when to remember and when to forget.\n",
    "    \n",
    "    Think of it like a notepad:\n",
    "    - Forget gate: Eraser (which notes to erase?)\n",
    "    - Input gate: Pen (which new notes to write?)\n",
    "    - Output gate: Highlighter (which notes are relevant right now?)\n",
    "    - Cell state: The notepad itself (persistent storage)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Initialize LSTM.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            Dimension of each input element (e.g., word embedding size)\n",
    "        hidden_size : int\n",
    "            Dimension of hidden state and cell state\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Combined input: [h_{t-1}, x_t] has size (hidden_size + input_size)\n",
    "        combined_size = hidden_size + input_size\n",
    "        \n",
    "        # Xavier initialization scale\n",
    "        scale = np.sqrt(1.0 / combined_size)\n",
    "        \n",
    "        # =====================================================================\n",
    "        # Forget Gate: Decides what to erase from cell state\n",
    "        # =====================================================================\n",
    "        # Output is between 0 (forget completely) and 1 (keep completely)\n",
    "        self.W_f = np.random.randn(hidden_size, combined_size) * scale\n",
    "        self.b_f = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # =====================================================================\n",
    "        # Input Gate: Decides what new information to store\n",
    "        # =====================================================================\n",
    "        self.W_i = np.random.randn(hidden_size, combined_size) * scale\n",
    "        self.b_i = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # =====================================================================\n",
    "        # Candidate Values: The new information that COULD be stored\n",
    "        # =====================================================================\n",
    "        self.W_c = np.random.randn(hidden_size, combined_size) * scale\n",
    "        self.b_c = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # =====================================================================\n",
    "        # Output Gate: Decides what to output from cell state\n",
    "        # =====================================================================\n",
    "        self.W_o = np.random.randn(hidden_size, combined_size) * scale\n",
    "        self.b_o = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        total_params = 4 * (self.W_f.size + self.b_f.size)\n",
    "        print(f\"LSTM initialized:\")\n",
    "        print(f\"  Input size: {input_size}\")\n",
    "        print(f\"  Hidden size: {hidden_size}\")\n",
    "        print(f\"  Total parameters: {total_params}\")\n",
    "        print(f\"  (Note: 4x more parameters than simple RNN due to 4 gates)\")\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation: squashes to (0, 1) for gating.\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def step(self, x_t, h_prev, c_prev, verbose=False):\n",
    "        \"\"\"\n",
    "        Process ONE time step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x_t : numpy array, shape (input_size, 1)\n",
    "            Current input\n",
    "        h_prev : numpy array, shape (hidden_size, 1)\n",
    "            Previous hidden state (what we output last time)\n",
    "        c_prev : numpy array, shape (hidden_size, 1)\n",
    "            Previous cell state (our persistent memory)\n",
    "        verbose : bool\n",
    "            Print detailed gate activations\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        h_t : numpy array, shape (hidden_size, 1)\n",
    "            New hidden state (output)\n",
    "        c_t : numpy array, shape (hidden_size, 1)\n",
    "            New cell state (updated memory)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Concatenate previous hidden state and current input\n",
    "        # This combined vector is what all gates look at\n",
    "        combined = np.vstack([h_prev, x_t])\n",
    "        \n",
    "        # =====================================================================\n",
    "        # Step 1: Forget Gate - What to erase from memory?\n",
    "        # =====================================================================\n",
    "        f_t = self.sigmoid(np.dot(self.W_f, combined) + self.b_f)\n",
    "        # f_t is between 0 and 1 for each cell state dimension\n",
    "        # 0 = completely forget, 1 = completely keep\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Forget gate (avg): {f_t.mean():.3f} (0=forget all, 1=keep all)\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # Step 2: Input Gate - What new info to store?\n",
    "        # =====================================================================\n",
    "        i_t = self.sigmoid(np.dot(self.W_i, combined) + self.b_i)\n",
    "        # i_t is between 0 and 1: how much to add\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Input gate (avg): {i_t.mean():.3f} (0=ignore, 1=store)\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # Step 3: Candidate Values - The new info that COULD be stored\n",
    "        # =====================================================================\n",
    "        c_tilde = np.tanh(np.dot(self.W_c, combined) + self.b_c)\n",
    "        # c_tilde is between -1 and 1: the candidate new values\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Candidate values range: [{c_tilde.min():.3f}, {c_tilde.max():.3f}]\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # Step 4: Update Cell State - The actual memory update\n",
    "        # =====================================================================\n",
    "        # This is the key equation! Mostly addition, not multiplication.\n",
    "        # c_t = (forget some old) + (add some new)\n",
    "        c_t = f_t * c_prev + i_t * c_tilde\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Cell state updated: range [{c_t.min():.3f}, {c_t.max():.3f}]\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # Step 5: Output Gate - What to output?\n",
    "        # =====================================================================\n",
    "        o_t = self.sigmoid(np.dot(self.W_o, combined) + self.b_o)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Output gate (avg): {o_t.mean():.3f} (0=hide, 1=expose)\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # Step 6: Compute Hidden State (Output)\n",
    "        # =====================================================================\n",
    "        # The hidden state is a filtered version of the cell state\n",
    "        h_t = o_t * np.tanh(c_t)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Hidden state range: [{h_t.min():.3f}, {h_t.max():.3f}]\")\n",
    "        \n",
    "        return h_t, c_t\n",
    "    \n",
    "    def forward(self, X, verbose=False):\n",
    "        \"\"\"\n",
    "        Process an entire sequence.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, shape (sequence_length, input_size)\n",
    "            The full input sequence\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        hidden_states : list\n",
    "            Hidden state after each time step\n",
    "        cell_states : list\n",
    "            Cell state after each time step\n",
    "        \"\"\"\n",
    "        sequence_length = X.shape[0]\n",
    "        \n",
    "        # Initialize both hidden state and cell state to zeros\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        c = np.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        hidden_states = []\n",
    "        cell_states = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nProcessing sequence of length {sequence_length}\")\n",
    "            print(\"=\" * 60)\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            x_t = X[t:t+1, :].T\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\nTime step {t}:\")\n",
    "            \n",
    "            h, c = self.step(x_t, h, c, verbose=verbose)\n",
    "            \n",
    "            hidden_states.append(h.copy())\n",
    "            cell_states.append(c.copy())\n",
    "        \n",
    "        return hidden_states, cell_states\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION: LSTM vs RNN on long sequences\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LSTM DEMONSTRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "lstm = LSTM(input_size=4, hidden_size=3)\n",
    "\n",
    "# Create a longer sequence\n",
    "sequence = np.random.randn(10, 4) * 0.5  # 10 time steps\n",
    "\n",
    "print(f\"\\nInput sequence shape: {sequence.shape}\")\n",
    "print(f\"(10 time steps, 4 features per step)\")\n",
    "\n",
    "hidden_states, cell_states = lstm.forward(sequence, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY INSIGHT: The cell state can carry information across many steps\")\n",
    "print(\"because it uses addition, not multiplication!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c52cd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ATTENTION MECHANISM DEMONSTRATION\n",
      "======================================================================\n",
      "\n",
      "Scenario: Translating to French, currently generating word related to 'cat'\n",
      "We expect attention to focus on position 1 ('cat')\n",
      "Computing Attention\n",
      "============================================================\n",
      "Encoder states: 5 positions, each of size 4\n",
      "Decoder state: size 4\n",
      "\n",
      "Step 1: Raw attention scores (dot products)\n",
      "  Position 0: score = 0.2550\n",
      "  Position 1: score = 0.8050\n",
      "  Position 2: score = 0.1950\n",
      "  Position 3: score = 0.1650\n",
      "  Position 4: score = 0.2150\n",
      "\n",
      "Step 2: Attention weights (softmax of scores)\n",
      "  Position 0: 0.1802 ███████\n",
      "  Position 1: 0.3123 ████████████\n",
      "  Position 2: 0.1697 ██████\n",
      "  Position 3: 0.1647 ██████\n",
      "  Position 4: 0.1731 ██████\n",
      "  Sum of weights: 1.0000 (should be 1.0)\n",
      "\n",
      "Step 3: Context vector (weighted sum)\n",
      "  context = Σ(attention_weight[j] × encoder_state[j])\n",
      "  Context shape: (4,)\n",
      "  Context values: [0.25906809 0.34985006 0.23609905 0.22047984]\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION\n",
      "======================================================================\n",
      "The model paid most attention to position 1 ('cat')\n",
      "This context vector can now be used to help generate the French word\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_attention(encoder_states, decoder_state, verbose=False):\n",
    "    \"\"\"\n",
    "    Compute attention over encoder states given current decoder state.\n",
    "    \n",
    "    This implements \"additive attention\" (Bahdanau attention):\n",
    "    - Score each encoder state based on its relevance to the decoder state\n",
    "    - Normalize scores with softmax to get attention weights\n",
    "    - Compute weighted sum of encoder states\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder_states : numpy array, shape (seq_len, hidden_size)\n",
    "        Hidden states from the encoder, one per input position.\n",
    "        Each row represents what the encoder \"understood\" at that position.\n",
    "    \n",
    "    decoder_state : numpy array, shape (hidden_size,)\n",
    "        Current decoder hidden state.\n",
    "        This represents \"what we're trying to generate right now.\"\n",
    "    \n",
    "    verbose : bool\n",
    "        If True, print step-by-step computation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    context : numpy array, shape (hidden_size,)\n",
    "        Weighted sum of encoder states (the \"attended\" representation)\n",
    "    attention_weights : numpy array, shape (seq_len,)\n",
    "        How much attention is paid to each encoder position (sums to 1)\n",
    "    \"\"\"\n",
    "    seq_len, hidden_size = encoder_states.shape\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Computing Attention\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Encoder states: {seq_len} positions, each of size {hidden_size}\")\n",
    "        print(f\"Decoder state: size {hidden_size}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 1: Compute attention scores\n",
    "    # =========================================================================\n",
    "    # For each encoder position, compute: \"How relevant is this to the decoder?\"\n",
    "    # \n",
    "    # Simple approach: dot product between encoder and decoder states\n",
    "    # Higher dot product = more similar = more relevant\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for j in range(seq_len):\n",
    "        # Dot product measures similarity\n",
    "        score = np.dot(encoder_states[j], decoder_state)\n",
    "        scores.append(score)\n",
    "    \n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 1: Raw attention scores (dot products)\")\n",
    "        for j, score in enumerate(scores):\n",
    "            print(f\"  Position {j}: score = {score:.4f}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 2: Normalize with softmax\n",
    "    # =========================================================================\n",
    "    # Convert scores to probabilities that sum to 1\n",
    "    # Higher score → higher probability → more attention\n",
    "    \n",
    "    # Softmax with numerical stability (subtract max)\n",
    "    exp_scores = np.exp(scores - np.max(scores))\n",
    "    attention_weights = exp_scores / np.sum(exp_scores)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 2: Attention weights (softmax of scores)\")\n",
    "        for j, weight in enumerate(attention_weights):\n",
    "            bar = \"█\" * int(weight * 40)  # Visual bar\n",
    "            print(f\"  Position {j}: {weight:.4f} {bar}\")\n",
    "        print(f\"  Sum of weights: {np.sum(attention_weights):.4f} (should be 1.0)\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 3: Compute weighted sum (context vector)\n",
    "    # =========================================================================\n",
    "    # The context vector is a weighted combination of all encoder states\n",
    "    # Positions with higher attention contribute more\n",
    "    \n",
    "    context = np.zeros(hidden_size)\n",
    "    \n",
    "    for j in range(seq_len):\n",
    "        # Each encoder state contributes proportionally to its attention weight\n",
    "        context += attention_weights[j] * encoder_states[j]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 3: Context vector (weighted sum)\")\n",
    "        print(f\"  context = Σ(attention_weight[j] × encoder_state[j])\")\n",
    "        print(f\"  Context shape: {context.shape}\")\n",
    "        print(f\"  Context values: {context}\")\n",
    "    \n",
    "    return context, attention_weights\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ATTENTION MECHANISM DEMONSTRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate encoder states for a 5-word sentence\n",
    "# In practice, these come from an RNN or transformer encoder\n",
    "seq_len = 5\n",
    "hidden_size = 4\n",
    "\n",
    "# Create encoder states (pretend these encode: \"The cat sat on mat\")\n",
    "encoder_states = np.array([\n",
    "    [0.8, 0.1, 0.1, 0.0],   # \"The\" - article, low information\n",
    "    [0.1, 0.9, 0.1, 0.1],   # \"cat\" - subject, high information\n",
    "    [0.1, 0.1, 0.8, 0.1],   # \"sat\" - verb\n",
    "    [0.3, 0.1, 0.1, 0.1],   # \"on\" - preposition\n",
    "    [0.1, 0.1, 0.2, 0.9],   # \"mat\" - object\n",
    "])\n",
    "\n",
    "# Simulate decoder state when generating a word related to \"cat\"\n",
    "# (we want the model to pay attention to \"cat\")\n",
    "decoder_state = np.array([0.2, 0.85, 0.1, 0.1])  # Similar to \"cat\" encoding\n",
    "\n",
    "print(\"\\nScenario: Translating to French, currently generating word related to 'cat'\")\n",
    "print(\"We expect attention to focus on position 1 ('cat')\")\n",
    "\n",
    "context, attention = compute_attention(encoder_states, decoder_state, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"The model paid most attention to position 1 ('{['The', 'cat', 'sat', 'on', 'mat'][np.argmax(attention)]}')\")\n",
    "print(f\"This context vector can now be used to help generate the French word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e218b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SELF-ATTENTION STEP BY STEP\n",
      "======================================================================\n",
      "\n",
      "Input: 3 positions, each with 4 dimensions\n",
      "\n",
      "Step 1: Projection matrices\n",
      "  W_Q shape: (4, 4) (projects to Query space)\n",
      "  W_K shape: (4, 4) (projects to Key space)\n",
      "  W_V shape: (4, 4) (projects to Value space)\n",
      "\n",
      "Step 2: Compute Q, K, V for each position\n",
      "  Q = X @ W_Q, shape: (3, 4)\n",
      "  K = X @ W_K, shape: (3, 4)\n",
      "  V = X @ W_V, shape: (3, 4)\n",
      "\n",
      "  Interpretation:\n",
      "  - Q[i] = What is position 0 looking for?\n",
      "  - K[i] = What does position 0 contain?\n",
      "  - V[i] = What should position 0 contribute?\n",
      "\n",
      "Step 3: Compute attention scores (Q @ K^T)\n",
      "  Scores shape: (3, 3)\n",
      "  scores[i,j] = How much should position i attend to position j?\n",
      "\n",
      "  Raw scores matrix:\n",
      "    Position 0: [ -0.03  -0.02   0.02]\n",
      "    Position 1: [ -0.01  -0.02   0.00]\n",
      "    Position 2: [  0.02   0.02   0.00]\n",
      "\n",
      "Step 4: Scale by √d_k = √4 = 2.00\n",
      "  Why? Large dot products make softmax too 'peaky'\n",
      "  Scaled scores range: [-0.02, 0.01]\n",
      "\n",
      "Step 5: Apply softmax (convert to probabilities)\n",
      "  Each row now sums to 1.0\n",
      "\n",
      "  Attention weight matrix:\n",
      "    Position 0: [0.329 0.332 0.338] → focuses on position 2\n",
      "    Position 1: [0.332 0.332 0.336] → focuses on position 2\n",
      "    Position 2: [0.334 0.335 0.332] → focuses on position 1\n",
      "\n",
      "Step 6: Compute weighted sum of values\n",
      "  output = attention_weights @ V\n",
      "  Output shape: (3, 4)\n",
      "\n",
      "  Interpretation:\n",
      "  output[i] now contains information from ALL positions,\n",
      "  weighted by how relevant each position is to position i.\n",
      "\n",
      "======================================================================\n",
      "KEY INSIGHT\n",
      "======================================================================\n",
      "\n",
      "After self-attention, each position's representation incorporates\n",
      "information from ALL other positions, weighted by relevance.\n",
      "\n",
      "This is different from RNNs:\n",
      "- RNN: Position 3 only \"sees\" position 1 through position 2\n",
      "- Self-attention: Position 3 directly attends to position 1\n",
      "\n",
      "This direct connection is why transformers handle long-range\n",
      "dependencies so much better than RNNs!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def self_attention_step_by_step(X, verbose=True):\n",
    "    \"\"\"\n",
    "    Demonstrate self-attention with detailed explanation.\n",
    "    \n",
    "    Self-attention allows each position in a sequence to \"look at\" all other\n",
    "    positions and compute a weighted combination based on relevance.\n",
    "    \n",
    "    The key innovation: Instead of using the same representation for everything,\n",
    "    we project into three different spaces:\n",
    "    - Query (Q): \"What am I looking for?\"\n",
    "    - Key (K): \"What do I contain?\"\n",
    "    - Value (V): \"What should I contribute?\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (seq_len, d_model)\n",
    "        Input sequence. Each row is one position's embedding.\n",
    "        In transformers, d_model is typically 512 or 768.\n",
    "    \n",
    "    verbose : bool\n",
    "        If True, print step-by-step details.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : numpy array, shape (seq_len, d_model)\n",
    "        Output sequence after self-attention.\n",
    "        Each position now contains information gathered from all positions.\n",
    "    attention_weights : numpy array, shape (seq_len, seq_len)\n",
    "        Attention weight matrix showing how much each position attends to others.\n",
    "    \"\"\"\n",
    "    \n",
    "    seq_len, d_model = X.shape\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\" * 70)\n",
    "        print(\"SELF-ATTENTION STEP BY STEP\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"\\nInput: {seq_len} positions, each with {d_model} dimensions\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 1: Create learnable projection matrices\n",
    "    # =========================================================================\n",
    "    # In practice, these are learned during training.\n",
    "    # Here we'll use random matrices for demonstration.\n",
    "    \n",
    "    # d_k is the dimension of queries and keys (often d_model or d_model/num_heads)\n",
    "    d_k = d_model\n",
    "    d_v = d_model\n",
    "    \n",
    "    # W_Q projects input to query space\n",
    "    # W_K projects input to key space  \n",
    "    # W_V projects input to value space\n",
    "    np.random.seed(42)\n",
    "    W_Q = np.random.randn(d_model, d_k) * 0.1\n",
    "    W_K = np.random.randn(d_model, d_k) * 0.1\n",
    "    W_V = np.random.randn(d_model, d_v) * 0.1\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 1: Projection matrices\")\n",
    "        print(f\"  W_Q shape: {W_Q.shape} (projects to Query space)\")\n",
    "        print(f\"  W_K shape: {W_K.shape} (projects to Key space)\")\n",
    "        print(f\"  W_V shape: {W_V.shape} (projects to Value space)\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 2: Compute Q, K, V for each position\n",
    "    # =========================================================================\n",
    "    # Each position gets its own Query, Key, and Value vector\n",
    "    \n",
    "    Q = np.dot(X, W_Q)  # Shape: (seq_len, d_k)\n",
    "    K = np.dot(X, W_K)  # Shape: (seq_len, d_k)\n",
    "    V = np.dot(X, W_V)  # Shape: (seq_len, d_v)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 2: Compute Q, K, V for each position\")\n",
    "        print(f\"  Q = X @ W_Q, shape: {Q.shape}\")\n",
    "        print(f\"  K = X @ W_K, shape: {K.shape}\")\n",
    "        print(f\"  V = X @ W_V, shape: {V.shape}\")\n",
    "        print(f\"\\n  Interpretation:\")\n",
    "        print(f\"  - Q[i] = What is position {0} looking for?\")\n",
    "        print(f\"  - K[i] = What does position {0} contain?\")\n",
    "        print(f\"  - V[i] = What should position {0} contribute?\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 3: Compute attention scores\n",
    "    # =========================================================================\n",
    "    # For each pair of positions (i, j):\n",
    "    # score[i,j] = Q[i] · K[j] = \"How relevant is position j to position i?\"\n",
    "    \n",
    "    # Matrix multiplication: Q @ K^T gives all pairwise dot products at once!\n",
    "    # Shape: (seq_len, d_k) @ (d_k, seq_len) = (seq_len, seq_len)\n",
    "    scores = np.dot(Q, K.T)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 3: Compute attention scores (Q @ K^T)\")\n",
    "        print(f\"  Scores shape: {scores.shape}\")\n",
    "        print(f\"  scores[i,j] = How much should position i attend to position j?\")\n",
    "        print(f\"\\n  Raw scores matrix:\")\n",
    "        for i in range(seq_len):\n",
    "            row = \" \".join([f\"{s:6.2f}\" for s in scores[i]])\n",
    "            print(f\"    Position {i}: [{row}]\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 4: Scale by sqrt(d_k)\n",
    "    # =========================================================================\n",
    "    # Why scale? Dot products grow with dimension size.\n",
    "    # Large values → softmax outputs very peaked (all weight on one position)\n",
    "    # Scaling keeps the variance manageable.\n",
    "    \n",
    "    scale = np.sqrt(d_k)\n",
    "    scaled_scores = scores / scale\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 4: Scale by √d_k = √{d_k} = {scale:.2f}\")\n",
    "        print(f\"  Why? Large dot products make softmax too 'peaky'\")\n",
    "        print(f\"  Scaled scores range: [{scaled_scores.min():.2f}, {scaled_scores.max():.2f}]\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 5: Apply softmax (row-wise)\n",
    "    # =========================================================================\n",
    "    # Convert scores to probabilities\n",
    "    # Each row sums to 1: attention_weights[i] sums to 1\n",
    "    \n",
    "    def softmax(x, axis=-1):\n",
    "        exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "    \n",
    "    attention_weights = softmax(scaled_scores, axis=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 5: Apply softmax (convert to probabilities)\")\n",
    "        print(f\"  Each row now sums to 1.0\")\n",
    "        print(f\"\\n  Attention weight matrix:\")\n",
    "        for i in range(seq_len):\n",
    "            row = \" \".join([f\"{w:5.3f}\" for w in attention_weights[i]])\n",
    "            max_j = np.argmax(attention_weights[i])\n",
    "            print(f\"    Position {i}: [{row}] → focuses on position {max_j}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Step 6: Compute weighted sum of values\n",
    "    # =========================================================================\n",
    "    # For each position i:\n",
    "    # output[i] = sum over j of (attention_weight[i,j] * V[j])\n",
    "    #\n",
    "    # Matrix form: attention_weights @ V\n",
    "    # Shape: (seq_len, seq_len) @ (seq_len, d_v) = (seq_len, d_v)\n",
    "    \n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nStep 6: Compute weighted sum of values\")\n",
    "        print(f\"  output = attention_weights @ V\")\n",
    "        print(f\"  Output shape: {output.shape}\")\n",
    "        print(f\"\\n  Interpretation:\")\n",
    "        print(f\"  output[i] now contains information from ALL positions,\")\n",
    "        print(f\"  weighted by how relevant each position is to position i.\")\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create a simple sequence (3 positions, 4 dimensions each)\n",
    "# Pretend this represents: [\"The\", \"cat\", \"sat\"]\n",
    "# with each word having a 4-dimensional embedding\n",
    "\n",
    "X = np.array([\n",
    "    [1.0, 0.0, 0.0, 0.0],   # \"The\" - article\n",
    "    [0.0, 1.0, 0.5, 0.0],   # \"cat\" - noun, animate\n",
    "    [0.0, 0.0, 0.0, 1.0],   # \"sat\" - verb\n",
    "])\n",
    "\n",
    "output, attention_weights = self_attention_step_by_step(X, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "After self-attention, each position's representation incorporates\n",
    "information from ALL other positions, weighted by relevance.\n",
    "\n",
    "This is different from RNNs:\n",
    "- RNN: Position 3 only \"sees\" position 1 through position 2\n",
    "- Self-attention: Position 3 directly attends to position 1\n",
    "\n",
    "This direct connection is why transformers handle long-range\n",
    "dependencies so much better than RNNs!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeb9dd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Encoding Visualization\n",
      "============================================================\n",
      "Shape: (50, 64) (50 positions, 64 dimensions)\n",
      "\n",
      "Each position gets a unique pattern of sin/cos values.\n",
      "These are ADDED to word embeddings to inject position info.\n",
      "\n",
      "Position 0, first 8 dims: [0. 1. 0. 1. 0. 1. 0. 1.]\n",
      "Position 1, first 8 dims: [0.841 0.54  0.682 0.732 0.533 0.846 0.409 0.912]\n",
      "Position 2, first 8 dims: [ 0.909 -0.416  0.997  0.071  0.902  0.431  0.747  0.665]\n"
     ]
    }
   ],
   "source": [
    "def positional_encoding(max_seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Compute sinusoidal positional encoding.\n",
    "    \n",
    "    Why sinusoidal?\n",
    "    1. Deterministic (no learning required)\n",
    "    2. Can extrapolate to longer sequences than seen in training\n",
    "    3. Relative positions can be computed via linear transformation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    max_seq_len : int\n",
    "        Maximum sequence length to generate encodings for\n",
    "    d_model : int\n",
    "        Dimension of the model (embedding dimension)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    PE : numpy array, shape (max_seq_len, d_model)\n",
    "        Positional encodings. Add these to word embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create position indices: [0, 1, 2, ..., max_seq_len-1]\n",
    "    positions = np.arange(max_seq_len)[:, np.newaxis]  # Shape: (max_seq_len, 1)\n",
    "    \n",
    "    # Create dimension indices: [0, 1, 2, ..., d_model-1]\n",
    "    dims = np.arange(d_model)[np.newaxis, :]  # Shape: (1, d_model)\n",
    "    \n",
    "    # Compute the angles\n",
    "    # The division by 10000^(2i/d_model) creates different frequencies\n",
    "    # for different dimensions\n",
    "    angles = positions / np.power(10000, (2 * (dims // 2)) / d_model)\n",
    "    \n",
    "    # Apply sin to even indices, cos to odd indices\n",
    "    PE = np.zeros((max_seq_len, d_model))\n",
    "    PE[:, 0::2] = np.sin(angles[:, 0::2])  # Even dimensions: sin\n",
    "    PE[:, 1::2] = np.cos(angles[:, 1::2])  # Odd dimensions: cos\n",
    "    \n",
    "    return PE\n",
    "\n",
    "\n",
    "# Visualize positional encoding\n",
    "PE = positional_encoding(max_seq_len=50, d_model=64)\n",
    "\n",
    "print(\"Positional Encoding Visualization\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {PE.shape} (50 positions, 64 dimensions)\")\n",
    "print(f\"\\nEach position gets a unique pattern of sin/cos values.\")\n",
    "print(f\"These are ADDED to word embeddings to inject position info.\")\n",
    "print(f\"\\nPosition 0, first 8 dims: {PE[0, :8].round(3)}\")\n",
    "print(f\"Position 1, first 8 dims: {PE[1, :8].round(3)}\")\n",
    "print(f\"Position 2, first 8 dims: {PE[2, :8].round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "422ca3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MULTI-HEAD ATTENTION\n",
      "======================================================================\n",
      "\n",
      "WHY MULTIPLE HEADS?\n",
      "\n",
      "Single attention: \"What should I focus on?\"\n",
      "Multi-head: \"Let me look at this from MULTIPLE perspectives simultaneously.\"\n",
      "\n",
      "Each head can learn to focus on different types of relationships:\n",
      "  - Head 1: Subject-verb agreement (\"The cats... ARE\")\n",
      "  - Head 2: Pronoun resolution (what does \"it\" refer to?)\n",
      "  - Head 3: Nearby words (local context)\n",
      "  - Head 4: Semantic similarity (related concepts)\n",
      "\n",
      "HOW IT WORKS:\n",
      "1. Split Q, K, V into num_heads smaller pieces\n",
      "2. Run scaled dot-product attention on each piece independently\n",
      "3. Concatenate all head outputs\n",
      "4. (Optional) Apply final linear projection W_O\n",
      "\n",
      "The key insight: by using different learned projections for each head,\n",
      "we get multiple \"views\" of the same data. This is more expressive than\n",
      "a single attention mechanism could ever be.\n",
      "\n",
      "\n",
      "--- Demo: 5 tokens, 16 dimensions, 4 heads ---\n",
      "\n",
      "Multi-Head Attention\n",
      "  4 heads, each with dimension 4\n",
      "  Head 0: attention pattern computed\n",
      "  Head 1: attention pattern computed\n",
      "  Head 2: attention pattern computed\n",
      "  Head 3: attention pattern computed\n",
      "  Concatenated output shape: (5, 16)\n",
      "\n",
      "Input shape:  (5, 16) — 5 tokens, 16 dimensions\n",
      "Output shape: (5, 16) — same shape, but now each position\n",
      "              contains information gathered from all positions\n",
      "              through 4 different 'lenses' (heads).\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MULTI-HEAD ATTENTION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "WHY MULTIPLE HEADS?\n",
    "\n",
    "Single attention: \"What should I focus on?\"\n",
    "Multi-head: \"Let me look at this from MULTIPLE perspectives simultaneously.\"\n",
    "\n",
    "Each head can learn to focus on different types of relationships:\n",
    "  - Head 1: Subject-verb agreement (\"The cats... ARE\")\n",
    "  - Head 2: Pronoun resolution (what does \"it\" refer to?)\n",
    "  - Head 3: Nearby words (local context)\n",
    "  - Head 4: Semantic similarity (related concepts)\n",
    "\n",
    "HOW IT WORKS:\n",
    "1. Split Q, K, V into num_heads smaller pieces\n",
    "2. Run scaled dot-product attention on each piece independently\n",
    "3. Concatenate all head outputs\n",
    "4. (Optional) Apply final linear projection W_O\n",
    "\n",
    "The key insight: by using different learned projections for each head,\n",
    "we get multiple \"views\" of the same data. This is more expressive than\n",
    "a single attention mechanism could ever be.\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def multi_head_attention(Q, K, V, num_heads=8, verbose=False):\n",
    "    \"\"\"\n",
    "    Multi-head attention: Run attention multiple times in parallel.\n",
    "    \n",
    "    Why multiple heads?\n",
    "    - Each head can learn to focus on different types of relationships\n",
    "    - One head might track syntax, another semantics, another proximity\n",
    "    - More expressive than single attention\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q, K, V : numpy arrays, shape (seq_len, d_model)\n",
    "        Query, Key, Value matrices\n",
    "    num_heads : int\n",
    "        Number of parallel attention heads\n",
    "    verbose : bool\n",
    "        Print details\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : numpy array, shape (seq_len, d_model)\n",
    "        Multi-head attention output\n",
    "    \"\"\"\n",
    "    \n",
    "    seq_len, d_model = Q.shape\n",
    "    \n",
    "    # Each head operates on a slice of the dimensions\n",
    "    # d_k = d_model / num_heads\n",
    "    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "    d_k = d_model // num_heads\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Multi-Head Attention\")\n",
    "        print(f\"  {num_heads} heads, each with dimension {d_k}\")\n",
    "    \n",
    "    head_outputs = []\n",
    "    \n",
    "    for head in range(num_heads):\n",
    "        # Slice the dimensions for this head\n",
    "        start = head * d_k\n",
    "        end = start + d_k\n",
    "        \n",
    "        Q_h = Q[:, start:end]\n",
    "        K_h = K[:, start:end]\n",
    "        V_h = V[:, start:end]\n",
    "        \n",
    "        # Standard scaled dot-product attention for this head\n",
    "        scores = np.dot(Q_h, K_h.T) / np.sqrt(d_k)\n",
    "        attention_weights = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
    "        attention_weights = attention_weights / attention_weights.sum(axis=1, keepdims=True)\n",
    "        head_output = np.dot(attention_weights, V_h)\n",
    "        \n",
    "        head_outputs.append(head_output)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Head {head}: attention pattern computed\")\n",
    "    \n",
    "    # Concatenate all head outputs\n",
    "    # Shape: (seq_len, num_heads * d_k) = (seq_len, d_model)\n",
    "    output = np.concatenate(head_outputs, axis=1)\n",
    "    \n",
    "    # In practice, there's a final linear projection here\n",
    "    # output = output @ W_O\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Concatenated output shape: {output.shape}\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n--- Demo: 5 tokens, 16 dimensions, 4 heads ---\\n\")\n",
    "\n",
    "np.random.seed(42)\n",
    "Q = np.random.randn(5, 16)  # 5 tokens, 16 dims\n",
    "K = np.random.randn(5, 16)\n",
    "V = np.random.randn(5, 16)\n",
    "\n",
    "output = multi_head_attention(Q, K, V, num_heads=4, verbose=True)\n",
    "\n",
    "print(f\"\\nInput shape:  (5, 16) — 5 tokens, 16 dimensions\")\n",
    "print(f\"Output shape: {output.shape} — same shape, but now each position\")\n",
    "print(f\"              contains information gathered from all positions\")\n",
    "print(f\"              through 4 different 'lenses' (heads).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca9d300a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEED-FORWARD NETWORK (Position-wise)\n",
      "======================================================================\n",
      "\n",
      "WHAT IT IS:\n",
      "\n",
      "After attention gathers information from all positions, we need to \n",
      "PROCESS that information. The feed-forward network is a simple 2-layer \n",
      "neural network applied to each position INDEPENDENTLY.\n",
      "\n",
      "    x → Linear(d_model → d_ff) → ReLU → Linear(d_ff → d_model) → output\n",
      "\n",
      "WHY THE EXPANSION?\n",
      "\n",
      "The hidden dimension d_ff is typically 4× the model dimension.\n",
      "  - GPT-3: d_model=12288, d_ff=49152 (4×)\n",
      "  - BERT:  d_model=768, d_ff=3072 (4×)\n",
      "\n",
      "This \"expand then compress\" pattern lets the network:\n",
      "  1. Project to a higher-dimensional space (more room to transform)\n",
      "  2. Apply non-linearity (ReLU creates complex features)\n",
      "  3. Project back to original dimension\n",
      "\n",
      "Think of it as: attention decides WHAT to look at, feed-forward \n",
      "decides WHAT TO DO with that information.\n",
      "\n",
      "POSITION-WISE means: The same weights are applied to each position,\n",
      "but each position is processed independently (no mixing between positions).\n",
      "\n",
      "--- Demo: 5 tokens, 64 dimensions, expand to 256 ---\n",
      "\n",
      "Feed-Forward Network\n",
      "  Input: 64 dims → Hidden: 256 dims → Output: 64 dims\n",
      "  Expansion ratio: 4.0×\n",
      "\n",
      "Input shape:  (5, 64)\n",
      "Output shape: (5, 64)\n",
      "\n",
      "Each position was processed independently through:\n",
      "  64 → 256 (expand) → ReLU → 256 → 64 (compress)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FEED-FORWARD NETWORK (Position-wise)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "WHAT IT IS:\n",
    "\n",
    "After attention gathers information from all positions, we need to \n",
    "PROCESS that information. The feed-forward network is a simple 2-layer \n",
    "neural network applied to each position INDEPENDENTLY.\n",
    "\n",
    "    x → Linear(d_model → d_ff) → ReLU → Linear(d_ff → d_model) → output\n",
    "\n",
    "WHY THE EXPANSION?\n",
    "\n",
    "The hidden dimension d_ff is typically 4× the model dimension.\n",
    "  - GPT-3: d_model=12288, d_ff=49152 (4×)\n",
    "  - BERT:  d_model=768, d_ff=3072 (4×)\n",
    "\n",
    "This \"expand then compress\" pattern lets the network:\n",
    "  1. Project to a higher-dimensional space (more room to transform)\n",
    "  2. Apply non-linearity (ReLU creates complex features)\n",
    "  3. Project back to original dimension\n",
    "\n",
    "Think of it as: attention decides WHAT to look at, feed-forward \n",
    "decides WHAT TO DO with that information.\n",
    "\n",
    "POSITION-WISE means: The same weights are applied to each position,\n",
    "but each position is processed independently (no mixing between positions).\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def feed_forward(x, d_ff=2048, verbose=False):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network.\n",
    "    \n",
    "    This is applied identically to each position independently.\n",
    "    It's essentially a 2-layer neural network that adds:\n",
    "    - Non-linearity (ReLU activation)\n",
    "    - Additional learnable transformations\n",
    "    \n",
    "    The hidden dimension (d_ff) is typically 4× the model dimension.\n",
    "    This expansion allows the model to learn more complex transformations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy array, shape (seq_len, d_model)\n",
    "        Input from attention layer\n",
    "    d_ff : int\n",
    "        Hidden dimension (typically 4 × d_model)\n",
    "    verbose : bool\n",
    "        Print details\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : numpy array, shape (seq_len, d_model)\n",
    "        Output after feed-forward processing\n",
    "    \"\"\"\n",
    "    \n",
    "    seq_len, d_model = x.shape\n",
    "    \n",
    "    # Initialize weights (in practice, these are learned)\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(d_model, d_ff) * 0.02\n",
    "    b1 = np.zeros(d_ff)\n",
    "    W2 = np.random.randn(d_ff, d_model) * 0.02\n",
    "    b2 = np.zeros(d_model)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Feed-Forward Network\")\n",
    "        print(f\"  Input: {d_model} dims → Hidden: {d_ff} dims → Output: {d_model} dims\")\n",
    "    \n",
    "    # First linear transformation + ReLU\n",
    "    # Expands to higher dimension\n",
    "    hidden = np.maximum(0, np.dot(x, W1) + b1)  # ReLU activation\n",
    "    \n",
    "    # Second linear transformation\n",
    "    # Projects back to model dimension\n",
    "    output = np.dot(hidden, W2) + b2\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Expansion ratio: {d_ff / d_model}×\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- Demo: 5 tokens, 64 dimensions, expand to 256 ---\\n\")\n",
    "\n",
    "x = np.random.randn(5, 64)  # 5 tokens, 64 dims (output from attention)\n",
    "output = feed_forward(x, d_ff=256, verbose=True)\n",
    "\n",
    "print(f\"\\nInput shape:  {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nEach position was processed independently through:\")\n",
    "print(f\"  64 → 256 (expand) → ReLU → 256 → 64 (compress)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2403220d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LAYER NORMALIZATION & RESIDUAL CONNECTIONS\n",
      "======================================================================\n",
      "\n",
      "TWO TECHNIQUES THAT MAKE DEEP TRANSFORMERS TRAINABLE:\n",
      "\n",
      "1. LAYER NORMALIZATION\n",
      "\n",
      "   Problem: Activations can have wildly different scales across layers.\n",
      "   Solution: Normalize each position to have mean=0, variance=1.\n",
      "\n",
      "   For each position independently:\n",
      "     normalized = (x - mean) / sqrt(variance + eps)\n",
      "\n",
      "   Why Layer Norm (not Batch Norm)?\n",
      "   - Batch Norm: normalizes across batch → needs large batches, weird at inference\n",
      "   - Layer Norm: normalizes across features → works with any batch size\n",
      "\n",
      "2. RESIDUAL CONNECTIONS (Skip Connections)\n",
      "\n",
      "   Problem: In deep networks, gradients vanish through many layers.\n",
      "   Solution: Add the input directly to the output: output = x + sublayer(x)\n",
      "\n",
      "   Why this works:\n",
      "   - Gradients flow DIRECTLY through the addition (no vanishing!)\n",
      "   - If sublayer learns nothing useful, network can just pass input through\n",
      "   - Same idea that made ResNets trainable (50+ layers)\n",
      "\n",
      "In transformers, we combine both:\n",
      "   output = LayerNorm(x + Sublayer(x))\n",
      "\n",
      "--- Demo: Layer Normalization ---\n",
      "\n",
      "Before normalization:\n",
      "  Position 0: mean=250.0, std=111.8\n",
      "  Position 1: mean=0.025, std=0.011\n",
      "\n",
      "After normalization:\n",
      "  Position 0: mean=0.000000, std=1.00\n",
      "  Position 1: mean=-0.000000, std=1.00\n",
      "\n",
      "Both positions now have mean≈0, std≈1 regardless of original scale!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "--- Demo: Residual Connection ---\n",
      "\n",
      "Input:            [1. 2. 3. 4.]\n",
      "Sublayer output:  [0.1 0.2 0.3 0.4]\n",
      "With residual:    [1.1 2.2 3.3 4.4]\n",
      "\n",
      "The residual connection preserves the original signal!\n",
      "Even if sublayer learns nothing useful (outputs zeros), \n",
      "the input passes through unchanged.\n",
      "\n",
      "This is why we can stack 12, 24, or even 96 transformer layers\n",
      "without gradients vanishing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LAYER NORMALIZATION & RESIDUAL CONNECTIONS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "TWO TECHNIQUES THAT MAKE DEEP TRANSFORMERS TRAINABLE:\n",
    "\n",
    "1. LAYER NORMALIZATION\n",
    "   \n",
    "   Problem: Activations can have wildly different scales across layers.\n",
    "   Solution: Normalize each position to have mean=0, variance=1.\n",
    "   \n",
    "   For each position independently:\n",
    "     normalized = (x - mean) / sqrt(variance + eps)\n",
    "   \n",
    "   Why Layer Norm (not Batch Norm)?\n",
    "   - Batch Norm: normalizes across batch → needs large batches, weird at inference\n",
    "   - Layer Norm: normalizes across features → works with any batch size\n",
    "   \n",
    "2. RESIDUAL CONNECTIONS (Skip Connections)\n",
    "   \n",
    "   Problem: In deep networks, gradients vanish through many layers.\n",
    "   Solution: Add the input directly to the output: output = x + sublayer(x)\n",
    "   \n",
    "   Why this works:\n",
    "   - Gradients flow DIRECTLY through the addition (no vanishing!)\n",
    "   - If sublayer learns nothing useful, network can just pass input through\n",
    "   - Same idea that made ResNets trainable (50+ layers)\n",
    "\n",
    "In transformers, we combine both:\n",
    "   output = LayerNorm(x + Sublayer(x))\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def layer_norm(x, gamma=None, beta=None, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Layer Normalization.\n",
    "    \n",
    "    Unlike Batch Normalization (normalizes across batch),\n",
    "    Layer Norm normalizes across features for each position independently.\n",
    "    \n",
    "    This is preferred in transformers because:\n",
    "    1. Works with variable sequence lengths\n",
    "    2. Consistent behavior for training and inference\n",
    "    3. Each position is normalized independently\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy array, shape (..., d_model)\n",
    "        Input tensor (last dimension is features)\n",
    "    gamma : numpy array, shape (d_model,), optional\n",
    "        Learned scale parameter\n",
    "    beta : numpy array, shape (d_model,), optional\n",
    "        Learned shift parameter\n",
    "    eps : float\n",
    "        Small constant for numerical stability\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    normalized : numpy array, same shape as x\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute mean and variance over the last dimension (features)\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    var = x.var(axis=-1, keepdims=True)\n",
    "    \n",
    "    # Normalize: zero mean, unit variance\n",
    "    normalized = (x - mean) / np.sqrt(var + eps)\n",
    "    \n",
    "    # Apply learnable scale and shift (if provided)\n",
    "    if gamma is not None:\n",
    "        normalized = normalized * gamma\n",
    "    if beta is not None:\n",
    "        normalized = normalized + beta\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "\n",
    "def residual_block(x, sublayer_fn, verbose=False):\n",
    "    \"\"\"\n",
    "    Residual connection with layer normalization.\n",
    "    \n",
    "    output = LayerNorm(x + Sublayer(x))\n",
    "    \n",
    "    The residual connection (adding x) is crucial:\n",
    "    - Gradients can flow directly through addition\n",
    "    - Easier to learn identity function if needed\n",
    "    - Enables training of very deep networks\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy array\n",
    "        Input\n",
    "    sublayer_fn : callable\n",
    "        The sub-layer (attention or feed-forward)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : numpy array\n",
    "        Output after residual + layer norm\n",
    "    \"\"\"\n",
    "    \n",
    "    # Apply sublayer\n",
    "    sublayer_output = sublayer_fn(x)\n",
    "    \n",
    "    # Add residual connection\n",
    "    residual = x + sublayer_output\n",
    "    \n",
    "    # Apply layer normalization\n",
    "    output = layer_norm(residual)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Residual connection: input + sublayer_output\")\n",
    "        print(f\"  Layer normalization: stabilize activations\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"--- Demo: Layer Normalization ---\\n\")\n",
    "\n",
    "# Create input with varying scales\n",
    "x = np.array([\n",
    "    [100.0, 200.0, 300.0, 400.0],  # Position 0: large values\n",
    "    [0.01, 0.02, 0.03, 0.04],       # Position 1: tiny values\n",
    "])\n",
    "\n",
    "print(f\"Before normalization:\")\n",
    "print(f\"  Position 0: mean={x[0].mean():.1f}, std={x[0].std():.1f}\")\n",
    "print(f\"  Position 1: mean={x[1].mean():.3f}, std={x[1].std():.3f}\")\n",
    "\n",
    "normalized = layer_norm(x)\n",
    "\n",
    "print(f\"\\nAfter normalization:\")\n",
    "print(f\"  Position 0: mean={normalized[0].mean():.6f}, std={normalized[0].std():.2f}\")\n",
    "print(f\"  Position 1: mean={normalized[1].mean():.6f}, std={normalized[1].std():.2f}\")\n",
    "print(f\"\\nBoth positions now have mean≈0, std≈1 regardless of original scale!\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"--- Demo: Residual Connection ---\\n\")\n",
    "\n",
    "# Simulate a sublayer that makes small changes\n",
    "def dummy_sublayer(x):\n",
    "    return x * 0.1  # Small modification\n",
    "\n",
    "x_input = np.array([[1.0, 2.0, 3.0, 4.0]])\n",
    "\n",
    "# Without residual: just the sublayer output\n",
    "without_residual = dummy_sublayer(x_input)\n",
    "\n",
    "# With residual: input + sublayer output\n",
    "with_residual = x_input + dummy_sublayer(x_input)\n",
    "\n",
    "print(f\"Input:            {x_input[0]}\")\n",
    "print(f\"Sublayer output:  {without_residual[0]}\")\n",
    "print(f\"With residual:    {with_residual[0]}\")\n",
    "print(f\"\"\"\n",
    "The residual connection preserves the original signal!\n",
    "Even if sublayer learns nothing useful (outputs zeros), \n",
    "the input passes through unchanged.\n",
    "\n",
    "This is why we can stack 12, 24, or even 96 transformer layers\n",
    "without gradients vanishing.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c8462ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MASKED SELF-ATTENTION DEMONSTRATION\n",
      "======================================================================\n",
      "Masked Self-Attention\n",
      "  Mask (upper triangle = -inf, prevents looking ahead):\n",
      "  Position 0: can only see position 0\n",
      "  Position 1: can see positions 0, 1\n",
      "  Position 2: can see positions 0, 1, 2\n",
      "  etc.\n",
      "\n",
      "Attention weights (rows should only have non-zero values up to diagonal):\n",
      "  Position 0: [1.000 0.000 0.000 0.000]\n",
      "  Position 1: [0.852 0.148 0.000 0.000]\n",
      "  Position 2: [0.201 0.063 0.736 0.000]\n",
      "  Position 3: [0.515 0.347 0.023 0.115]\n",
      "\n",
      "Notice: Position 0 can only attend to itself (100%)\n",
      "        Position 3 can attend to all positions 0-3\n"
     ]
    }
   ],
   "source": [
    "def masked_self_attention(Q, K, V, verbose=False):\n",
    "    \"\"\"\n",
    "    Masked self-attention for autoregressive generation.\n",
    "    \n",
    "    When generating text left-to-right, position i can only attend\n",
    "    to positions 0, 1, ..., i-1 (not future positions).\n",
    "    \n",
    "    We implement this with a mask that sets future attention scores to -infinity,\n",
    "    which becomes 0 after softmax.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q, K, V : numpy arrays, shape (seq_len, d_model)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output : numpy array, shape (seq_len, d_model)\n",
    "    \"\"\"\n",
    "    \n",
    "    seq_len, d_k = Q.shape\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Create mask: upper triangular matrix of -infinity\n",
    "    # Position i can attend to positions 0..i only\n",
    "    mask = np.triu(np.ones((seq_len, seq_len)) * (-1e9), k=1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Masked Self-Attention\")\n",
    "        print(f\"  Mask (upper triangle = -inf, prevents looking ahead):\")\n",
    "        print(f\"  Position 0: can only see position 0\")\n",
    "        print(f\"  Position 1: can see positions 0, 1\")\n",
    "        print(f\"  Position 2: can see positions 0, 1, 2\")\n",
    "        print(f\"  etc.\")\n",
    "    \n",
    "    # Apply mask\n",
    "    masked_scores = scores + mask\n",
    "    \n",
    "    # Softmax (masked positions become ~0)\n",
    "    attention_weights = np.exp(masked_scores - np.max(masked_scores, axis=1, keepdims=True))\n",
    "    attention_weights = attention_weights / attention_weights.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# Demonstrate masking\n",
    "print(\"=\" * 70)\n",
    "print(\"MASKED SELF-ATTENTION DEMONSTRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "Q = np.random.randn(4, 8)\n",
    "K = np.random.randn(4, 8)\n",
    "V = np.random.randn(4, 8)\n",
    "\n",
    "output, weights = masked_self_attention(Q, K, V, verbose=True)\n",
    "\n",
    "print(\"\\nAttention weights (rows should only have non-zero values up to diagonal):\")\n",
    "for i in range(4):\n",
    "    row = \" \".join([f\"{w:.3f}\" for w in weights[i]])\n",
    "    print(f\"  Position {i}: [{row}]\")\n",
    "\n",
    "print(\"\\nNotice: Position 0 can only attend to itself (100%)\")\n",
    "print(\"        Position 3 can attend to all positions 0-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cffa0bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PUTTING IT ALL TOGETHER: THE TRANSFORMER LAYER\n",
      "======================================================================\n",
      "\n",
      "This is where everything we've learned comes together.\n",
      "\n",
      "A single Transformer layer combines:\n",
      "\n",
      "    ┌─────────────────────────────────────────────────────────────┐\n",
      "    │  Input (seq_len, d_model)                                   │\n",
      "    │       ↓                                                     │\n",
      "    │  ┌─────────────────────────────────────────┐                │\n",
      "    │  │  Multi-Head Self-Attention              │                │\n",
      "    │  │  \"Look at all positions, focus on       │                │\n",
      "    │  │   what's relevant from each\"            │                │\n",
      "    │  └─────────────────────────────────────────┘                │\n",
      "    │       ↓                                                     │\n",
      "    │  Add & Norm  (residual + layer norm)                        │\n",
      "    │       ↓                                                     │\n",
      "    │  ┌─────────────────────────────────────────┐                │\n",
      "    │  │  Feed-Forward Network                   │                │\n",
      "    │  │  \"Process each position independently\"  │                │\n",
      "    │  └─────────────────────────────────────────┘                │\n",
      "    │       ↓                                                     │\n",
      "    │  Add & Norm  (residual + layer norm)                        │\n",
      "    │       ↓                                                     │\n",
      "    │  Output (seq_len, d_model) — same shape!                    │\n",
      "    └─────────────────────────────────────────────────────────────┘\n",
      "\n",
      "KEY INSIGHTS:\n",
      "  - Input and output have the SAME shape → can stack many layers\n",
      "  - Attention: positions communicate with each other\n",
      "  - Feed-forward: each position processed independently\n",
      "  - Residual connections: gradients flow freely (no vanishing!)\n",
      "  - Layer norm: keeps activations stable\n",
      "\n",
      "Real transformers stack 6-96 of these layers:\n",
      "  - BERT-base: 12 layers\n",
      "  - GPT-3: 96 layers\n",
      "  - Each layer refines the representations further\n",
      "\n",
      "======================================================================\n",
      "COMPLETE TRANSFORMER LAYER DEMONSTRATION\n",
      "======================================================================\n",
      "\n",
      "Input shape: (5, 64) (5 tokens, 64 dimensions)\n",
      "\n",
      "Transformer Layer Forward Pass\n",
      "==================================================\n",
      "1. Multi-head self-attention...\n",
      "   + Residual connection + Layer Norm\n",
      "2. Feed-forward network...\n",
      "   + Residual connection + Layer Norm\n",
      "   Output shape: (5, 64)\n",
      "\n",
      "Output shape: (5, 64)\n",
      "\n",
      "WHAT JUST HAPPENED:\n",
      "\n",
      "1. Self-attention looked at ALL positions and computed relevance weights\n",
      "2. Each position gathered information from relevant positions  \n",
      "3. Residual connection preserved the original signal\n",
      "4. Layer norm stabilized the activations\n",
      "5. Feed-forward processed each position independently\n",
      "6. Another residual + layer norm\n",
      "\n",
      "The output has the SAME SHAPE as input — this is crucial!\n",
      "It means we can stack as many layers as we want.\n",
      "\n",
      "Stack 12 of these → BERT\n",
      "Stack 96 of these → GPT-3\n",
      "\n",
      "Each additional layer refines the representations,\n",
      "building increasingly sophisticated understanding of the input.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PUTTING IT ALL TOGETHER: THE TRANSFORMER LAYER\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "This is where everything we've learned comes together.\n",
    "\n",
    "A single Transformer layer combines:\n",
    "\n",
    "    ┌─────────────────────────────────────────────────────────────┐\n",
    "    │  Input (seq_len, d_model)                                   │\n",
    "    │       ↓                                                     │\n",
    "    │  ┌─────────────────────────────────────────┐                │\n",
    "    │  │  Multi-Head Self-Attention              │                │\n",
    "    │  │  \"Look at all positions, focus on       │                │\n",
    "    │  │   what's relevant from each\"            │                │\n",
    "    │  └─────────────────────────────────────────┘                │\n",
    "    │       ↓                                                     │\n",
    "    │  Add & Norm  (residual + layer norm)                        │\n",
    "    │       ↓                                                     │\n",
    "    │  ┌─────────────────────────────────────────┐                │\n",
    "    │  │  Feed-Forward Network                   │                │\n",
    "    │  │  \"Process each position independently\"  │                │\n",
    "    │  └─────────────────────────────────────────┘                │\n",
    "    │       ↓                                                     │\n",
    "    │  Add & Norm  (residual + layer norm)                        │\n",
    "    │       ↓                                                     │\n",
    "    │  Output (seq_len, d_model) — same shape!                    │\n",
    "    └─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "KEY INSIGHTS:\n",
    "  - Input and output have the SAME shape → can stack many layers\n",
    "  - Attention: positions communicate with each other\n",
    "  - Feed-forward: each position processed independently\n",
    "  - Residual connections: gradients flow freely (no vanishing!)\n",
    "  - Layer norm: keeps activations stable\n",
    "\n",
    "Real transformers stack 6-96 of these layers:\n",
    "  - BERT-base: 12 layers\n",
    "  - GPT-3: 96 layers\n",
    "  - Each layer refines the representations further\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "class TransformerLayer:\n",
    "    \"\"\"\n",
    "    A single Transformer encoder layer.\n",
    "    \n",
    "    Components:\n",
    "    1. Multi-head self-attention (look at all positions)\n",
    "    2. Residual connection + Layer normalization\n",
    "    3. Feed-forward network (process each position)\n",
    "    4. Residual connection + Layer normalization\n",
    "    \n",
    "    The full Transformer stacks N of these layers (typically 6-12).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        d_model : int\n",
    "            Model dimension (embedding size)\n",
    "        num_heads : int  \n",
    "            Number of attention heads\n",
    "        d_ff : int\n",
    "            Feed-forward hidden dimension\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        \n",
    "        # Initialize weights (simplified - real implementation uses proper init)\n",
    "        np.random.seed(42)\n",
    "        self.W_Q = np.random.randn(d_model, d_model) * 0.02\n",
    "        self.W_K = np.random.randn(d_model, d_model) * 0.02\n",
    "        self.W_V = np.random.randn(d_model, d_model) * 0.02\n",
    "        self.W_O = np.random.randn(d_model, d_model) * 0.02\n",
    "        \n",
    "        self.W_ff1 = np.random.randn(d_model, d_ff) * 0.02\n",
    "        self.W_ff2 = np.random.randn(d_ff, d_model) * 0.02\n",
    "    \n",
    "    def self_attention(self, x):\n",
    "        \"\"\"Multi-head self-attention.\"\"\"\n",
    "        Q = np.dot(x, self.W_Q)\n",
    "        K = np.dot(x, self.W_K)\n",
    "        V = np.dot(x, self.W_V)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        d_k = self.d_model // self.num_heads\n",
    "        scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "        weights = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
    "        weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        attended = np.dot(weights, V)\n",
    "        output = np.dot(attended, self.W_O)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "        hidden = np.maximum(0, np.dot(x, self.W_ff1))  # ReLU\n",
    "        output = np.dot(hidden, self.W_ff2)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, x, verbose=False):\n",
    "        \"\"\"\n",
    "        Forward pass through one transformer layer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy array, shape (seq_len, d_model)\n",
    "            Input embeddings\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        output : numpy array, shape (seq_len, d_model)\n",
    "            Processed embeddings\n",
    "        \"\"\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nTransformer Layer Forward Pass\")\n",
    "            print(\"=\" * 50)\n",
    "        \n",
    "        # Sub-layer 1: Self-attention\n",
    "        if verbose:\n",
    "            print(\"1. Multi-head self-attention...\")\n",
    "        attention_output = self.self_attention(x)\n",
    "        \n",
    "        # Add & Norm\n",
    "        x = layer_norm(x + attention_output)\n",
    "        if verbose:\n",
    "            print(\"   + Residual connection + Layer Norm\")\n",
    "        \n",
    "        # Sub-layer 2: Feed-forward\n",
    "        if verbose:\n",
    "            print(\"2. Feed-forward network...\")\n",
    "        ff_output = self.feed_forward(x)\n",
    "        \n",
    "        # Add & Norm\n",
    "        output = layer_norm(x + ff_output)\n",
    "        if verbose:\n",
    "            print(\"   + Residual connection + Layer Norm\")\n",
    "            print(f\"   Output shape: {output.shape}\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# DEMONSTRATION: Full Transformer Layer\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPLETE TRANSFORMER LAYER DEMONSTRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a transformer layer\n",
    "layer = TransformerLayer(d_model=64, num_heads=8, d_ff=256)\n",
    "\n",
    "# Create input sequence (5 tokens, 64 dimensions each)\n",
    "x = np.random.randn(5, 64)\n",
    "\n",
    "print(f\"\\nInput shape: {x.shape} (5 tokens, 64 dimensions)\")\n",
    "\n",
    "# Process through transformer layer\n",
    "output = layer.forward(x, verbose=True)\n",
    "\n",
    "print(f\"\\nOutput shape: {output.shape}\")\n",
    "print(\"\"\"\n",
    "WHAT JUST HAPPENED:\n",
    "\n",
    "1. Self-attention looked at ALL positions and computed relevance weights\n",
    "2. Each position gathered information from relevant positions  \n",
    "3. Residual connection preserved the original signal\n",
    "4. Layer norm stabilized the activations\n",
    "5. Feed-forward processed each position independently\n",
    "6. Another residual + layer norm\n",
    "\n",
    "The output has the SAME SHAPE as input — this is crucial!\n",
    "It means we can stack as many layers as we want.\n",
    "\n",
    "Stack 12 of these → BERT\n",
    "Stack 96 of these → GPT-3\n",
    "\n",
    "Each additional layer refines the representations,\n",
    "building increasingly sophisticated understanding of the input.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d314bb78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
