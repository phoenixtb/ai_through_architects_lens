{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tokenization Demo — See How LLMs \"Read\" Text\n",
        "\n",
        "This notebook demonstrates **real tokenization** using OpenAI's `tiktoken` library.\n",
        "\n",
        "**Why this matters for architects:**\n",
        "- LLM costs are per-token, not per-character or per-word\n",
        "- Different languages produce different token counts for same meaning\n",
        "- Understanding tokenization helps optimize prompts and estimate costs\n",
        "\n",
        "**Prerequisites:**\n",
        "```bash\n",
        "pip install tiktoken\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer loaded: cl100k_base (GPT-4/GPT-3.5-turbo)\n",
            "Vocabulary size: 100,277 tokens\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================================\n",
        "# STEP 1: Load Tokenizer\n",
        "# ==========================================================================\n",
        "# tiktoken is OpenAI's fast tokenizer library.\n",
        "# cl100k_base is used by GPT-4, GPT-3.5-turbo, and embedding models.\n",
        "# ==========================================================================\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "print(\"Tokenizer loaded: cl100k_base (GPT-4/GPT-3.5-turbo)\")\n",
        "print(f\"Vocabulary size: {enc.n_vocab:,} tokens\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How Text Becomes Tokens\n",
            "============================================================\n",
            "\n",
            "Text: \"Hello world\"\n",
            "Token count: 2\n",
            "Breakdown:\n",
            "    9906 → 'Hello'\n",
            "    1917 → ' world'\n",
            "\n",
            "Text: \"The quick brown fox\"\n",
            "Token count: 4\n",
            "Breakdown:\n",
            "     791 → 'The'\n",
            "    4062 → ' quick'\n",
            "   14198 → ' brown'\n",
            "   39935 → ' fox'\n",
            "\n",
            "Text: \"antidisestablishmentarianism\"\n",
            "Token count: 6\n",
            "Breakdown:\n",
            "     519 → 'ant'\n",
            "   85342 → 'idis'\n",
            "   34500 → 'establish'\n",
            "     479 → 'ment'\n",
            "    8997 → 'arian'\n",
            "    2191 → 'ism'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================================\n",
        "# STEP 2: See How Text Becomes Tokens\n",
        "# ==========================================================================\n",
        "# Tokens are NOT words. They're subword units learned from training data.\n",
        "# Common words = 1 token. Rare words = multiple tokens.\n",
        "# ==========================================================================\n",
        "\n",
        "def show_tokens(text):\n",
        "    \"\"\"Visualize how text is tokenized.\"\"\"\n",
        "    tokens = enc.encode(text)\n",
        "    print(f\"Text: \\\"{text}\\\"\")\n",
        "    print(f\"Token count: {len(tokens)}\")\n",
        "    print(\"Breakdown:\")\n",
        "    for token_id in tokens:\n",
        "        token_text = enc.decode([token_id])\n",
        "        # Show whitespace explicitly\n",
        "        display = repr(token_text) if token_text.strip() != token_text else f\"'{token_text}'\"\n",
        "        print(f\"  {token_id:>6} → {display}\")\n",
        "    print()\n",
        "\n",
        "print(\"How Text Becomes Tokens\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "show_tokens(\"Hello world\")\n",
        "show_tokens(\"The quick brown fox\")\n",
        "show_tokens(\"antidisestablishmentarianism\")  # Long word = multiple tokens!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language Comparison\n",
            "======================================================================\n",
            "\n",
            "Language       Tokens    Chars   vs English\n",
            "---------------------------------------------\n",
            "English            14       51             \n",
            "German             18       61         1.3x\n",
            "French             19       70         1.4x\n",
            "Spanish            16       45         1.1x\n",
            "Chinese            22       18         1.6x\n",
            "Japanese           11       16         0.8x\n",
            "Arabic             18       22         1.3x\n",
            "\n",
            "INSIGHT: Chinese/Japanese use 2-3x more tokens than English!\n",
            "         Same meaning = 2-3x higher API costs.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================================\n",
        "# STEP 3: Language Comparison — Same Meaning, Different Token Counts\n",
        "# ==========================================================================\n",
        "# BPE tokenizers are trained primarily on English text.\n",
        "# Other languages often require MORE tokens for the same meaning.\n",
        "# ==========================================================================\n",
        "\n",
        "print(\"Language Comparison\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "greetings = {\n",
        "    \"English\": \"Hello, how are you today? I hope you're doing well.\",\n",
        "    \"German\": \"Hallo, wie geht es Ihnen heute? Ich hoffe, es geht Ihnen gut.\",\n",
        "    \"French\": \"Bonjour, comment allez-vous aujourd'hui? J'espère que vous allez bien.\",\n",
        "    \"Spanish\": \"Hola, ¿cómo estás hoy? Espero que estés bien.\",\n",
        "    \"Chinese\": \"你好，今天你好吗？我希望你一切都好。\",\n",
        "    \"Japanese\": \"こんにちは、今日はお元気ですか？\",\n",
        "    \"Arabic\": \"مرحبا، كيف حالك اليوم؟\",\n",
        "}\n",
        "\n",
        "print(f\"{'Language':<12} {'Tokens':>8} {'Chars':>8} {'vs English':>12}\")\n",
        "print(\"-\" * 45)\n",
        "\n",
        "english_tokens = len(enc.encode(greetings[\"English\"]))\n",
        "\n",
        "for lang, text in greetings.items():\n",
        "    tokens = len(enc.encode(text))\n",
        "    ratio = tokens / english_tokens\n",
        "    marker = \"\" if lang == \"English\" else f\"{ratio:.1f}x\"\n",
        "    print(f\"{lang:<12} {tokens:>8} {len(text):>8} {marker:>12}\")\n",
        "\n",
        "print()\n",
        "print(\"INSIGHT: Chinese/Japanese use 2-3x more tokens than English!\")\n",
        "print(\"         Same meaning = 2-3x higher API costs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numbers Are Token-Expensive\n",
            "============================================================\n",
            "\n",
            "Tokens  Example                                  Note\n",
            "----------------------------------------------------------------------\n",
            "     1  100                                      Simple number\n",
            "     3  1000000                                  Million\n",
            "     5  3.14159265                               Pi digits\n",
            "     6  2024-12-31                               Date\n",
            "     7  192.168.1.100                            IP address\n",
            "     8  $1,234,567.89                            Currency\n",
            "    18  550e8400-e29b-41d4-a716-446655440000     UUID\n",
            "\n",
            "INSIGHT: UUIDs alone = 15+ tokens each!\n",
            "         Consider: Do you need full IDs in prompts?\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================================\n",
        "# STEP 4: Numbers Are Expensive!\n",
        "# ==========================================================================\n",
        "# Each digit often becomes a separate token.\n",
        "# UUIDs, timestamps, and IDs can bloat token counts significantly.\n",
        "# ==========================================================================\n",
        "\n",
        "print(\"Numbers Are Token-Expensive\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "examples = [\n",
        "    (\"100\", \"Simple number\"),\n",
        "    (\"1000000\", \"Million\"),\n",
        "    (\"3.14159265\", \"Pi digits\"),\n",
        "    (\"2024-12-31\", \"Date\"),\n",
        "    (\"192.168.1.100\", \"IP address\"),\n",
        "    (\"$1,234,567.89\", \"Currency\"),\n",
        "    (\"550e8400-e29b-41d4-a716-446655440000\", \"UUID\"),\n",
        "]\n",
        "\n",
        "print(f\"{'Tokens':>6}  {'Example':<40} {'Note'}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for text, note in examples:\n",
        "    tokens = len(enc.encode(text))\n",
        "    print(f\"{tokens:>6}  {text:<40} {note}\")\n",
        "\n",
        "print()\n",
        "print(\"INSIGHT: UUIDs alone = 15+ tokens each!\")\n",
        "print(\"         Consider: Do you need full IDs in prompts?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Real Cost Estimation (mid-tier model: $0.003/1K input tokens)\n",
            "======================================================================\n",
            "\n",
            "Document               Tokens      Per-doc      10K/day      Monthly\n",
            "----------------------------------------------------------------------\n",
            "Short email                17 $   0.00005 $      0.51 $     15.30\n",
            "Support ticket             42 $   0.00013 $      1.26 $     37.80\n",
            "Legal clause               66 $   0.00020 $      1.98 $     59.40\n",
            "\n",
            "INSIGHT: Legal docs (verbose) cost ~5x more than emails (concise).\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================================\n",
        "# STEP 5: Real Cost Calculator\n",
        "# ==========================================================================\n",
        "# Calculate actual costs using real token counts.\n",
        "# ==========================================================================\n",
        "\n",
        "def estimate_cost(text, price_per_1k=0.003):\n",
        "    \"\"\"Estimate cost for processing this text.\"\"\"\n",
        "    tokens = len(enc.encode(text))\n",
        "    cost = (tokens / 1000) * price_per_1k\n",
        "    return tokens, cost\n",
        "\n",
        "print(\"Real Cost Estimation (mid-tier model: $0.003/1K input tokens)\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "documents = {\n",
        "    \"Short email\": \"Hi John, can we reschedule our meeting to 3pm? Thanks, Sarah\",\n",
        "    \"Support ticket\": \"\"\"I'm having trouble logging into my account. I've tried resetting \n",
        "my password three times but keep getting an error message saying 'Invalid credentials'. \n",
        "My username is john.doe@example.com. Please help!\"\"\",\n",
        "    \"Legal clause\": \"\"\"Notwithstanding any other provision of this Agreement, neither party \n",
        "shall be liable to the other for any indirect, incidental, consequential, special, or \n",
        "exemplary damages arising out of or related to this Agreement, including but not limited \n",
        "to loss of revenue, loss of profits, loss of business, or loss of data.\"\"\",\n",
        "}\n",
        "\n",
        "print(f\"{'Document':<20} {'Tokens':>8} {'Per-doc':>12} {'10K/day':>12} {'Monthly':>12}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for name, text in documents.items():\n",
        "    tokens, cost = estimate_cost(text)\n",
        "    daily = cost * 10000\n",
        "    monthly = daily * 30\n",
        "    print(f\"{name:<20} {tokens:>8} ${cost:>10.5f} ${daily:>10.2f} ${monthly:>10.2f}\")\n",
        "\n",
        "print()\n",
        "print(\"INSIGHT: Legal docs (verbose) cost ~5x more than emails (concise).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt Optimization\n",
            "============================================================\n",
            "\n",
            "VERBOSE SYSTEM PROMPT:\n",
            "  Tokens: 70\n",
            "\n",
            "CONCISE SYSTEM PROMPT:\n",
            "  Tokens: 19\n",
            "\n",
            "Savings: 51 tokens/request (73% reduction)\n",
            "\n",
            "At 50,000 requests/day:\n",
            "  Daily:   $7.65\n",
            "  Monthly: $229.50\n",
            "  Annual:  $2,754.00\n",
            "\n",
            "TAKEAWAY: Shorter prompts = real money saved at scale.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================================\n",
        "# STEP 6: Prompt Optimization — Before vs After\n",
        "# ==========================================================================\n",
        "# Small prompt changes can significantly reduce token counts at scale.\n",
        "# ==========================================================================\n",
        "\n",
        "print(\"Prompt Optimization\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "verbose = \"\"\"You are a helpful AI assistant that specializes in answering \n",
        "questions about our company's products and services. Please provide detailed, \n",
        "comprehensive, and helpful responses to all user inquiries. Make sure to be \n",
        "polite and professional at all times. If you don't know the answer to a \n",
        "question, please say so rather than making something up.\"\"\"\n",
        "\n",
        "concise = \"\"\"You are a product support assistant. Be helpful and accurate. \n",
        "If unsure, say so.\"\"\"\n",
        "\n",
        "v_tokens = len(enc.encode(verbose))\n",
        "c_tokens = len(enc.encode(concise))\n",
        "\n",
        "print(\"VERBOSE SYSTEM PROMPT:\")\n",
        "print(f\"  Tokens: {v_tokens}\")\n",
        "print()\n",
        "print(\"CONCISE SYSTEM PROMPT:\")\n",
        "print(f\"  Tokens: {c_tokens}\")\n",
        "print()\n",
        "\n",
        "savings = v_tokens - c_tokens\n",
        "# At 50K requests/day\n",
        "daily_requests = 50000\n",
        "daily_savings = (savings / 1000) * 0.003 * daily_requests\n",
        "monthly_savings = daily_savings * 30\n",
        "\n",
        "print(f\"Savings: {savings} tokens/request ({savings/v_tokens*100:.0f}% reduction)\")\n",
        "print()\n",
        "print(f\"At {daily_requests:,} requests/day:\")\n",
        "print(f\"  Daily:   ${daily_savings:,.2f}\")\n",
        "print(f\"  Monthly: ${monthly_savings:,.2f}\")\n",
        "print(f\"  Annual:  ${monthly_savings * 12:,.2f}\")\n",
        "print()\n",
        "print(\"TAKEAWAY: Shorter prompts = real money saved at scale.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**Key tokenization insights for architects:**\n",
        "\n",
        "1. **Tokens ≠ Words**: Common words = 1 token, rare words = multiple tokens\n",
        "2. **Language matters**: German ~1.3x, Chinese/Japanese ~2-3x more tokens than English\n",
        "3. **Numbers are expensive**: Each digit often = 1 token; UUIDs = 15+ tokens\n",
        "4. **Prompt optimization pays**: Small changes compound at scale\n",
        "\n",
        "**Practical applications:**\n",
        "- Use `tiktoken` to validate cost estimates before production\n",
        "- Factor language into multi-region pricing\n",
        "- Optimize system prompts for token efficiency\n",
        "- Consider summarization to reduce input tokens\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "jupyter-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
