{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c9c3b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Computation Scaling\n",
      "======================================================================\n",
      "\n",
      "Self-attention computes scores between EVERY pair of positions.\n",
      "This creates an n × n attention matrix.\n",
      "\n",
      "Sequence Length → Attention Matrix Size → Relative Cost\n",
      "----------------------------------------------------------------------\n",
      "     512 tokens →     262K scores →      1.0× baseline cost\n",
      "    1024 tokens →     1.0M scores →      4.0× baseline cost\n",
      "    2048 tokens →     4.2M scores →     16.0× baseline cost\n",
      "    4096 tokens →    16.8M scores →     64.0× baseline cost\n",
      "    8192 tokens →    67.1M scores →    256.0× baseline cost\n",
      "   16384 tokens →   268.4M scores →   1024.0× baseline cost\n",
      "   32768 tokens →     1.1B scores →   4096.0× baseline cost\n",
      "  131072 tokens →    17.2B scores →  65536.0× baseline cost\n",
      "\n",
      "Key insight: Doubling sequence length QUADRUPLES computation!\n",
      "\n",
      "This is why 'just use a bigger context window' is rarely the answer.\n",
      "A 128K context window costs 256× more than a 8K window for attention alone.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def demonstrate_attention_scaling():\n",
    "    \"\"\"\n",
    "    Show how attention computation scales with sequence length.\n",
    "    \n",
    "    This is the fundamental constraint that shapes LLM architecture decisions.\n",
    "    Understanding this helps you make informed choices about:\n",
    "    - Context window sizing\n",
    "    - Chunking strategies\n",
    "    - Cost optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Attention Computation Scaling\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(\"Self-attention computes scores between EVERY pair of positions.\")\n",
    "    print(\"This creates an n × n attention matrix.\")\n",
    "    print()\n",
    "    \n",
    "    # Simulate different sequence lengths\n",
    "    sequence_lengths = [512, 1024, 2048, 4096, 8192, 16384, 32768, 131072]\n",
    "    \n",
    "    print(\"Sequence Length → Attention Matrix Size → Relative Cost\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    base_cost = 512 * 512  # Baseline: 512 tokens\n",
    "    \n",
    "    for n in sequence_lengths:\n",
    "        matrix_size = n * n\n",
    "        relative_cost = matrix_size / base_cost\n",
    "        \n",
    "        # Format large numbers readably\n",
    "        if matrix_size >= 1_000_000_000:\n",
    "            size_str = f\"{matrix_size / 1_000_000_000:.1f}B\"\n",
    "        elif matrix_size >= 1_000_000:\n",
    "            size_str = f\"{matrix_size / 1_000_000:.1f}M\"\n",
    "        else:\n",
    "            size_str = f\"{matrix_size / 1000:.0f}K\"\n",
    "        \n",
    "        print(f\"  {n:>6} tokens → {size_str:>8} scores → {relative_cost:>8.1f}× baseline cost\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Key insight: Doubling sequence length QUADRUPLES computation!\")\n",
    "    print()\n",
    "    print(\"This is why 'just use a bigger context window' is rarely the answer.\")\n",
    "    print(\"A 128K context window costs 256× more than a 8K window for attention alone.\")\n",
    "\n",
    "\n",
    "demonstrate_attention_scaling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72b984b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Window Budget Planning\n",
      "======================================================================\n",
      "\n",
      "THE CONTEXT WINDOW STORY\n",
      "─────────────────────────────────────────────────────────────────────\n",
      "\n",
      "Imagine your context window as a FIXED-SIZE ROOM (e.g., 8,000 tokens).\n",
      "Everything the model sees must fit in this room AT THE SAME TIME.\n",
      "\n",
      "Here's what needs to fit:\n",
      "\n",
      "    ┌─────────────────────────────────────────────────────────────┐\n",
      "    │                   8,000 TOKEN ROOM                          │\n",
      "    │                                                             │\n",
      "    │   ┌─────────────────┐                                       │\n",
      "    │   │ SYSTEM PROMPT   │  \"You are a helpful assistant...\"    │\n",
      "    │   │ (always there)  │  Instructions, persona, rules         │\n",
      "    │   └─────────────────┘                                       │\n",
      "    │                                                             │\n",
      "    │   ┌─────────────────┐                                       │\n",
      "    │   │ RETRIEVED DOCS  │  Your RAG content, the \"evidence\"     │\n",
      "    │   │ (retrieval      │  This is what we're budgeting FOR     │\n",
      "    │   │  budget)        │                                       │\n",
      "    │   └─────────────────┘                                       │\n",
      "    │                                                             │\n",
      "    │   ┌─────────────────┐                                       │\n",
      "    │   │ USER QUERY      │  \"What is the refund policy?\"         │\n",
      "    │   └─────────────────┘                                       │\n",
      "    │                                                             │\n",
      "    │   ┌─────────────────┐                                       │\n",
      "    │   │ MODEL RESPONSE  │  ← THIS GROWS TOKEN BY TOKEN!         │\n",
      "    │   │ (response       │  \"The\" → \"The refund\" → \"The refund   │\n",
      "    │   │  budget)        │   policy states...\" Each new token    │\n",
      "    │   └─────────────────┘   CONSUMES space in the room!         │\n",
      "    │                                                             │\n",
      "    └─────────────────────────────────────────────────────────────┘\n",
      "\n",
      "KEY INSIGHT: Why does response need a \"budget\"?\n",
      "───────────────────────────────────────────────\n",
      "During generation, the model produces ONE token at a time.\n",
      "Each new token becomes CONTEXT for generating the NEXT token.\n",
      "\n",
      "    Step 1: [System + Docs + Query] → Model outputs \"The\"\n",
      "    Step 2: [System + Docs + Query + \"The\"] → Model outputs \"refund\"\n",
      "    Step 3: [System + Docs + Query + \"The refund\"] → Model outputs \"policy\"\n",
      "    ...\n",
      "\n",
      "The response GROWS INSIDE the context window!\n",
      "If you want a 500-token response, you must RESERVE 500 tokens upfront.\n",
      "\n",
      "RETRIEVAL BUDGET = What's LEFT after reserving space for everything else.\n",
      "─────────────────────────────────────────────────────────────────────────\n",
      "This is why retrieval budget is calculated LAST — it's the remainder.\n",
      "\n",
      "Now let's do the math:\n",
      "\n",
      "Total context budget: 8000 tokens\n",
      "\n",
      "Allocation strategy:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. Fixed costs (same for every query):\n",
      "   System prompt:       200 tokens\n",
      "   Output format:        50 tokens\n",
      "   Safety buffer:       200 tokens\n",
      "   ─────────────────────────────\n",
      "   Total fixed:         450 tokens (5.6%)\n",
      "\n",
      "2. Variable costs (per-query):\n",
      "   User query (avg):    100 tokens\n",
      "   Response (avg):      500 tokens\n",
      "   ─────────────────────────────\n",
      "   Total variable:      600 tokens\n",
      "\n",
      "3. Retrieval budget (what's left for context):\n",
      "   Available for retrieved content: 6950 tokens\n",
      "   This is 86.9% of your total budget\n",
      "\n",
      "4. Chunk planning:\n",
      "   If each chunk is ~500 tokens\n",
      "   You can fit approximately 13 chunks\n",
      "\n",
      "======================================================================\n",
      "INSIGHT: In a typical 8K context RAG system, you only have room\n",
      "for 10-15 chunks. Quality of retrieval matters more than quantity!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def context_window_budget_calculator():\n",
    "    \"\"\"\n",
    "    Calculate how to allocate your context window budget.\n",
    "    \n",
    "    Context windows are precious real estate. Every token you add:\n",
    "    1. Costs money (per-token pricing)\n",
    "    2. Costs computation (attention scaling)\n",
    "    3. May not even help (lost-in-the-middle phenomenon)\n",
    "    \n",
    "    Smart allocation is essential.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Context Window Budget Planning\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # THE STORY: Why do we need to budget?\n",
    "    # =========================================================================\n",
    "    print(\"\"\"\n",
    "THE CONTEXT WINDOW STORY\n",
    "─────────────────────────────────────────────────────────────────────\n",
    "\n",
    "Imagine your context window as a FIXED-SIZE ROOM (e.g., 8,000 tokens).\n",
    "Everything the model sees must fit in this room AT THE SAME TIME.\n",
    "\n",
    "Here's what needs to fit:\n",
    "\n",
    "    ┌─────────────────────────────────────────────────────────────┐\n",
    "    │                   8,000 TOKEN ROOM                          │\n",
    "    │                                                             │\n",
    "    │   ┌─────────────────┐                                       │\n",
    "    │   │ SYSTEM PROMPT   │  \"You are a helpful assistant...\"    │\n",
    "    │   │ (always there)  │  Instructions, persona, rules         │\n",
    "    │   └─────────────────┘                                       │\n",
    "    │                                                             │\n",
    "    │   ┌─────────────────┐                                       │\n",
    "    │   │ RETRIEVED DOCS  │  Your RAG content, the \"evidence\"     │\n",
    "    │   │ (retrieval      │  This is what we're budgeting FOR     │\n",
    "    │   │  budget)        │                                       │\n",
    "    │   └─────────────────┘                                       │\n",
    "    │                                                             │\n",
    "    │   ┌─────────────────┐                                       │\n",
    "    │   │ USER QUERY      │  \"What is the refund policy?\"         │\n",
    "    │   └─────────────────┘                                       │\n",
    "    │                                                             │\n",
    "    │   ┌─────────────────┐                                       │\n",
    "    │   │ MODEL RESPONSE  │  ← THIS GROWS TOKEN BY TOKEN!         │\n",
    "    │   │ (response       │  \"The\" → \"The refund\" → \"The refund   │\n",
    "    │   │  budget)        │   policy states...\" Each new token    │\n",
    "    │   └─────────────────┘   CONSUMES space in the room!         │\n",
    "    │                                                             │\n",
    "    └─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "KEY INSIGHT: Why does response need a \"budget\"?\n",
    "───────────────────────────────────────────────\n",
    "During generation, the model produces ONE token at a time.\n",
    "Each new token becomes CONTEXT for generating the NEXT token.\n",
    "\n",
    "    Step 1: [System + Docs + Query] → Model outputs \"The\"\n",
    "    Step 2: [System + Docs + Query + \"The\"] → Model outputs \"refund\"\n",
    "    Step 3: [System + Docs + Query + \"The refund\"] → Model outputs \"policy\"\n",
    "    ...\n",
    "\n",
    "The response GROWS INSIDE the context window!\n",
    "If you want a 500-token response, you must RESERVE 500 tokens upfront.\n",
    "\n",
    "RETRIEVAL BUDGET = What's LEFT after reserving space for everything else.\n",
    "─────────────────────────────────────────────────────────────────────────\n",
    "This is why retrieval budget is calculated LAST — it's the remainder.\n",
    "\n",
    "Now let's do the math:\n",
    "\"\"\")\n",
    "    \n",
    "    # Example: Planning for a RAG system\n",
    "    total_budget = 8000  # tokens\n",
    "    \n",
    "    print(f\"Total context budget: {total_budget} tokens\")\n",
    "    print(\"\\nAllocation strategy:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Fixed costs (these don't change per query)\n",
    "    system_prompt = 200      # Instructions to the model\n",
    "    output_format = 50       # Specifying expected output structure\n",
    "    safety_buffer = 200      # Room for model to \"think\" and respond\n",
    "    \n",
    "    fixed_cost = system_prompt + output_format + safety_buffer\n",
    "    print(f\"\\n1. Fixed costs (same for every query):\")\n",
    "    print(f\"   System prompt:     {system_prompt:>5} tokens\")\n",
    "    print(f\"   Output format:     {output_format:>5} tokens\")\n",
    "    print(f\"   Safety buffer:     {safety_buffer:>5} tokens\")\n",
    "    print(f\"   ─────────────────────────────\")\n",
    "    print(f\"   Total fixed:       {fixed_cost:>5} tokens ({fixed_cost/total_budget*100:.1f}%)\")\n",
    "    \n",
    "    # Variable costs (depend on the query)\n",
    "    remaining = total_budget - fixed_cost\n",
    "    \n",
    "    avg_query_length = 100   # User's question\n",
    "    avg_response_length = 500  # Expected model response\n",
    "    \n",
    "    variable_known = avg_query_length + avg_response_length\n",
    "    \n",
    "    print(f\"\\n2. Variable costs (per-query):\")\n",
    "    print(f\"   User query (avg):  {avg_query_length:>5} tokens\")\n",
    "    print(f\"   Response (avg):    {avg_response_length:>5} tokens\")\n",
    "    print(f\"   ─────────────────────────────\")\n",
    "    print(f\"   Total variable:    {variable_known:>5} tokens\")\n",
    "    \n",
    "    # What's left for retrieval?\n",
    "    retrieval_budget = remaining - variable_known\n",
    "    \n",
    "    print(f\"\\n3. Retrieval budget (what's left for context):\")\n",
    "    print(f\"   Available for retrieved content: {retrieval_budget} tokens\")\n",
    "    print(f\"   This is {retrieval_budget/total_budget*100:.1f}% of your total budget\")\n",
    "    \n",
    "    # Chunk planning\n",
    "    typical_chunk_size = 500  # tokens per chunk\n",
    "    max_chunks = retrieval_budget // typical_chunk_size\n",
    "    \n",
    "    print(f\"\\n4. Chunk planning:\")\n",
    "    print(f\"   If each chunk is ~{typical_chunk_size} tokens\")\n",
    "    print(f\"   You can fit approximately {max_chunks} chunks\")\n",
    "    print()\n",
    "    print(\"=\" * 70)\n",
    "    print(\"INSIGHT: In a typical 8K context RAG system, you only have room\")\n",
    "    print(\"for 10-15 chunks. Quality of retrieval matters more than quantity!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "context_window_budget_calculator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea146e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'Lost in the Middle' Phenomenon\n",
      "======================================================================\n",
      "\n",
      "Research Finding (Liu et al., 2023):\n",
      "When LLMs process long contexts, they DON'T attend equally to all\n",
      "positions. Instead, they exhibit a U-shaped attention pattern:\n",
      "\n",
      "  HIGH   │  ●                                         ●\n",
      "         │                                               \n",
      "  MEDIUM │       ●                               ●      \n",
      "         │                                               \n",
      "  LOW    │                       ●                       \n",
      "         └────────────────────────────────────────────────\n",
      "           Begin    Early    Middle    Late      End\n",
      "                        Position in Context\n",
      "\n",
      "Information at the beginning and end gets HIGH attention.\n",
      "Information in the middle often gets IGNORED — even if it's relevant!\n",
      "\n",
      "======================================================================\n",
      "WHY THIS MATTERS FOR RAG\n",
      "======================================================================\n",
      "\n",
      "In a typical RAG system, you retrieve chunks and order them by\n",
      "relevance score (most relevant first). But look what happens:\n",
      "\n",
      "  Standard Ordering (by score):\n",
      "  ─────────────────────────────────────────────────────────────\n",
      "  Position 1: Chunk A (score: 0.95, BEST)     → HIGH attention ✓\n",
      "  Position 2: Chunk B (score: 0.90, 2nd best) → MEDIUM attention\n",
      "  Position 3: Chunk C (score: 0.85, 3rd best) → LOW attention ← LOST!\n",
      "  Position 4: Chunk D (score: 0.80, 4th best) → MEDIUM attention\n",
      "  Position 5: Chunk E (score: 0.75, WORST)    → HIGH attention ✗ WASTED!\n",
      "\n",
      "  PROBLEM: Your 2nd best chunk gets pushed to low-attention zone,\n",
      "           while your WORST chunk gets high attention!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "SOLUTION: Strategic Chunk Placement\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  Optimized Ordering:\n",
      "  ─────────────────────────────────────────────────────────────\n",
      "  Position 1: Chunk A (BEST)        → HIGH attention ✓\n",
      "  Position 2: Chunk C (3rd best)    → MEDIUM attention\n",
      "  Position 3: Chunk E (WORST)       → LOW attention (who cares!)\n",
      "  Position 4: Chunk D (4th best)    → MEDIUM attention\n",
      "  Position 5: Chunk B (2nd BEST)    → HIGH attention ✓\n",
      "\n",
      "  RESULT: Your TOP 2 chunks now BOTH get high attention!\n",
      "          Weaker chunks are in the middle where attention is low anyway.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "IMPLICATIONS FOR RAG DESIGN:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. Don't just dump all retrieved chunks in relevance order\n",
      "   → Put the MOST relevant chunk FIRST\n",
      "   → Put the SECOND most relevant chunk LAST\n",
      "   → Less critical context goes in the middle\n",
      "\n",
      "2. Consider limiting total chunks\n",
      "   → 5 highly relevant chunks often beat 20 somewhat relevant ones\n",
      "   → The middle chunks might be ignored anyway\n",
      "\n",
      "3. Use strategic prompting\n",
      "   → Repeat key information at the end of your prompt\n",
      "   → 'Based on the context above, especially [key point]...'\n",
      "\n",
      "4. Test with target information at different positions\n",
      "   → Your evaluation suite should measure position sensitivity\n",
      "   → Check if answers change when you move key info to the middle\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "RESEARCH REFERENCE:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Liu et al. (2023) - 'Lost in the Middle: How Language Models Use Long Contexts'\n",
      "https://arxiv.org/abs/2307.03172\n",
      "\n",
      "Key findings from the paper:\n",
      "  • Effect observed in models from 7B to 70B+ parameters\n",
      "  • 7B models show primarily RECENCY bias (end > beginning)\n",
      "  • 13B+ models show full U-SHAPE (beginning & end > middle)\n",
      "  • Effect strongest when context fills 50% of model's window\n",
      "  • Tested with contexts from 4K to 16K+ tokens\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "WHY NO LIVE DEMO?\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Reproducing this effect in a lightweight notebook is challenging:\n",
      "  • Small models (<7B) don't reliably show the U-shaped pattern\n",
      "  • Effect requires long contexts (4K+ tokens) to manifest\n",
      "  • Simple retrieval tasks may not trigger position sensitivity\n",
      "  • Consistent reproduction needs 13B+ models and 8K+ contexts\n",
      "\n",
      "The strategic chunk reordering advice above is based on the cited\n",
      "research and is applicable regardless of whether you can reproduce\n",
      "the effect in a demo setting.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def lost_in_the_middle_explanation():\n",
    "    \"\"\"\n",
    "    Explain the 'lost in the middle' phenomenon and its RAG implications.\n",
    "    \n",
    "    Research by Liu et al. (2023) found that LLMs exhibit a U-shaped\n",
    "    attention pattern when processing long contexts:\n",
    "    - Beginning: HIGH attention (primacy effect)\n",
    "    - Middle: LOW attention (often overlooked!)\n",
    "    - End: HIGH attention (recency effect)\n",
    "    \n",
    "    This has critical implications for how we order retrieved chunks in RAG.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"The 'Lost in the Middle' Phenomenon\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(\"Research Finding (Liu et al., 2023):\")\n",
    "    print(\"When LLMs process long contexts, they DON'T attend equally to all\")\n",
    "    print(\"positions. Instead, they exhibit a U-shaped attention pattern:\")\n",
    "    print()\n",
    "    \n",
    "    # Visualize with ASCII art\n",
    "    print(\"  HIGH   │  ●                                         ●\")\n",
    "    print(\"         │                                               \")\n",
    "    print(\"  MEDIUM │       ●                               ●      \")\n",
    "    print(\"         │                                               \")\n",
    "    print(\"  LOW    │                       ●                       \")\n",
    "    print(\"         └────────────────────────────────────────────────\")\n",
    "    print(\"           Begin    Early    Middle    Late      End\")\n",
    "    print(\"                        Position in Context\")\n",
    "    print()\n",
    "    print(\"Information at the beginning and end gets HIGH attention.\")\n",
    "    print(\"Information in the middle often gets IGNORED — even if it's relevant!\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"WHY THIS MATTERS FOR RAG\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(\"In a typical RAG system, you retrieve chunks and order them by\")\n",
    "    print(\"relevance score (most relevant first). But look what happens:\")\n",
    "    print()\n",
    "    print(\"  Standard Ordering (by score):\")\n",
    "    print(\"  ─────────────────────────────────────────────────────────────\")\n",
    "    print(\"  Position 1: Chunk A (score: 0.95, BEST)     → HIGH attention ✓\")\n",
    "    print(\"  Position 2: Chunk B (score: 0.90, 2nd best) → MEDIUM attention\")\n",
    "    print(\"  Position 3: Chunk C (score: 0.85, 3rd best) → LOW attention ← LOST!\")\n",
    "    print(\"  Position 4: Chunk D (score: 0.80, 4th best) → MEDIUM attention\")\n",
    "    print(\"  Position 5: Chunk E (score: 0.75, WORST)    → HIGH attention ✗ WASTED!\")\n",
    "    print()\n",
    "    print(\"  PROBLEM: Your 2nd best chunk gets pushed to low-attention zone,\")\n",
    "    print(\"           while your WORST chunk gets high attention!\")\n",
    "    print()\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(\"SOLUTION: Strategic Chunk Placement\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"  Optimized Ordering:\")\n",
    "    print(\"  ─────────────────────────────────────────────────────────────\")\n",
    "    print(\"  Position 1: Chunk A (BEST)        → HIGH attention ✓\")\n",
    "    print(\"  Position 2: Chunk C (3rd best)    → MEDIUM attention\")\n",
    "    print(\"  Position 3: Chunk E (WORST)       → LOW attention (who cares!)\")\n",
    "    print(\"  Position 4: Chunk D (4th best)    → MEDIUM attention\")\n",
    "    print(\"  Position 5: Chunk B (2nd BEST)    → HIGH attention ✓\")\n",
    "    print()\n",
    "    print(\"  RESULT: Your TOP 2 chunks now BOTH get high attention!\")\n",
    "    print(\"          Weaker chunks are in the middle where attention is low anyway.\")\n",
    "    print()\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(\"IMPLICATIONS FOR RAG DESIGN:\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"1. Don't just dump all retrieved chunks in relevance order\")\n",
    "    print(\"   → Put the MOST relevant chunk FIRST\")\n",
    "    print(\"   → Put the SECOND most relevant chunk LAST\")\n",
    "    print(\"   → Less critical context goes in the middle\")\n",
    "    print()\n",
    "    print(\"2. Consider limiting total chunks\")\n",
    "    print(\"   → 5 highly relevant chunks often beat 20 somewhat relevant ones\")\n",
    "    print(\"   → The middle chunks might be ignored anyway\")\n",
    "    print()\n",
    "    print(\"3. Use strategic prompting\")\n",
    "    print(\"   → Repeat key information at the end of your prompt\")\n",
    "    print(\"   → 'Based on the context above, especially [key point]...'\")\n",
    "    print()\n",
    "    print(\"4. Test with target information at different positions\")\n",
    "    print(\"   → Your evaluation suite should measure position sensitivity\")\n",
    "    print(\"   → Check if answers change when you move key info to the middle\")\n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"RESEARCH REFERENCE:\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"Liu et al. (2023) - 'Lost in the Middle: How Language Models Use Long Contexts'\")\n",
    "    print(\"https://arxiv.org/abs/2307.03172\")\n",
    "    print()\n",
    "    print(\"Key findings from the paper:\")\n",
    "    print(\"  • Effect observed in models from 7B to 70B+ parameters\")\n",
    "    print(\"  • 7B models show primarily RECENCY bias (end > beginning)\")\n",
    "    print(\"  • 13B+ models show full U-SHAPE (beginning & end > middle)\")\n",
    "    print(\"  • Effect strongest when context fills 50% of model's window\")\n",
    "    print(\"  • Tested with contexts from 4K to 16K+ tokens\")\n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"WHY NO LIVE DEMO?\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"Reproducing this effect in a lightweight notebook is challenging:\")\n",
    "    print(\"  • Small models (<7B) don't reliably show the U-shaped pattern\")\n",
    "    print(\"  • Effect requires long contexts (4K+ tokens) to manifest\")\n",
    "    print(\"  • Simple retrieval tasks may not trigger position sensitivity\")\n",
    "    print(\"  • Consistent reproduction needs 13B+ models and 8K+ contexts\")\n",
    "    print()\n",
    "    print(\"The strategic chunk reordering advice above is based on the cited\")\n",
    "    print(\"research and is applicable regardless of whether you can reproduce\")\n",
    "    print(\"the effect in a demo setting.\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "\n",
    "lost_in_the_middle_explanation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d20c4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking Strategy Trade-offs\n",
      "======================================================================\n",
      "\n",
      "Scenario: Technical documentation (75,000 tokens)\n",
      "Available context budget: 6,000 tokens\n",
      "\n",
      "Strategy: Small chunks (200 tokens)\n",
      "--------------------------------------------------\n",
      "  Chunks that fit in context: 30\n",
      "  Document coverage per query: 8.0%\n",
      "  Advantages:\n",
      "    ✓ Fine-grained retrieval\n",
      "    ✓ Good for factoid questions\n",
      "  Disadvantages:\n",
      "    ✗ Loses paragraph context\n",
      "    ✗ May need many chunks\n",
      "    ✗ Higher storage\n",
      "\n",
      "Strategy: Medium chunks (500 tokens)\n",
      "--------------------------------------------------\n",
      "  Chunks that fit in context: 12\n",
      "  Document coverage per query: 8.0%\n",
      "  Advantages:\n",
      "    ✓ Balanced context\n",
      "    ✓ Good default choice\n",
      "  Disadvantages:\n",
      "    ✗ May split important sections\n",
      "\n",
      "Strategy: Large chunks (1000 tokens)\n",
      "--------------------------------------------------\n",
      "  Chunks that fit in context: 6\n",
      "  Document coverage per query: 8.0%\n",
      "  Advantages:\n",
      "    ✓ Preserves full context\n",
      "    ✓ Good for complex topics\n",
      "  Disadvantages:\n",
      "    ✗ Fewer chunks fit\n",
      "    ✗ May include irrelevant content\n",
      "\n",
      "Strategy: Semantic chunks (variable)\n",
      "--------------------------------------------------\n",
      "  Chunks that fit in context: 10\n",
      "  Document coverage per query: 8.0%\n",
      "  Advantages:\n",
      "    ✓ Respects document structure\n",
      "    ✓ Clean boundaries\n",
      "  Disadvantages:\n",
      "    ✗ Requires preprocessing\n",
      "    ✗ Variable sizes complicate batching\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDATION:\n",
      "Start with 400-600 token chunks with ~100 token overlap.\n",
      "Measure retrieval quality on YOUR data, then adjust.\n",
      "Different document types may need different strategies!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def chunking_strategy_comparison():\n",
    "    \"\"\"\n",
    "    Compare different chunking strategies and their trade-offs.\n",
    "    \n",
    "    Chunking is where the rubber meets the road for transformer economics.\n",
    "    Too big → can't fit many chunks, attention spreads thin\n",
    "    Too small → loses context, retrieval might miss relevant info\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Chunking Strategy Trade-offs\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    # Scenario: 100-page technical document, ~75,000 tokens\n",
    "    document_tokens = 75000\n",
    "    context_budget = 6000  # Available for retrieval after fixed costs\n",
    "    \n",
    "    print(f\"Scenario: Technical documentation ({document_tokens:,} tokens)\")\n",
    "    print(f\"Available context budget: {context_budget:,} tokens\")\n",
    "    print()\n",
    "    \n",
    "    strategies = [\n",
    "        {\n",
    "            \"name\": \"Small chunks (200 tokens)\",\n",
    "            \"chunk_size\": 200,\n",
    "            \"overlap\": 50,\n",
    "            \"pros\": [\"Fine-grained retrieval\", \"Good for factoid questions\"],\n",
    "            \"cons\": [\"Loses paragraph context\", \"May need many chunks\", \"Higher storage\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Medium chunks (500 tokens)\", \n",
    "            \"chunk_size\": 500,\n",
    "            \"overlap\": 100,\n",
    "            \"pros\": [\"Balanced context\", \"Good default choice\"],\n",
    "            \"cons\": [\"May split important sections\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Large chunks (1000 tokens)\",\n",
    "            \"chunk_size\": 1000,\n",
    "            \"overlap\": 200,\n",
    "            \"pros\": [\"Preserves full context\", \"Good for complex topics\"],\n",
    "            \"cons\": [\"Fewer chunks fit\", \"May include irrelevant content\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Semantic chunks (variable)\",\n",
    "            \"chunk_size\": 600,  # average\n",
    "            \"overlap\": 0,  # boundaries are semantic\n",
    "            \"pros\": [\"Respects document structure\", \"Clean boundaries\"],\n",
    "            \"cons\": [\"Requires preprocessing\", \"Variable sizes complicate batching\"]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"Strategy: {strategy['name']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        chunk_size = strategy[\"chunk_size\"]\n",
    "        num_chunks_possible = context_budget // chunk_size\n",
    "        coverage = (num_chunks_possible * chunk_size) / document_tokens * 100\n",
    "        \n",
    "        print(f\"  Chunks that fit in context: {num_chunks_possible}\")\n",
    "        print(f\"  Document coverage per query: {coverage:.1f}%\")\n",
    "        print(f\"  Advantages:\")\n",
    "        for pro in strategy[\"pros\"]:\n",
    "            print(f\"    ✓ {pro}\")\n",
    "        print(f\"  Disadvantages:\")\n",
    "        for con in strategy[\"cons\"]:\n",
    "            print(f\"    ✗ {con}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"RECOMMENDATION:\")\n",
    "    print(\"Start with 400-600 token chunks with ~100 token overlap.\")\n",
    "    print(\"Measure retrieval quality on YOUR data, then adjust.\")\n",
    "    print(\"Different document types may need different strategies!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "chunking_strategy_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a14c9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Cost Modeling\n",
      "======================================================================\n",
      "\n",
      "Usage Scenario:\n",
      "  Daily queries: 50,000\n",
      "  Average input tokens: 4,000\n",
      "  Average output tokens: 500\n",
      "\n",
      "Cost Comparison Across Model Tiers:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  Fast/Cheap Model:\n",
      "    Input:  €0.0005/1K tokens, Output: €0.0015/1K tokens\n",
      "    Daily cost:   €137.50\n",
      "    Monthly cost: €4,125.00\n",
      "    Annual cost:  €49,500.00\n",
      "\n",
      "  Balanced Model:\n",
      "    Input:  €0.003/1K tokens, Output: €0.015/1K tokens\n",
      "    Daily cost:   €975.00\n",
      "    Monthly cost: €29,250.00\n",
      "    Annual cost:  €351,000.00\n",
      "\n",
      "  Most Capable Model:\n",
      "    Input:  €0.01/1K tokens, Output: €0.03/1K tokens\n",
      "    Daily cost:   €2,750.00\n",
      "    Monthly cost: €82,500.00\n",
      "    Annual cost:  €990,000.00\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "OPTIMIZATION STRATEGIES:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. Prompt optimization\n",
      "   Reducing system prompt by 100 tokens saves:\n",
      "   €450.00/month on a tier-2 model\n",
      "\n",
      "2. Response length control\n",
      "   Reducing average response by 200 tokens saves:\n",
      "   €4,500.00/month (output tokens cost more!)\n",
      "\n",
      "3. Model routing\n",
      "   If 70% of queries can use tier-1, 30% need tier-2:\n",
      "   Monthly savings: €17,587.50\n",
      "\n",
      "4. Caching\n",
      "   If 20% of queries are repeat/similar:\n",
      "   Monthly savings: €5,850.00\n"
     ]
    }
   ],
   "source": [
    "def cost_modeling_example():\n",
    "    \"\"\"\n",
    "    Build a cost model for an LLM-powered system.\n",
    "    \n",
    "    This is the kind of analysis you'll do for architecture reviews\n",
    "    and budget planning. The numbers are illustrative—actual pricing\n",
    "    varies by provider and changes over time.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"LLM Cost Modeling\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    # Typical pricing (illustrative, in euros per 1000 tokens)\n",
    "    # These represent different model tiers\n",
    "    pricing = {\n",
    "        \"tier1_input\": 0.0005,    # Smaller/faster models\n",
    "        \"tier1_output\": 0.0015,\n",
    "        \"tier2_input\": 0.003,     # Mid-tier models\n",
    "        \"tier2_output\": 0.015,\n",
    "        \"tier3_input\": 0.01,      # Largest/most capable models\n",
    "        \"tier3_output\": 0.03,\n",
    "    }\n",
    "    \n",
    "    # Usage scenario\n",
    "    daily_queries = 50000\n",
    "    avg_input_tokens = 4000   # Context + query\n",
    "    avg_output_tokens = 500   # Response\n",
    "    \n",
    "    print(\"Usage Scenario:\")\n",
    "    print(f\"  Daily queries: {daily_queries:,}\")\n",
    "    print(f\"  Average input tokens: {avg_input_tokens:,}\")\n",
    "    print(f\"  Average output tokens: {avg_output_tokens:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Cost Comparison Across Model Tiers:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for tier in [\"tier1\", \"tier2\", \"tier3\"]:\n",
    "        input_price = pricing[f\"{tier}_input\"]\n",
    "        output_price = pricing[f\"{tier}_output\"]\n",
    "        \n",
    "        # Daily cost\n",
    "        daily_input_cost = (daily_queries * avg_input_tokens / 1000) * input_price\n",
    "        daily_output_cost = (daily_queries * avg_output_tokens / 1000) * output_price\n",
    "        daily_total = daily_input_cost + daily_output_cost\n",
    "        \n",
    "        # Monthly (30 days)\n",
    "        monthly_total = daily_total * 30\n",
    "        \n",
    "        # Annual\n",
    "        annual_total = monthly_total * 12\n",
    "        \n",
    "        tier_name = {\"tier1\": \"Fast/Cheap\", \"tier2\": \"Balanced\", \"tier3\": \"Most Capable\"}[tier]\n",
    "        \n",
    "        print(f\"\\n  {tier_name} Model:\")\n",
    "        print(f\"    Input:  €{input_price}/1K tokens, Output: €{output_price}/1K tokens\")\n",
    "        print(f\"    Daily cost:   €{daily_total:,.2f}\")\n",
    "        print(f\"    Monthly cost: €{monthly_total:,.2f}\")\n",
    "        print(f\"    Annual cost:  €{annual_total:,.2f}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"OPTIMIZATION STRATEGIES:\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"1. Prompt optimization\")\n",
    "    print(\"   Reducing system prompt by 100 tokens saves:\")\n",
    "    saved_per_query = 100 / 1000 * pricing[\"tier2_input\"]\n",
    "    saved_monthly = saved_per_query * daily_queries * 30\n",
    "    print(f\"   €{saved_monthly:,.2f}/month on a tier-2 model\")\n",
    "    print()\n",
    "    print(\"2. Response length control\")\n",
    "    print(\"   Reducing average response by 200 tokens saves:\")\n",
    "    saved_per_query = 200 / 1000 * pricing[\"tier2_output\"]\n",
    "    saved_monthly = saved_per_query * daily_queries * 30\n",
    "    print(f\"   €{saved_monthly:,.2f}/month (output tokens cost more!)\")\n",
    "    print()\n",
    "    print(\"3. Model routing\")\n",
    "    print(\"   If 70% of queries can use tier-1, 30% need tier-2:\")\n",
    "    tier1_queries = daily_queries * 0.7\n",
    "    tier2_queries = daily_queries * 0.3\n",
    "    blended_daily = (\n",
    "        (tier1_queries * avg_input_tokens / 1000) * pricing[\"tier1_input\"] +\n",
    "        (tier1_queries * avg_output_tokens / 1000) * pricing[\"tier1_output\"] +\n",
    "        (tier2_queries * avg_input_tokens / 1000) * pricing[\"tier2_input\"] +\n",
    "        (tier2_queries * avg_output_tokens / 1000) * pricing[\"tier2_output\"]\n",
    "    )\n",
    "    tier2_only_daily = (\n",
    "        (daily_queries * avg_input_tokens / 1000) * pricing[\"tier2_input\"] +\n",
    "        (daily_queries * avg_output_tokens / 1000) * pricing[\"tier2_output\"]\n",
    "    )\n",
    "    savings = (tier2_only_daily - blended_daily) * 30\n",
    "    print(f\"   Monthly savings: €{savings:,.2f}\")\n",
    "    print()\n",
    "    print(\"4. Caching\")\n",
    "    print(\"   If 20% of queries are repeat/similar:\")\n",
    "    cache_hit_rate = 0.20\n",
    "    queries_saved = daily_queries * cache_hit_rate\n",
    "    saved_daily = (\n",
    "        (queries_saved * avg_input_tokens / 1000) * pricing[\"tier2_input\"] +\n",
    "        (queries_saved * avg_output_tokens / 1000) * pricing[\"tier2_output\"]\n",
    "    )\n",
    "    print(f\"   Monthly savings: €{saved_daily * 30:,.2f}\")\n",
    "\n",
    "\n",
    "cost_modeling_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962bb26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Intuition\n",
      "======================================================================\n",
      "\n",
      "Embeddings map words/sentences to vectors where:\n",
      "  Similar meaning → Similar vectors → Small distance\n",
      "\n",
      "Cosine Similarities (1.0 = identical, 0.0 = unrelated):\n",
      "--------------------------------------------------\n",
      "  king       ↔ queen     : 0.835 █████████████████████████ (Related (royalty))\n",
      "  king       ↔ man       : 1.000 █████████████████████████████ (Related (male))\n",
      "  queen      ↔ woman     : 0.999 █████████████████████████████ (Related (female))\n",
      "  apple      ↔ orange    : 0.998 █████████████████████████████ (Related (fruits))\n",
      "  king       ↔ apple     : 0.324 █████████ (Unrelated)\n",
      "  computer   ↔ apple     : 0.447 █████████████ (Weakly related (tech company?))\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "THE FAMOUS EXAMPLE: king - man + woman ≈ queen\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  king - man + woman = [0.75 0.85 0.85 0.2 ]\n",
      "  Actual queen vector = [0.75 0.85 0.9  0.2 ]\n",
      "\n",
      "  Closest word to result: 'queen' (similarity: 1.000)\n",
      "\n",
      "This shows embeddings capture RELATIONSHIPS, not just similarity!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embedding_intuition():\n",
    "    \"\"\"\n",
    "    Build intuition for what embeddings capture.\n",
    "    \n",
    "    An embedding is a vector (list of numbers) that represents meaning.\n",
    "    Similar meanings → Similar vectors → Close in space\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Embedding Intuition\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(\"Embeddings map words/sentences to vectors where:\")\n",
    "    print(\"  Similar meaning → Similar vectors → Small distance\")\n",
    "    print()\n",
    "    \n",
    "    # Simulated embeddings (real ones would be 384-3072 dimensions)\n",
    "    # Here we use 4 dimensions for illustration\n",
    "    embeddings = {\n",
    "        \"king\": np.array([0.8, 0.9, 0.1, 0.2]),\n",
    "        \"queen\": np.array([0.75, 0.85, 0.9, 0.2]),\n",
    "        \"man\": np.array([0.7, 0.8, 0.1, 0.15]),\n",
    "        \"woman\": np.array([0.65, 0.75, 0.85, 0.15]),\n",
    "        \"apple\": np.array([0.1, 0.1, 0.2, 0.9]),\n",
    "        \"orange\": np.array([0.15, 0.12, 0.18, 0.88]),\n",
    "        \"computer\": np.array([0.3, 0.2, 0.3, 0.1]),\n",
    "    }\n",
    "    \n",
    "    def cosine_similarity(a, b):\n",
    "        \"\"\"Cosine similarity: how aligned are two vectors?\"\"\"\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    print(\"Cosine Similarities (1.0 = identical, 0.0 = unrelated):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    pairs = [\n",
    "        (\"king\", \"queen\", \"Related (royalty)\"),\n",
    "        (\"king\", \"man\", \"Related (male)\"),\n",
    "        (\"queen\", \"woman\", \"Related (female)\"),\n",
    "        (\"apple\", \"orange\", \"Related (fruits)\"),\n",
    "        (\"king\", \"apple\", \"Unrelated\"),\n",
    "        (\"computer\", \"apple\", \"Weakly related (tech company?)\"),\n",
    "    ]\n",
    "    \n",
    "    for word1, word2, explanation in pairs:\n",
    "        sim = cosine_similarity(embeddings[word1], embeddings[word2])\n",
    "        bar = \"█\" * int(sim * 30)\n",
    "        print(f\"  {word1:10} ↔ {word2:10}: {sim:.3f} {bar} ({explanation})\")\n",
    "    \n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"THE FAMOUS EXAMPLE: king - man + woman ≈ queen\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Vector arithmetic\n",
    "    result = embeddings[\"king\"] - embeddings[\"man\"] + embeddings[\"woman\"]\n",
    "    \n",
    "    print(f\"\\n  king - man + woman = {result.round(2)}\")\n",
    "    print(f\"  Actual queen vector = {embeddings['queen']}\")\n",
    "    \n",
    "    # Find closest embedding to result\n",
    "    best_match = None\n",
    "    best_sim = -1\n",
    "    for word, vec in embeddings.items():\n",
    "        if word not in [\"king\", \"man\", \"woman\"]:\n",
    "            sim = cosine_similarity(result, vec)\n",
    "            if sim > best_sim:\n",
    "                best_sim = sim\n",
    "                best_match = word\n",
    "    \n",
    "    print(f\"\\n  Closest word to result: '{best_match}' (similarity: {best_sim:.3f})\")\n",
    "    print()\n",
    "    print(\"This shows embeddings capture RELATIONSHIPS, not just similarity!\")\n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"HANDS-ON: Run 1A/embedding_demo.ipynb for REAL embeddings demo\")\n",
    "    print(\"  Uses all-MiniLM-L6-v2 (80MB) to show:\")\n",
    "    print(\"  - Actual semantic similarity\")\n",
    "    print(\"  - Negation blindness in action\")\n",
    "    print(\"  - Entity confusion problem\")\n",
    "    print(\"  - Simple semantic search\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "\n",
    "embedding_intuition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3101f9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How Embedding Models Learn\n",
      "======================================================================\n",
      "\n",
      "Contrastive learning trains the model with pairs:\n",
      "\n",
      "Example 1: Password questions should cluster together\n",
      "--------------------------------------------------\n",
      "  Anchor:   \"How do I reset my password?\"\n",
      "  Positive: \"I forgot my password, how can I recover it?\" → PULL CLOSE\n",
      "  Negative: \"What are your business hours?\" → PUSH APART\n",
      "\n",
      "Example 2: Financial statements should cluster\n",
      "--------------------------------------------------\n",
      "  Anchor:   \"The quarterly revenue exceeded expectations\"\n",
      "  Positive: \"Q3 earnings beat analyst forecasts\" → PULL CLOSE\n",
      "  Negative: \"The weather was sunny yesterday\" → PUSH APART\n",
      "\n",
      "Example 3: Technical content should cluster\n",
      "--------------------------------------------------\n",
      "  Anchor:   \"Machine learning requires large datasets\"\n",
      "  Positive: \"ML models need lots of training data\" → PULL CLOSE\n",
      "  Negative: \"I enjoy hiking in the mountains\" → PUSH APART\n",
      "\n",
      "======================================================================\n",
      "After training on millions of such pairs, the model learns\n",
      "a general notion of 'semantic similarity' that transfers\n",
      "to sentences it has never seen before.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "def contrastive_training_intuition():\n",
    "    \"\"\"\n",
    "    Explain how embedding models learn through contrastive training.\n",
    "    \n",
    "    The model sees pairs of sentences and learns:\n",
    "    - Similar sentences → pull embeddings together\n",
    "    - Different sentences → push embeddings apart\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"How Embedding Models Learn\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(\"Contrastive learning trains the model with pairs:\")\n",
    "    print()\n",
    "    \n",
    "    training_examples = [\n",
    "        {\n",
    "            \"anchor\": \"How do I reset my password?\",\n",
    "            \"positive\": \"I forgot my password, how can I recover it?\",\n",
    "            \"negative\": \"What are your business hours?\",\n",
    "            \"explanation\": \"Password questions should cluster together\"\n",
    "        },\n",
    "        {\n",
    "            \"anchor\": \"The quarterly revenue exceeded expectations\",\n",
    "            \"positive\": \"Q3 earnings beat analyst forecasts\",\n",
    "            \"negative\": \"The weather was sunny yesterday\",\n",
    "            \"explanation\": \"Financial statements should cluster\"\n",
    "        },\n",
    "        {\n",
    "            \"anchor\": \"Machine learning requires large datasets\",\n",
    "            \"positive\": \"ML models need lots of training data\",\n",
    "            \"negative\": \"I enjoy hiking in the mountains\",\n",
    "            \"explanation\": \"Technical content should cluster\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, example in enumerate(training_examples, 1):\n",
    "        print(f\"Example {i}: {example['explanation']}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"  Anchor:   \\\"{example['anchor']}\\\"\")\n",
    "        print(f\"  Positive: \\\"{example['positive']}\\\" → PULL CLOSE\")\n",
    "        print(f\"  Negative: \\\"{example['negative']}\\\" → PUSH APART\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"After training on millions of such pairs, the model learns\")\n",
    "    print(\"a general notion of 'semantic similarity' that transfers\")\n",
    "    print(\"to sentences it has never seen before.\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "contrastive_training_intuition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2078ed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Failure Mode: Negation Blindness\n",
      "======================================================================\n",
      "\n",
      "These pairs have OPPOSITE meanings but similar embeddings:\n",
      "\n",
      "  \"The system is secure\"\n",
      "  \"The system is not secure\"\n",
      "  → Real similarity: 0.70-0.90 (problematically HIGH!)\n",
      "\n",
      "  \"The patient is healthy\"\n",
      "  \"The patient is not healthy\"\n",
      "  → Real similarity: 0.70-0.90 (problematically HIGH!)\n",
      "\n",
      "  \"Payment was successful\"\n",
      "  \"Payment was not successful\"\n",
      "  → Real similarity: 0.70-0.90 (problematically HIGH!)\n",
      "\n",
      "  \"The test passed\"\n",
      "  \"The test failed\"\n",
      "  → Real similarity: 0.70-0.90 (problematically HIGH!)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "WHY THIS HAPPENS:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Embeddings capture word CO-OCCURRENCE patterns, not logical meaning.\n",
      "'The system is secure' and 'The system is not secure' share:\n",
      "  - Most of the same words\n",
      "  - Similar grammatical structure\n",
      "  - Similar topic (system security)\n",
      "\n",
      "The word 'not' is just one token among many, easily overwhelmed.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "MITIGATION STRATEGIES:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. Use hybrid search (combine embedding search with keyword matching)\n",
      "   → 'not' becomes an important keyword signal\n",
      "\n",
      "2. Use cross-encoder reranking\n",
      "   → Processes query AND document together, catches negation\n",
      "\n",
      "3. Add metadata or structured fields\n",
      "   → 'status: failed' vs 'status: passed' can be exact matched\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "HANDS-ON: Run 1A/embedding_demo.ipynb (STEP 5) to see real results!\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def negation_blindness_explanation():\n",
    "    \"\"\"\n",
    "    Explain how embeddings struggle with negation.\n",
    "    \n",
    "    This is a critical limitation for systems that need to\n",
    "    distinguish between a fact and its opposite.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Embedding Failure Mode: Negation Blindness\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    problematic_pairs = [\n",
    "        (\"The system is secure\", \"The system is not secure\"),\n",
    "        (\"The patient is healthy\", \"The patient is not healthy\"),\n",
    "        (\"Payment was successful\", \"Payment was not successful\"),\n",
    "        (\"The test passed\", \"The test failed\"),\n",
    "    ]\n",
    "    \n",
    "    print(\"These pairs have OPPOSITE meanings but similar embeddings:\")\n",
    "    print()\n",
    "    \n",
    "    for positive, negative in problematic_pairs:\n",
    "        print(f\"  \\\"{positive}\\\"\")\n",
    "        print(f\"  \\\"{negative}\\\"\")\n",
    "        print(f\"  → Real similarity: 0.70-0.90 (problematically HIGH!)\")\n",
    "        print()\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(\"WHY THIS HAPPENS:\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"Embeddings capture word CO-OCCURRENCE patterns, not logical meaning.\")\n",
    "    print(\"'The system is secure' and 'The system is not secure' share:\")\n",
    "    print(\"  - Most of the same words\")\n",
    "    print(\"  - Similar grammatical structure\")\n",
    "    print(\"  - Similar topic (system security)\")\n",
    "    print()\n",
    "    print(\"The word 'not' is just one token among many, easily overwhelmed.\")\n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"MITIGATION STRATEGIES:\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"1. Use hybrid search (combine embedding search with keyword matching)\")\n",
    "    print(\"   → 'not' becomes an important keyword signal\")\n",
    "    print()\n",
    "    print(\"2. Use cross-encoder reranking\")\n",
    "    print(\"   → Processes query AND document together, catches negation\")\n",
    "    print()\n",
    "    print(\"3. Add metadata or structured fields\")\n",
    "    print(\"   → 'status: failed' vs 'status: passed' can be exact matched\")\n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"HANDS-ON: Run 1A/embedding_demo.ipynb (STEP 5) to see real results!\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "\n",
    "negation_blindness_explanation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd7f7223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Failure Mode: Entity Confusion\n",
      "======================================================================\n",
      "\n",
      "Embeddings see 'database config' as similar regardless of WHICH database!\n",
      "\n",
      "Query: \"What is the price of the Enterprise plan?\"\n",
      "  ✓ Correct match: \"Enterprise plan costs €499/month\"\n",
      "  ✗ Wrong match:   \"Professional plan costs €199/month\"\n",
      "  Problem: Both are 'pricing plans' - embeddings see them as similar\n",
      "\n",
      "Query: \"How do I configure PostgreSQL?\"\n",
      "  ✓ Correct match: \"PostgreSQL configuration guide\"\n",
      "  ✗ Wrong match:   \"MySQL configuration guide\"\n",
      "  Problem: Both are 'database configuration' - high semantic similarity\n",
      "\n",
      "Query: \"John Smith's contact information\"\n",
      "  ✓ Correct match: \"John Smith: john.smith@example.com\"\n",
      "  ✗ Wrong match:   \"Jane Smith: jane.smith@example.com\"\n",
      "  Problem: Both are 'Smith contact info' - embeddings miss the first name\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "MITIGATION STRATEGIES:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. Hybrid search with exact matching\n",
      "   → 'Enterprise' as keyword boosts the right document\n",
      "\n",
      "2. Metadata filtering\n",
      "   → Filter by plan_type='enterprise' BEFORE semantic search\n",
      "\n",
      "3. Entity extraction + linking\n",
      "   → Recognize 'PostgreSQL' as specific entity, match exactly\n",
      "\n",
      "4. Query expansion\n",
      "   → Rewrite query to emphasize specific entities\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "HANDS-ON: Run 1A/embedding_demo.ipynb (STEP 6) to see real results!\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def entity_confusion_explanation():\n",
    "    \"\"\"\n",
    "    Explain how embeddings confuse different entities of the same type.\n",
    "    \n",
    "    This matters for enterprise systems dealing with specific products,\n",
    "    people, or technical terms.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Embedding Failure Mode: Entity Confusion\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(\"Embeddings see 'database config' as similar regardless of WHICH database!\")\n",
    "    print()\n",
    "    \n",
    "    confusing_pairs = [\n",
    "        {\n",
    "            \"query\": \"What is the price of the Enterprise plan?\",\n",
    "            \"good_match\": \"Enterprise plan costs €499/month\",\n",
    "            \"bad_match\": \"Professional plan costs €199/month\",\n",
    "            \"problem\": \"Both are 'pricing plans' - embeddings see them as similar\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"How do I configure PostgreSQL?\",\n",
    "            \"good_match\": \"PostgreSQL configuration guide\",\n",
    "            \"bad_match\": \"MySQL configuration guide\",\n",
    "            \"problem\": \"Both are 'database configuration' - high semantic similarity\"\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"John Smith's contact information\",\n",
    "            \"good_match\": \"John Smith: john.smith@example.com\",\n",
    "            \"bad_match\": \"Jane Smith: jane.smith@example.com\",\n",
    "            \"problem\": \"Both are 'Smith contact info' - embeddings miss the first name\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for pair in confusing_pairs:\n",
    "        print(f\"Query: \\\"{pair['query']}\\\"\")\n",
    "        print(f\"  ✓ Correct match: \\\"{pair['good_match']}\\\"\")\n",
    "        print(f\"  ✗ Wrong match:   \\\"{pair['bad_match']}\\\"\")\n",
    "        print(f\"  Problem: {pair['problem']}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(\"MITIGATION STRATEGIES:\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"1. Hybrid search with exact matching\")\n",
    "    print(\"   → 'Enterprise' as keyword boosts the right document\")\n",
    "    print()\n",
    "    print(\"2. Metadata filtering\")\n",
    "    print(\"   → Filter by plan_type='enterprise' BEFORE semantic search\")\n",
    "    print()\n",
    "    print(\"3. Entity extraction + linking\")\n",
    "    print(\"   → Recognize 'PostgreSQL' as specific entity, match exactly\")\n",
    "    print()\n",
    "    print(\"4. Query expansion\")\n",
    "    print(\"   → Rewrite query to emphasize specific entities\")\n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"HANDS-ON: Run 1A/embedding_demo.ipynb (STEP 6) to see real results!\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "\n",
    "entity_confusion_explanation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d2b8b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Failure Mode: Numerical Blindness\n",
      "======================================================================\n",
      "\n",
      "Embeddings treat numbers as tokens, not as quantities.\n",
      "\n",
      "These should be very different, but embeddings often confuse them:\n",
      "\n",
      "  \"Price: €100\"\n",
      "  \"Price: €10,000\"\n",
      "  → 100× price difference!\n",
      "\n",
      "  \"Deadline: January 5\"\n",
      "  \"Deadline: January 25\"\n",
      "  → 20 days difference!\n",
      "\n",
      "  \"Server has 99.9% uptime\"\n",
      "  \"Server has 9.9% uptime\"\n",
      "  → Massive reliability difference!\n",
      "\n",
      "  \"2023 annual report\"\n",
      "  \"2013 annual report\"\n",
      "  → 10 year old data!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "WHY THIS HAPPENS:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "To embeddings, '100' and '10,000' are just different tokens.\n",
      "There's no built-in understanding that 10,000 > 100.\n",
      "Tokenization makes it worse: '10,000' might become ['10', ',', '000']\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "MITIGATION STRATEGIES:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. Extract numbers into structured metadata\n",
      "   → price_euros: 100 vs price_euros: 10000\n",
      "   → Use numeric comparison, not embedding similarity\n",
      "\n",
      "2. Normalize numerical expressions\n",
      "   → Convert 'last year', '2023', 'FY23' to consistent format\n",
      "\n",
      "3. Use hybrid retrieval with range queries\n",
      "   → 'price < 500' as a filter before semantic search\n",
      "\n",
      "4. Let the LLM do numerical reasoning\n",
      "   → Retrieve candidates broadly, let LLM compare numbers\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "HANDS-ON: Run 1A/embedding_demo.ipynb (STEP 9) to see real results!\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def numerical_reasoning_explanation():\n",
    "    \"\"\"\n",
    "    Explain how embeddings don't understand numbers mathematically.\n",
    "    \n",
    "    Critical for systems dealing with quantities, prices, dates, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Embedding Failure Mode: Numerical Blindness\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    print(\"Embeddings treat numbers as tokens, not as quantities.\")\n",
    "    print()\n",
    "    \n",
    "    print(\"These should be very different, but embeddings often confuse them:\")\n",
    "    print()\n",
    "    \n",
    "    examples = [\n",
    "        (\"Price: €100\", \"Price: €10,000\", \"100× price difference!\"),\n",
    "        (\"Deadline: January 5\", \"Deadline: January 25\", \"20 days difference!\"),\n",
    "        (\"Server has 99.9% uptime\", \"Server has 9.9% uptime\", \"Massive reliability difference!\"),\n",
    "        (\"2023 annual report\", \"2013 annual report\", \"10 year old data!\"),\n",
    "    ]\n",
    "    \n",
    "    for text1, text2, severity in examples:\n",
    "        print(f\"  \\\"{text1}\\\"\")\n",
    "        print(f\"  \\\"{text2}\\\"\")\n",
    "        print(f\"  → {severity}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(\"WHY THIS HAPPENS:\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"To embeddings, '100' and '10,000' are just different tokens.\")\n",
    "    print(\"There's no built-in understanding that 10,000 > 100.\")\n",
    "    print(\"Tokenization makes it worse: '10,000' might become ['10', ',', '000']\")\n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"MITIGATION STRATEGIES:\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"1. Extract numbers into structured metadata\")\n",
    "    print(\"   → price_euros: 100 vs price_euros: 10000\")\n",
    "    print(\"   → Use numeric comparison, not embedding similarity\")\n",
    "    print()\n",
    "    print(\"2. Normalize numerical expressions\")\n",
    "    print(\"   → Convert 'last year', '2023', 'FY23' to consistent format\")\n",
    "    print()\n",
    "    print(\"3. Use hybrid retrieval with range queries\")\n",
    "    print(\"   → 'price < 500' as a filter before semantic search\")\n",
    "    print()\n",
    "    print(\"4. Let the LLM do numerical reasoning\")\n",
    "    print(\"   → Retrieve candidates broadly, let LLM compare numbers\")\n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"HANDS-ON: Run 1A/embedding_demo.ipynb (STEP 9) to see real results!\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "\n",
    "numerical_reasoning_explanation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f487b6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Dimensionality Trade-offs\n",
      "======================================================================\n",
      "\n",
      "Scenario: 1,000,000 documents\n",
      "\n",
      "Dimension → Storage → Quality Notes\n",
      "----------------------------------------------------------------------\n",
      "   384d →   1.4 GB → Good for English, basic similarity\n",
      "   768d →   2.9 GB → Better nuance, good multilingual\n",
      "  1024d →   3.8 GB → Strong multilingual, complex queries\n",
      "  1536d →   5.7 GB → High quality, diminishing returns\n",
      "  3072d →  11.4 GB → Maximum quality, high cost\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "KEY DECISION FACTORS:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. Multilingual requirements\n",
      "   → Multiple languages need more dimensions (768-1024 minimum)\n",
      "\n",
      "2. Query complexity\n",
      "   → Simple factoid lookup: 384-768 sufficient\n",
      "   → Complex semantic queries: 1024+ helps\n",
      "\n",
      "3. Scale constraints\n",
      "   → 10M+ documents: dimension reduction matters\n",
      "   → Consider quantization (float32 → int8)\n",
      "\n",
      "4. Accuracy requirements\n",
      "   → Always benchmark on YOUR data\n",
      "   → 384-dim domain-specific can beat 1536-dim general\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "PRACTICAL RECOMMENDATION:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Start with 768-1024 dimensions (good balance).\n",
      "Benchmark retrieval quality on a representative test set.\n",
      "Only increase dimensions if you see quality improvements\n",
      "that justify the storage and latency costs.\n"
     ]
    }
   ],
   "source": [
    "def embedding_dimension_tradeoffs():\n",
    "    \"\"\"\n",
    "    Analyze the trade-offs of different embedding dimensions.\n",
    "    \n",
    "    More dimensions = more expressive power, but also more:\n",
    "    - Storage cost\n",
    "    - Search latency  \n",
    "    - Memory requirements\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Embedding Dimensionality Trade-offs\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    # Different embedding sizes and their characteristics\n",
    "    dimensions = [384, 768, 1024, 1536, 3072]\n",
    "    \n",
    "    # Baseline: 1 million documents\n",
    "    num_documents = 1_000_000\n",
    "    bytes_per_float = 4  # float32\n",
    "    \n",
    "    print(f\"Scenario: {num_documents:,} documents\")\n",
    "    print()\n",
    "    print(\"Dimension → Storage → Quality Notes\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for dim in dimensions:\n",
    "        # Calculate storage\n",
    "        storage_bytes = num_documents * dim * bytes_per_float\n",
    "        storage_gb = storage_bytes / (1024 ** 3)\n",
    "        \n",
    "        # Relative latency (simplified: scales with dim)\n",
    "        relative_latency = dim / 384\n",
    "        \n",
    "        # Quality notes\n",
    "        if dim <= 384:\n",
    "            quality = \"Good for English, basic similarity\"\n",
    "        elif dim <= 768:\n",
    "            quality = \"Better nuance, good multilingual\"\n",
    "        elif dim <= 1024:\n",
    "            quality = \"Strong multilingual, complex queries\"\n",
    "        elif dim <= 1536:\n",
    "            quality = \"High quality, diminishing returns\"\n",
    "        else:\n",
    "            quality = \"Maximum quality, high cost\"\n",
    "        \n",
    "        print(f\"  {dim:4d}d → {storage_gb:5.1f} GB → {quality}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"KEY DECISION FACTORS:\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"1. Multilingual requirements\")\n",
    "    print(\"   → Multiple languages need more dimensions (768-1024 minimum)\")\n",
    "    print()\n",
    "    print(\"2. Query complexity\")\n",
    "    print(\"   → Simple factoid lookup: 384-768 sufficient\")\n",
    "    print(\"   → Complex semantic queries: 1024+ helps\")\n",
    "    print()\n",
    "    print(\"3. Scale constraints\")\n",
    "    print(\"   → 10M+ documents: dimension reduction matters\")\n",
    "    print(\"   → Consider quantization (float32 → int8)\")\n",
    "    print()\n",
    "    print(\"4. Accuracy requirements\")\n",
    "    print(\"   → Always benchmark on YOUR data\")\n",
    "    print(\"   → 384-dim domain-specific can beat 1536-dim general\")\n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"PRACTICAL RECOMMENDATION:\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"Start with 768-1024 dimensions (good balance).\")\n",
    "    print(\"Benchmark retrieval quality on a representative test set.\")\n",
    "    print(\"Only increase dimensions if you see quality improvements\")\n",
    "    print(\"that justify the storage and latency costs.\")\n",
    "\n",
    "\n",
    "embedding_dimension_tradeoffs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f18da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Search: Best of Both Worlds\n",
      "======================================================================\n",
      "\n",
      "Query: \"Enterprise plan API limits\"\n",
      "\n",
      "Dense Retrieval (Embeddings):\n",
      "  Captures semantic similarity\n",
      "  'API limits' matches 'rate limits' conceptually\n",
      "\n",
      "Sparse Retrieval (Keywords/BM25):\n",
      "  Captures exact matches\n",
      "  'Enterprise' and 'plan' match exactly in doc 1\n",
      "\n",
      "Comparison of Retrieval Methods:\n",
      "----------------------------------------------------------------------\n",
      "Doc        Dense     Sparse   Combined\n",
      "----------------------------------------------------------------------\n",
      "1           0.89       0.95       0.92\n",
      "      The Enterprise plan includes unlimited API calls and priorit...\n",
      "2           0.72       0.60       0.66\n",
      "      Professional plan offers 10,000 API calls per month with ema...\n",
      "3           0.85       0.70       0.77\n",
      "      API rate limits can be increased by upgrading your subscript...\n",
      "4           0.65       0.80       0.73\n",
      "      Our enterprise customers receive dedicated account managemen...\n",
      "5           0.45       0.30       0.38\n",
      "      Contact support@example.com for billing questions about your...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Rankings (1 = best):\n",
      "  Dense only:    [1, 3, 2, 4, 5]\n",
      "  Sparse only:   [1, 4, 3, 2, 5]\n",
      "  Combined:      [1, 3, 4, 2, 5]\n",
      "\n",
      "The combined approach gets Document 1 at top, which is correct!\n",
      "Pure dense would rank Document 3 higher (semantic match on 'rate limits')\n",
      "but Document 1 is the actual answer about Enterprise plan.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "IMPLEMENTATION OPTIONS:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "1. Reciprocal Rank Fusion (RRF)\n",
      "   score = Σ 1/(k + rank_i) for each retrieval method\n",
      "   Simple, effective, no score normalization needed\n",
      "\n",
      "2. Weighted combination\n",
      "   score = α × dense_score + (1-α) × sparse_score\n",
      "   Requires score normalization\n",
      "\n",
      "3. Learned fusion\n",
      "   Train a model to combine scores\n",
      "   Most complex, potentially best quality\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "HANDS-ON: Run 1A/embedding_demo.ipynb (STEP 10) for real hybrid search!\n",
      "  Uses ChromaDB with metadata filtering — production-ready pattern.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def hybrid_search_explanation():\n",
    "    \"\"\"\n",
    "    Demonstrate hybrid search combining dense and sparse retrieval.\n",
    "    \n",
    "    This is the standard approach for production RAG systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Hybrid Search: Best of Both Worlds\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    # Simulated document corpus\n",
    "    documents = [\n",
    "        \"The Enterprise plan includes unlimited API calls and priority support.\",\n",
    "        \"Professional plan offers 10,000 API calls per month with email support.\",\n",
    "        \"API rate limits can be increased by upgrading your subscription plan.\",\n",
    "        \"Our enterprise customers receive dedicated account management.\",\n",
    "        \"Contact support@example.com for billing questions about your plan.\",\n",
    "    ]\n",
    "    \n",
    "    query = \"Enterprise plan API limits\"\n",
    "    \n",
    "    print(f\"Query: \\\"{query}\\\"\")\n",
    "    print()\n",
    "    \n",
    "    # Simulated dense (embedding) scores\n",
    "    # Based on semantic similarity\n",
    "    dense_scores = [0.89, 0.72, 0.85, 0.65, 0.45]\n",
    "    \n",
    "    # Simulated sparse (BM25/keyword) scores\n",
    "    # Based on word overlap\n",
    "    sparse_scores = [0.95, 0.60, 0.70, 0.80, 0.30]\n",
    "    \n",
    "    print(\"Dense Retrieval (Embeddings):\")\n",
    "    print(\"  Captures semantic similarity\")\n",
    "    print(\"  'API limits' matches 'rate limits' conceptually\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Sparse Retrieval (Keywords/BM25):\")\n",
    "    print(\"  Captures exact matches\")\n",
    "    print(\"  'Enterprise' and 'plan' match exactly in doc 1\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Comparison of Retrieval Methods:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Doc':<5} {'Dense':>10} {'Sparse':>10} {'Combined':>10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Combine using simple weighted average\n",
    "    # In practice, Reciprocal Rank Fusion (RRF) is often better\n",
    "    alpha = 0.5  # Weight for dense\n",
    "    combined_scores = [\n",
    "        alpha * d + (1 - alpha) * s \n",
    "        for d, s in zip(dense_scores, sparse_scores)\n",
    "    ]\n",
    "    \n",
    "    for i, (doc, dense, sparse, combined) in enumerate(zip(\n",
    "        documents, dense_scores, sparse_scores, combined_scores\n",
    "    )):\n",
    "        print(f\"{i+1:<5} {dense:>10.2f} {sparse:>10.2f} {combined:>10.2f}\")\n",
    "        print(f\"      {doc[:60]}...\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Show ranking differences\n",
    "    dense_rank = np.argsort(dense_scores)[::-1]\n",
    "    sparse_rank = np.argsort(sparse_scores)[::-1]\n",
    "    combined_rank = np.argsort(combined_scores)[::-1]\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "    print(\"Rankings (1 = best):\")\n",
    "    print(f\"  Dense only:    {[x+1 for x in dense_rank]}\")\n",
    "    print(f\"  Sparse only:   {[x+1 for x in sparse_rank]}\")\n",
    "    print(f\"  Combined:      {[x+1 for x in combined_rank]}\")\n",
    "    print()\n",
    "    print(\"The combined approach gets Document 1 at top, which is correct!\")\n",
    "    print(\"Pure dense would rank Document 3 higher (semantic match on 'rate limits')\")\n",
    "    print(\"but Document 1 is the actual answer about Enterprise plan.\")\n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"IMPLEMENTATION OPTIONS:\")\n",
    "    print(\"-\" * 70)\n",
    "    print()\n",
    "    print(\"1. Reciprocal Rank Fusion (RRF)\")\n",
    "    print(\"   score = Σ 1/(k + rank_i) for each retrieval method\")\n",
    "    print(\"   Simple, effective, no score normalization needed\")\n",
    "    print()\n",
    "    print(\"2. Weighted combination\")\n",
    "    print(\"   score = α × dense_score + (1-α) × sparse_score\")\n",
    "    print(\"   Requires score normalization\")\n",
    "    print()\n",
    "    print(\"3. Learned fusion\")\n",
    "    print(\"   Train a model to combine scores\")\n",
    "    print(\"   Most complex, potentially best quality\")\n",
    "    print()\n",
    "    print(\"-\" * 70)\n",
    "    print(\"HANDS-ON: Run 1A/embedding_demo.ipynb (STEP 10) for real hybrid search!\")\n",
    "    print(\"  Uses ChromaDB with metadata filtering — production-ready pattern.\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "\n",
    "hybrid_search_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e80bc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Cost Estimator — Architecture Decision Tool\n",
      "======================================================================\n",
      "\n",
      "SCENARIO 1: Customer Support Ticket Analysis\n",
      "----------------------------------------------------------------------\n",
      "  10,000 tickets/day, avg 200 words each, English\n",
      "\n",
      "  Daily tokens:     2,600,000\n",
      "  Daily cost:    €       7.80\n",
      "  Monthly cost:  €     234.00\n",
      "  Annual cost:   €   2,847.00\n",
      "\n",
      "SCENARIO 2: Legal Document Review\n",
      "----------------------------------------------------------------------\n",
      "  500 contracts/day, avg 5,000 words each, English\n",
      "\n",
      "  Daily tokens:     3,250,000\n",
      "  Daily cost:    €       9.75\n",
      "  Monthly cost:  €     292.50\n",
      "  Annual cost:   €   3,558.75\n",
      "\n",
      "SCENARIO 3: Same Legal Docs — German Language\n",
      "----------------------------------------------------------------------\n",
      "  500 contracts/day, avg 5,000 words, German (1.8 tokens/word)\n",
      "\n",
      "  Daily tokens:     4,500,000\n",
      "  Daily cost:    €      13.50\n",
      "  Monthly cost:  €     405.00\n",
      "  Annual cost:   €   4,927.50\n",
      "\n",
      "======================================================================\n",
      "COST IMPACT ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "German vs English (same content):\n",
      "  English annual: €3,558.75\n",
      "  German annual:  €4,927.50\n",
      "  Difference:     €1,368.75 (+38%)\n",
      "\n",
      "ARCHITECT TAKEAWAY:\n",
      "  → Language choice affects costs significantly\n",
      "  → German/French ~40% more expensive than English\n",
      "  → CJK languages (Chinese, Japanese) can be 2-3x more\n",
      "  → Factor this into multi-region deployment decisions\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "HANDS-ON: Run 1A/tokenization_demo.ipynb for real tokenization!\n",
      "  Uses tiktoken to show actual token breakdowns and costs.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def estimate_token_cost(\n",
    "    daily_documents: int,\n",
    "    avg_words_per_doc: int,\n",
    "    tokens_per_word: float = 1.3,\n",
    "    price_per_1k_tokens: float = 0.003,\n",
    "    days: int = 30\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Estimate LLM costs for document processing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    daily_documents : int\n",
    "        Number of documents processed per day\n",
    "    avg_words_per_doc : int\n",
    "        Average words per document\n",
    "    tokens_per_word : float\n",
    "        Token/word ratio (1.3 English, 1.5 code, 1.8 German, 2.5 CJK)\n",
    "    price_per_1k_tokens : float\n",
    "        Price per 1000 input tokens\n",
    "    days : int\n",
    "        Number of days to calculate for\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with daily_tokens, daily_cost, period_cost, annual_cost\n",
    "    \"\"\"\n",
    "    daily_tokens = daily_documents * avg_words_per_doc * tokens_per_word\n",
    "    daily_cost = (daily_tokens / 1000) * price_per_1k_tokens\n",
    "    \n",
    "    return {\n",
    "        'daily_tokens': int(daily_tokens),\n",
    "        'daily_cost': round(daily_cost, 2),\n",
    "        'period_cost': round(daily_cost * days, 2),\n",
    "        'annual_cost': round(daily_cost * 365, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "# DEMO: Real-world cost scenarios\n",
    "# ==========================================================================\n",
    "\n",
    "print(\"Token Cost Estimator — Architecture Decision Tool\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Scenario 1: Customer support ticket analysis\n",
    "print(\"SCENARIO 1: Customer Support Ticket Analysis\")\n",
    "print(\"-\" * 70)\n",
    "print(\"  10,000 tickets/day, avg 200 words each, English\")\n",
    "print()\n",
    "\n",
    "result = estimate_token_cost(\n",
    "    daily_documents=10000,\n",
    "    avg_words_per_doc=200,\n",
    "    tokens_per_word=1.3,\n",
    "    price_per_1k_tokens=0.003  # Mid-tier model\n",
    ")\n",
    "\n",
    "print(f\"  Daily tokens:  {result['daily_tokens']:>12,}\")\n",
    "print(f\"  Daily cost:    €{result['daily_cost']:>11,.2f}\")\n",
    "print(f\"  Monthly cost:  €{result['period_cost']:>11,.2f}\")\n",
    "print(f\"  Annual cost:   €{result['annual_cost']:>11,.2f}\")\n",
    "print()\n",
    "\n",
    "# Scenario 2: Legal document review (longer docs)\n",
    "print(\"SCENARIO 2: Legal Document Review\")\n",
    "print(\"-\" * 70)\n",
    "print(\"  500 contracts/day, avg 5,000 words each, English\")\n",
    "print()\n",
    "\n",
    "result_legal = estimate_token_cost(\n",
    "    daily_documents=500,\n",
    "    avg_words_per_doc=5000,\n",
    "    tokens_per_word=1.3,\n",
    "    price_per_1k_tokens=0.003\n",
    ")\n",
    "\n",
    "print(f\"  Daily tokens:  {result_legal['daily_tokens']:>12,}\")\n",
    "print(f\"  Daily cost:    €{result_legal['daily_cost']:>11,.2f}\")\n",
    "print(f\"  Monthly cost:  €{result_legal['period_cost']:>11,.2f}\")\n",
    "print(f\"  Annual cost:   €{result_legal['annual_cost']:>11,.2f}\")\n",
    "print()\n",
    "\n",
    "# Scenario 3: Same legal docs but German (more tokens!)\n",
    "print(\"SCENARIO 3: Same Legal Docs — German Language\")\n",
    "print(\"-\" * 70)\n",
    "print(\"  500 contracts/day, avg 5,000 words, German (1.8 tokens/word)\")\n",
    "print()\n",
    "\n",
    "result_german = estimate_token_cost(\n",
    "    daily_documents=500,\n",
    "    avg_words_per_doc=5000,\n",
    "    tokens_per_word=1.8,  # German needs more tokens!\n",
    "    price_per_1k_tokens=0.003\n",
    ")\n",
    "\n",
    "print(f\"  Daily tokens:  {result_german['daily_tokens']:>12,}\")\n",
    "print(f\"  Daily cost:    €{result_german['daily_cost']:>11,.2f}\")\n",
    "print(f\"  Monthly cost:  €{result_german['period_cost']:>11,.2f}\")\n",
    "print(f\"  Annual cost:   €{result_german['annual_cost']:>11,.2f}\")\n",
    "print()\n",
    "\n",
    "# Cost comparison\n",
    "print(\"=\" * 70)\n",
    "print(\"COST IMPACT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(f\"German vs English (same content):\")\n",
    "print(f\"  English annual: €{result_legal['annual_cost']:,.2f}\")\n",
    "print(f\"  German annual:  €{result_german['annual_cost']:,.2f}\")\n",
    "print(f\"  Difference:     €{result_german['annual_cost'] - result_legal['annual_cost']:,.2f} (+{(result_german['annual_cost']/result_legal['annual_cost']-1)*100:.0f}%)\")\n",
    "print()\n",
    "print(\"ARCHITECT TAKEAWAY:\")\n",
    "print(\"  → Language choice affects costs significantly\")\n",
    "print(\"  → German/French ~40% more expensive than English\")\n",
    "print(\"  → CJK languages (Chinese, Japanese) can be 2-3x more\")\n",
    "print(\"  → Factor this into multi-region deployment decisions\")\n",
    "print()\n",
    "print(\"-\" * 70)\n",
    "print(\"HANDS-ON: Run 1A/tokenization_demo.ipynb for real tokenization!\")\n",
    "print(\"  Uses tiktoken to show actual token breakdowns and costs.\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d141fe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Cost Calculator\n",
      "======================================================================\n",
      "\n",
      "Scenario: RAG request with 4,000 input, 500 output tokens\n",
      "\n",
      "Tier            Input     Output      Total     50K req/day\n",
      "------------------------------------------------------------\n",
      "fast       €   0.0020 €   0.0008 €   0.0027 €        135.00\n",
      "mid        €   0.0120 €   0.0075 €   0.0195 €        975.00\n",
      "premium    €   0.0400 €   0.0150 €   0.0550 €      2,750.00\n",
      "\n",
      "INSIGHT: Output tokens cost 3-5x more than input tokens!\n",
      "         Control response length to manage costs.\n"
     ]
    }
   ],
   "source": [
    "def calculate_request_cost(\n",
    "    input_tokens: int,\n",
    "    output_tokens: int,\n",
    "    input_price_per_1k: float,\n",
    "    output_price_per_1k: float\n",
    ") -> dict:\n",
    "    \"\"\"Calculate cost for a single LLM request.\"\"\"\n",
    "    input_cost = (input_tokens / 1000) * input_price_per_1k\n",
    "    output_cost = (output_tokens / 1000) * output_price_per_1k\n",
    "    return {\n",
    "        'input_cost': round(input_cost, 4),\n",
    "        'output_cost': round(output_cost, 4),\n",
    "        'total_cost': round(input_cost + output_cost, 4)\n",
    "    }\n",
    "\n",
    "\n",
    "# Model pricing tiers (€ per 1K tokens)\n",
    "PRICING = {\n",
    "    \"fast\":    {\"input\": 0.0005, \"output\": 0.0015},\n",
    "    \"mid\":     {\"input\": 0.003,  \"output\": 0.015},\n",
    "    \"premium\": {\"input\": 0.01,   \"output\": 0.03},\n",
    "}\n",
    "\n",
    "print(\"Request Cost Calculator\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Typical RAG request: 4K input (context + query), 500 output\n",
    "input_tokens = 4000\n",
    "output_tokens = 500\n",
    "\n",
    "print(f\"Scenario: RAG request with {input_tokens:,} input, {output_tokens} output tokens\")\n",
    "print()\n",
    "print(f\"{'Tier':<10} {'Input':>10} {'Output':>10} {'Total':>10} {'50K req/day':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for tier, prices in PRICING.items():\n",
    "    result = calculate_request_cost(\n",
    "        input_tokens, output_tokens,\n",
    "        prices[\"input\"], prices[\"output\"]\n",
    "    )\n",
    "    daily = result['total_cost'] * 50000\n",
    "    print(f\"{tier:<10} €{result['input_cost']:>9.4f} €{result['output_cost']:>9.4f} €{result['total_cost']:>9.4f} €{daily:>14,.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"INSIGHT: Output tokens cost 3-5x more than input tokens!\")\n",
    "print(\"         Control response length to manage costs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c987ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Context Strategy\n",
      "======================================================================\n",
      "\n",
      "PROBLEM: Full-document search is expensive at scale\n",
      "  10K docs × 2000 tokens = 20M tokens to index/search\n",
      "\n",
      "SOLUTION: Two-pass retrieval\n",
      "  Pass 1: Search summaries (10K × 200 = 2M tokens)\n",
      "  Pass 2: Load top-5 full docs (5 × 2000 = 10K tokens)\n",
      "\n",
      "SAVINGS: 90%+ reduction in tokens processed per query\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "HANDS-ON: Run 1A/embedding_demo.ipynb (STEP 11) for working demo!\n",
      "  Shows ChromaDB with summary + full-doc collections.\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Hierarchical Context Strategy\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"PROBLEM: Full-document search is expensive at scale\")\n",
    "print(\"  10K docs × 2000 tokens = 20M tokens to index/search\")\n",
    "print()\n",
    "print(\"SOLUTION: Two-pass retrieval\")\n",
    "print(\"  Pass 1: Search summaries (10K × 200 = 2M tokens)\")\n",
    "print(\"  Pass 2: Load top-5 full docs (5 × 2000 = 10K tokens)\")\n",
    "print()\n",
    "print(\"SAVINGS: 90%+ reduction in tokens processed per query\")\n",
    "print()\n",
    "print(\"-\" * 70)\n",
    "print(\"HANDS-ON: Run 1A/embedding_demo.ipynb (STEP 11) for working demo!\")\n",
    "print(\"  Shows ChromaDB with summary + full-doc collections.\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9512785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Caching Savings Calculator\n",
      "======================================================================\n",
      "\n",
      "SCENARIO: RAG system with fixed system prompt + few-shot examples\n",
      "\n",
      "Prompt structure:\n",
      "  Cacheable prefix:    2,000 tokens (system + examples)\n",
      "  Variable content:    3,000 tokens (docs + query)\n",
      "  Total per request:   5,000 tokens\n",
      "\n",
      "Pricing:\n",
      "  Regular:  €0.003/1K tokens\n",
      "  Cached:   €0.00075/1K tokens (75% off)\n",
      "\n",
      "   Requests/day   Without Cache      With Cache    Monthly Savings\n",
      "----------------------------------------------------------------------\n",
      "         10,000 €        150.00 €        105.00 €         1,350.00\n",
      "         50,000 €        750.00 €        525.00 €         6,750.00\n",
      "        100,000 €      1,500.00 €      1,050.00 €        13,500.00\n",
      "\n",
      "INSIGHT: At 100K req/day, caching saves €13,500/month!\n",
      "\n",
      "REQUIREMENTS FOR CACHING:\n",
      "  ✓ Prompt prefix must be IDENTICAL across requests\n",
      "  ✓ Put static content FIRST (system prompt, examples)\n",
      "  ✓ Put variable content LAST (retrieved docs, query)\n",
      "  ✗ Changing even 1 token in prefix invalidates cache\n"
     ]
    }
   ],
   "source": [
    "def calculate_cache_savings(\n",
    "    cacheable_tokens: int,\n",
    "    non_cacheable_tokens: int,\n",
    "    regular_price_per_1k: float,\n",
    "    cached_price_per_1k: float,\n",
    "    daily_requests: int\n",
    ") -> dict:\n",
    "    \"\"\"Calculate savings from prompt caching (75-90% discount on cached tokens).\"\"\"\n",
    "    total_tokens = cacheable_tokens + non_cacheable_tokens\n",
    "    \n",
    "    # Without caching\n",
    "    daily_without = (total_tokens / 1000) * regular_price_per_1k * daily_requests\n",
    "    \n",
    "    # With caching\n",
    "    cached_cost = (cacheable_tokens / 1000) * cached_price_per_1k * daily_requests\n",
    "    regular_cost = (non_cacheable_tokens / 1000) * regular_price_per_1k * daily_requests\n",
    "    daily_with = cached_cost + regular_cost\n",
    "    \n",
    "    daily_savings = daily_without - daily_with\n",
    "    \n",
    "    return {\n",
    "        'daily_without_cache': round(daily_without, 2),\n",
    "        'daily_with_cache': round(daily_with, 2),\n",
    "        'daily_savings': round(daily_savings, 2),\n",
    "        'monthly_savings': round(daily_savings * 30, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "print(\"Prompt Caching Savings Calculator\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"SCENARIO: RAG system with fixed system prompt + few-shot examples\")\n",
    "print()\n",
    "\n",
    "# Typical RAG setup\n",
    "cacheable = 2000       # System prompt + 3 few-shot examples\n",
    "non_cacheable = 3000   # Retrieved docs + user query\n",
    "\n",
    "# Anthropic-style pricing (example)\n",
    "regular_price = 0.003      # €0.003 per 1K input tokens\n",
    "cached_price = 0.00075     # 75% discount for cached tokens\n",
    "\n",
    "print(f\"Prompt structure:\")\n",
    "print(f\"  Cacheable prefix:    {cacheable:,} tokens (system + examples)\")\n",
    "print(f\"  Variable content:    {non_cacheable:,} tokens (docs + query)\")\n",
    "print(f\"  Total per request:   {cacheable + non_cacheable:,} tokens\")\n",
    "print()\n",
    "print(f\"Pricing:\")\n",
    "print(f\"  Regular:  €{regular_price}/1K tokens\")\n",
    "print(f\"  Cached:   €{cached_price}/1K tokens (75% off)\")\n",
    "print()\n",
    "\n",
    "# Calculate for different volumes\n",
    "volumes = [10000, 50000, 100000]\n",
    "\n",
    "print(f\"{'Requests/day':>15} {'Without Cache':>15} {'With Cache':>15} {'Monthly Savings':>18}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for daily in volumes:\n",
    "    result = calculate_cache_savings(\n",
    "        cacheable_tokens=cacheable,\n",
    "        non_cacheable_tokens=non_cacheable,\n",
    "        regular_price_per_1k=regular_price,\n",
    "        cached_price_per_1k=cached_price,\n",
    "        daily_requests=daily\n",
    "    )\n",
    "    print(f\"{daily:>15,} €{result['daily_without_cache']:>14,.2f} €{result['daily_with_cache']:>14,.2f} €{result['monthly_savings']:>17,.2f}\")\n",
    "\n",
    "print()\n",
    "print(\"INSIGHT: At 100K req/day, caching saves €13,500/month!\")\n",
    "print()\n",
    "print(\"REQUIREMENTS FOR CACHING:\")\n",
    "print(\"  ✓ Prompt prefix must be IDENTICAL across requests\")\n",
    "print(\"  ✓ Put static content FIRST (system prompt, examples)\")\n",
    "print(\"  ✓ Put variable content LAST (retrieved docs, query)\")\n",
    "print(\"  ✗ Changing even 1 token in prefix invalidates cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47b3eb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TOON vs JSON Comparison\n",
      "============================================================\n",
      "\n",
      "JSON format:\n",
      "{\n",
      "  \"products\": [\n",
      "    {\n",
      "      \"sku\": \"PRD-001\",\n",
      "      \"name\": \"Wireless Mouse\",\n",
      "      \"price\": 29.99,\n",
      "      \"stock\": 150\n",
      "    },\n",
      "    {\n",
      "      \"sku\": \"PRD-002\",\n",
      "      \"name\": \"Mechanical Keyboard\",\n",
      "     ...\n",
      "\n",
      "TOON format:\n",
      "products[5]{sku,name,price,stock}:\n",
      "  PRD-001,Wireless Mouse,29.99,150\n",
      "  PRD-002,Mechanical Keyboard,89.99,75\n",
      "  PRD-003,USB-C Hub,45.99,200\n",
      "  PRD-004,Monitor Stand,34.99,120\n",
      "  PRD-005,Webcam HD,59.99,90\n",
      "\n",
      "------------------------------------------------------------\n",
      "Token Analysis:\n",
      "  JSON tokens:  ~96\n",
      "  TOON tokens:  ~57\n",
      "  Savings:      40.5%\n",
      "  Cost saved:   €0.12/1K requests\n",
      "\n",
      "  At 100,000 requests/day: €360/month saved\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "def encode_toon(data: dict) -> str:\n",
    "    \"\"\"\n",
    "    Encode data to TOON format for LLM prompts.\n",
    "    \n",
    "    Handles the common case: dict with arrays of uniform objects.\n",
    "    For production, use the toon-format package.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        Data structure to encode. Best savings when values are\n",
    "        arrays of objects with identical keys.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        TOON-formatted string\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, list) and len(value) > 0 and isinstance(value[0], dict):\n",
    "            # Tabular format for uniform object arrays\n",
    "            fields = list(value[0].keys())\n",
    "            header = f\"{key}[{len(value)}]{{{','.join(fields)}}}:\"\n",
    "            lines.append(header)\n",
    "            \n",
    "            for item in value:\n",
    "                row_values = [str(item.get(f, '')) for f in fields]\n",
    "                lines.append(f\"  {','.join(row_values)}\")\n",
    "        else:\n",
    "            # Simple key-value for non-tabular data\n",
    "            lines.append(f\"{key}: {json.dumps(value)}\")\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def estimate_toon_savings(data: dict, price_per_1k: float = 0.003) -> dict:\n",
    "    \"\"\"\n",
    "    Estimate token and cost savings from TOON encoding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        Data to analyze\n",
    "    price_per_1k : float\n",
    "        Price per 1000 tokens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict with json_tokens, toon_tokens, savings_percent, cost_saved_per_1k_requests\n",
    "    \"\"\"\n",
    "    json_str = json.dumps(data)\n",
    "    toon_str = encode_toon(data)\n",
    "    \n",
    "    # Rough token estimate: ~4 chars per token for JSON, ~3.5 for TOON\n",
    "    json_tokens = len(json_str) / 4\n",
    "    toon_tokens = len(toon_str) / 3.5\n",
    "    \n",
    "    savings_pct = (1 - toon_tokens / json_tokens) * 100\n",
    "    tokens_saved = json_tokens - toon_tokens\n",
    "    cost_saved = (tokens_saved / 1000) * price_per_1k * 1000  # per 1K requests\n",
    "    \n",
    "    return {\n",
    "        'json_tokens': int(json_tokens),\n",
    "        'toon_tokens': int(toon_tokens),\n",
    "        'savings_percent': round(savings_pct, 1),\n",
    "        'cost_saved_per_1k_requests': round(cost_saved, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: Comparing JSON vs TOON for RAG context\n",
    "# =============================================================================\n",
    "\n",
    "# Scenario: E-commerce RAG system passing product data to LLM\n",
    "products = {\n",
    "    \"products\": [\n",
    "        {\"sku\": \"PRD-001\", \"name\": \"Wireless Mouse\", \"price\": 29.99, \"stock\": 150},\n",
    "        {\"sku\": \"PRD-002\", \"name\": \"Mechanical Keyboard\", \"price\": 89.99, \"stock\": 75},\n",
    "        {\"sku\": \"PRD-003\", \"name\": \"USB-C Hub\", \"price\": 45.99, \"stock\": 200},\n",
    "        {\"sku\": \"PRD-004\", \"name\": \"Monitor Stand\", \"price\": 34.99, \"stock\": 120},\n",
    "        {\"sku\": \"PRD-005\", \"name\": \"Webcam HD\", \"price\": 59.99, \"stock\": 90},\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOON vs JSON Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show the formats\n",
    "print(\"\\nJSON format:\")\n",
    "print(json.dumps(products, indent=2)[:200] + \"...\")\n",
    "\n",
    "print(\"\\nTOON format:\")\n",
    "print(encode_toon(products))\n",
    "\n",
    "# Calculate savings\n",
    "savings = estimate_toon_savings(products)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Token Analysis:\")\n",
    "print(f\"  JSON tokens:  ~{savings['json_tokens']}\")\n",
    "print(f\"  TOON tokens:  ~{savings['toon_tokens']}\")\n",
    "print(f\"  Savings:      {savings['savings_percent']}%\")\n",
    "print(f\"  Cost saved:   €{savings['cost_saved_per_1k_requests']}/1K requests\")\n",
    "\n",
    "# Scale to production\n",
    "daily_requests = 100000\n",
    "monthly_savings = savings['cost_saved_per_1k_requests'] * (daily_requests / 1000) * 30\n",
    "print(f\"\\n  At {daily_requests:,} requests/day: €{monthly_savings:,.0f}/month saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90587520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_prompt(query: str, retrieved_data: dict) -> str:\n",
    "    \"\"\"Build prompt with TOON-encoded context.\"\"\"\n",
    "    toon_context = encode_toon(retrieved_data)\n",
    "    return f\"\"\"Answer based on the product data below.\n",
    "```toon\n",
    "{toon_context}\n",
    "```\n",
    "\n",
    "Question: {query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6edca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llmlingua\n",
      "  Downloading llmlingua-0.2.2-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: transformers>=4.26.0 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from llmlingua) (4.57.1)\n",
      "Collecting accelerate (from llmlingua)\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from llmlingua) (2.9.0)\n",
      "Requirement already satisfied: tiktoken in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from llmlingua) (0.12.0)\n",
      "Requirement already satisfied: nltk in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from llmlingua) (3.9.1)\n",
      "Requirement already satisfied: numpy in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from llmlingua) (1.26.4)\n",
      "Requirement already satisfied: filelock in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from transformers>=4.26.0->llmlingua) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from transformers>=4.26.0->llmlingua) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from transformers>=4.26.0->llmlingua) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from transformers>=4.26.0->llmlingua) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from transformers>=4.26.0->llmlingua) (2025.7.34)\n",
      "Requirement already satisfied: requests in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from transformers>=4.26.0->llmlingua) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from transformers>=4.26.0->llmlingua) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from transformers>=4.26.0->llmlingua) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from transformers>=4.26.0->llmlingua) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.26.0->llmlingua) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.26.0->llmlingua) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.26.0->llmlingua) (1.2.0)\n",
      "Requirement already satisfied: psutil in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from accelerate->llmlingua) (7.0.0)\n",
      "Requirement already satisfied: setuptools in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from torch->llmlingua) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from torch->llmlingua) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from torch->llmlingua) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from torch->llmlingua) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from sympy>=1.13.3->torch->llmlingua) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from jinja2->torch->llmlingua) (3.0.2)\n",
      "Requirement already satisfied: click in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from nltk->llmlingua) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from nltk->llmlingua) (1.5.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from requests->transformers>=4.26.0->llmlingua) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from requests->transformers>=4.26.0->llmlingua) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from requests->transformers>=4.26.0->llmlingua) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages (from requests->transformers>=4.26.0->llmlingua) (2025.6.15)\n",
      "Downloading llmlingua-0.2.2-py3-none-any.whl (30 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Installing collected packages: accelerate, llmlingua\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [llmlingua]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 llmlingua-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llmlingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d2654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f40aab4acaf4611a0743cf1e189ea7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/752 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fde8f15b03458086b519a5476d6f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358183bdbc294d098f2622075ebd40ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ad629ee60a493fbf2428024b8265f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7794588967408a80f87d048fbd011c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# pip install llmlingua\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllmlingua\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptCompressor\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m compressor = \u001b[43mPromptCompressor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmicrosoft/llmlingua-2-xlm-roberta-large-meetingbank\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Compress a long document for RAG\u001b[39;00m\n\u001b[32m      9\u001b[39m result = compressor.compress_prompt(\n\u001b[32m     10\u001b[39m     long_document_text,\n\u001b[32m     11\u001b[39m     rate=\u001b[32m0.5\u001b[39m  \u001b[38;5;66;03m# Target 50% of original size\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/jupyter-env/lib/python3.12/site-packages/llmlingua/prompt_compressor.py:89\u001b[39m, in \u001b[36mPromptCompressor.__init__\u001b[39m\u001b[34m(self, model_name, device_map, model_config, open_api_config, use_llmlingua2, llmlingua2_config)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mself\u001b[39m.prefix_bos_num = \u001b[32m100\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.oai_tokenizer = tiktoken.encoding_for_model(\u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_llmlingua2:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28mself\u001b[39m.init_llmlingua2(**llmlingua2_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/jupyter-env/lib/python3.12/site-packages/llmlingua/prompt_compressor.py:140\u001b[39m, in \u001b[36mPromptCompressor.load_model\u001b[39m\u001b[34m(self, model_name, device_map, model_config)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mself\u001b[39m.device = (\n\u001b[32m    135\u001b[39m     device_map\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m device_map \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m )\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     model = \u001b[43mMODEL_CLASS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorch_dtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    151\u001b[39m     model = MODEL_CLASS.from_pretrained(\n\u001b[32m    152\u001b[39m         model_name,\n\u001b[32m    153\u001b[39m         device_map=device_map,\n\u001b[32m   (...)\u001b[39m\u001b[32m    156\u001b[39m         **model_config,\n\u001b[32m    157\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/jupyter-env/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/jupyter-env/lib/python3.12/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/jupyter-env/lib/python3.12/site-packages/transformers/modeling_utils.py:5048\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5038\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5039\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5041\u001b[39m     (\n\u001b[32m   5042\u001b[39m         model,\n\u001b[32m   5043\u001b[39m         missing_keys,\n\u001b[32m   5044\u001b[39m         unexpected_keys,\n\u001b[32m   5045\u001b[39m         mismatched_keys,\n\u001b[32m   5046\u001b[39m         offload_index,\n\u001b[32m   5047\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5065\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/jupyter-env/lib/python3.12/site-packages/transformers/modeling_utils.py:5432\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5430\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_hqq_or_quark:\n\u001b[32m   5431\u001b[39m     expanded_device_map = expand_device_map(device_map, expected_keys)\n\u001b[32m-> \u001b[39m\u001b[32m5432\u001b[39m     \u001b[43mcaching_allocator_warmup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_device_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5434\u001b[39m \u001b[38;5;66;03m# Prepare and compatabilize arguments for serial and parallel shard loading\u001b[39;00m\n\u001b[32m   5435\u001b[39m args_list = [\n\u001b[32m   5436\u001b[39m     (\n\u001b[32m   5437\u001b[39m         shard_file,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5452\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m shard_file \u001b[38;5;129;01min\u001b[39;00m checkpoint_files\n\u001b[32m   5453\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/jupyter-env/lib/python3.12/site-packages/transformers/modeling_utils.py:6089\u001b[39m, in \u001b[36mcaching_allocator_warmup\u001b[39m\u001b[34m(model, expanded_device_map, hf_quantizer)\u001b[39m\n\u001b[32m   6087\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device.type \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   6088\u001b[39m     torch_accelerator_module = \u001b[38;5;28mgetattr\u001b[39m(torch, device.type)\n\u001b[32m-> \u001b[39m\u001b[32m6089\u001b[39m     index = device.index \u001b[38;5;28;01mif\u001b[39;00m device.index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtorch_accelerator_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6090\u001b[39m     device_memory = torch_accelerator_module.mem_get_info(index)[\u001b[32m0\u001b[39m]\n\u001b[32m   6091\u001b[39m     \u001b[38;5;66;03m# Allow up to (max device memory - 1.2 GiB) in resource-constrained hardware configurations. Trying to reserve more\u001b[39;00m\n\u001b[32m   6092\u001b[39m     \u001b[38;5;66;03m# than that amount might sometimes lead to unnecessary cuda/xpu OOM, if the last parameter to be loaded on the device is large,\u001b[39;00m\n\u001b[32m   6093\u001b[39m     \u001b[38;5;66;03m# and the remaining reserved memory portion is smaller than the param size -> torch will then try to fully re-allocate all\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   6096\u001b[39m     \u001b[38;5;66;03m# Note that we use an absolute value instead of device proportion here, as a 8GiB device could still allocate too much\u001b[39;00m\n\u001b[32m   6097\u001b[39m     \u001b[38;5;66;03m# if using e.g. 90% of device size, while a 140GiB device would allocate too little\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/jupyter-env/lib/python3.12/site-packages/torch/cuda/__init__.py:1069\u001b[39m, in \u001b[36mcurrent_device\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1067\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcurrent_device\u001b[39m() -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   1068\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return the index of a currently selected device.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1069\u001b[39m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1070\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._cuda_getDevice()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/jupyter-env/lib/python3.12/site-packages/torch/cuda/__init__.py:403\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    400\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    401\u001b[39m     )\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "# \n",
    "def get_device():\n",
    "    \"\"\"Detect best available device for PyTorch.\"\"\"\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cuda\"\n",
    "        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "            return \"mps\"\n",
    "        else:\n",
    "            return \"cpu\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting device: {e}\")\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "def demo_llmlingua_compression():\n",
    "    \"\"\"\n",
    "    Demonstrate prompt compression with LLMLingua.\n",
    "    \n",
    "    This is OPTIONAL — requires ~1.5GB model download and can be slow on CPU.\n",
    "    \"\"\"\n",
    "    print(\"LLMLingua Prompt Compression Demo\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    # Step 1: Detect device\n",
    "    print(\"Step 1: Detecting device...\")\n",
    "    try:\n",
    "        device = get_device()\n",
    "        print(f\"  Using device: {device}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")\n",
    "        print(\"  Falling back to CPU\")\n",
    "        device = \"cpu\"\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Load model\n",
    "    print(\"Step 2: Loading LLMLingua model...\")\n",
    "    print(\"  (First run downloads ~1.5GB — this may take a few minutes)\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        from llmlingua import PromptCompressor\n",
    "        \n",
    "        compressor = PromptCompressor(\n",
    "            model_name=\"microsoft/llmlingua-2-xlm-roberta-large-meetingbank\",\n",
    "            device_map=device\n",
    "        )\n",
    "        print(\"  Model loaded successfully!\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR loading model: {e}\")\n",
    "        print()\n",
    "        print(\"  Common issues:\")\n",
    "        print(\"    - First run: model is downloading (wait a few minutes)\")\n",
    "        print(\"    - Memory: model needs ~2GB RAM\")\n",
    "        print(\"    - Try restarting kernel if stuck\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Compress sample text\n",
    "    print(\"Step 3: Compressing sample text...\")\n",
    "    print()\n",
    "    \n",
    "    long_document_text = \"\"\"\n",
    "    The quarterly financial report indicates that revenue increased by 15% \n",
    "    compared to the same period last year. This growth was primarily driven \n",
    "    by strong performance in the enterprise segment, which saw a 23% increase \n",
    "    in new customer acquisitions. The company's gross margin improved to 72%, \n",
    "    up from 68% in the previous quarter, mainly due to operational efficiencies \n",
    "    and reduced cloud infrastructure costs. Operating expenses remained stable \n",
    "    at $45 million, with R&D investments accounting for 35% of total expenses. \n",
    "    The company ended the quarter with $120 million in cash and equivalents, \n",
    "    providing a strong runway for planned expansion into European markets.\n",
    "    \"\"\"\n",
    "    \n",
    "    original_words = len(long_document_text.split())\n",
    "    print(f\"  Original: {original_words} words\")\n",
    "    \n",
    "    try:\n",
    "        result = compressor.compress_prompt(\n",
    "            long_document_text,\n",
    "            rate=0.5  # Target 50% compression\n",
    "        )\n",
    "        \n",
    "        compressed = result['compressed_prompt']\n",
    "        compressed_words = len(compressed.split())\n",
    "        \n",
    "        print(f\"  Compressed: {compressed_words} words\")\n",
    "        print(f\"  Reduction: {(1 - compressed_words/original_words)*100:.1f}%\")\n",
    "        print()\n",
    "        print(\"-\" * 70)\n",
    "        print(\"ORIGINAL (excerpt):\")\n",
    "        print(f\"  {long_document_text[:200].strip()}...\")\n",
    "        print()\n",
    "        print(\"COMPRESSED:\")\n",
    "        print(f\"  {compressed.strip()}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR during compression: {e}\")\n",
    "        return\n",
    "    \n",
    "    print()\n",
    "    print(\"ARCHITECT TAKEAWAY:\")\n",
    "    print(\"  LLMLingua can reduce token count by 30-50% with minimal info loss.\")\n",
    "    print(\"  Trade-off: adds latency (~100-500ms per compression).\")\n",
    "    print(\"  Best for: pre-processing documents at ingestion, not real-time.\")\n",
    "\n",
    "\n",
    "# Run the demo\n",
    "demo_llmlingua_compression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28cc339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753744e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
