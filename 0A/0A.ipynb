{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21fd07c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOAN APPROVAL NEURON EXAMPLE\n",
      "============================================================\n",
      "\n",
      "Applicant data:\n",
      "  Income: â‚¬45,000\n",
      "  Credit Score: 680\n",
      "  Years Employed: 3\n",
      "  Existing Debt: â‚¬12,000\n",
      "\n",
      "Step 1: Calculate each input's contribution\n",
      "--------------------------------------------------\n",
      "  Input 1:   45000.00 Ã— weight    0.00001 =     0.45 (pushes toward YES)\n",
      "  Input 2:     680.00 Ã— weight    0.01000 =     6.80 (pushes toward YES)\n",
      "  Input 3:       3.00 Ã— weight    0.10000 =     0.30 (pushes toward YES)\n",
      "  Input 4:   12000.00 Ã— weight   -0.00005 =    -0.60 (pushes toward NO)\n",
      "\n",
      "Step 2: Sum all contributions\n",
      "--------------------------------------------------\n",
      "  Weighted sum = 6.95\n",
      "\n",
      "Step 3: Add bias\n",
      "--------------------------------------------------\n",
      "  Pre-activation = weighted_sum + bias\n",
      "  Pre-activation = 6.95 + (-3.00) = 3.95\n",
      "\n",
      "Step 4: Apply activation (decision rule)\n",
      "--------------------------------------------------\n",
      "  Rule: If pre-activation > 0, output 1 (yes), else output 0 (no)\n",
      "  3.95 > 0? â†’ APPROVE (score > 0)\n",
      "\n",
      "============================================================\n",
      "FINAL DECISION: APPROVED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def single_neuron_step_by_step(inputs, weights, bias):\n",
    "    \"\"\"\n",
    "    A single artificial neuron - the fundamental building block of neural networks.\n",
    "    \n",
    "    This function mimics how a loan officer might make a decision:\n",
    "    1. Consider multiple factors (inputs)\n",
    "    2. Weight each factor by its importance (weights)\n",
    "    3. Combine into a single score (weighted sum + bias)\n",
    "    4. Make a decision based on the score (activation)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    inputs : list or numpy array\n",
    "        The raw data we're making a decision about.\n",
    "        Example: [income, credit_score, years_employed, existing_debt]\n",
    "        \n",
    "    weights : list or numpy array\n",
    "        How important each input is. Learned during training.\n",
    "        Positive weight = input helps, Negative weight = input hurts\n",
    "        Example: [0.00001, 0.01, 0.1, -0.00005]\n",
    "        \n",
    "    bias : float\n",
    "        A baseline adjustment. Like a default tendency toward yes or no.\n",
    "        Positive bias = lean toward yes, Negative bias = lean toward no\n",
    "        Example: -3.0 (slightly conservative)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (weighted_sum, pre_activation, output)\n",
    "        - weighted_sum: The sum of (input Ã— weight) for each input\n",
    "        - pre_activation: weighted_sum + bias (the raw \"score\")\n",
    "        - output: The final decision (1 or 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 1: Calculate each input's contribution\n",
    "    # =========================================================================\n",
    "    # Each input contributes to the decision based on its weight.\n",
    "    # Think: \"How much does this factor push toward yes or no?\"\n",
    "    \n",
    "    contributions = []\n",
    "    print(\"Step 1: Calculate each input's contribution\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "        # Multiply input by its weight to get its contribution\n",
    "        contribution = inputs[i] * weights[i]\n",
    "        contributions.append(contribution)\n",
    "        \n",
    "        # Explain what's happening\n",
    "        if weights[i] > 0:\n",
    "            direction = \"pushes toward YES\"\n",
    "        else:\n",
    "            direction = \"pushes toward NO\"\n",
    "        \n",
    "        print(f\"  Input {i+1}: {inputs[i]:>10.2f} Ã— weight {weights[i]:>10.5f} = {contribution:>8.2f} ({direction})\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 2: Sum all contributions (weighted sum)\n",
    "    # =========================================================================\n",
    "    # Add up all the individual contributions to get a combined \"evidence\" score\n",
    "    \n",
    "    weighted_sum = sum(contributions)\n",
    "    print(f\"\\nStep 2: Sum all contributions\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"  Weighted sum = {weighted_sum:.2f}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 3: Add bias (pre-activation)\n",
    "    # =========================================================================\n",
    "    # The bias shifts our decision threshold.\n",
    "    # Positive bias = easier to say yes, Negative bias = harder to say yes\n",
    "    \n",
    "    pre_activation = weighted_sum + bias\n",
    "    print(f\"\\nStep 3: Add bias\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"  Pre-activation = weighted_sum + bias\")\n",
    "    print(f\"  Pre-activation = {weighted_sum:.2f} + ({bias:.2f}) = {pre_activation:.2f}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STEP 4: Apply activation function (make decision)\n",
    "    # =========================================================================\n",
    "    # Convert the raw score into a final decision.\n",
    "    # Here we use a simple \"step function\": positive = yes, negative/zero = no\n",
    "    \n",
    "    if pre_activation > 0:\n",
    "        output = 1  # Approve\n",
    "        decision = \"APPROVE (score > 0)\"\n",
    "    else:\n",
    "        output = 0  # Reject\n",
    "        decision = \"REJECT (score â‰¤ 0)\"\n",
    "    \n",
    "    print(f\"\\nStep 4: Apply activation (decision rule)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"  Rule: If pre-activation > 0, output 1 (yes), else output 0 (no)\")\n",
    "    print(f\"  {pre_activation:.2f} > 0? â†’ {decision}\")\n",
    "    \n",
    "    return weighted_sum, pre_activation, output\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE: Loan approval decision\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOAN APPROVAL NEURON EXAMPLE\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Our applicant's data\n",
    "inputs = [\n",
    "    45000,   # Annual income (â‚¬)\n",
    "    680,     # Credit score (300-850 scale)\n",
    "    3,       # Years employed\n",
    "    12000    # Existing debt (â‚¬)\n",
    "]\n",
    "\n",
    "# How important each factor is (these would be LEARNED in a real model)\n",
    "weights = [\n",
    "    0.00001,   # Income: small weight because numbers are large\n",
    "    0.01,      # Credit score: moderate importance\n",
    "    0.1,       # Years employed: some importance\n",
    "    -0.00005   # Existing debt: NEGATIVE because debt is bad\n",
    "]\n",
    "\n",
    "# Baseline tendency (slightly conservative bank)\n",
    "bias = -3.0\n",
    "\n",
    "print(\"Applicant data:\")\n",
    "print(f\"  Income: â‚¬{inputs[0]:,}\")\n",
    "print(f\"  Credit Score: {inputs[1]}\")\n",
    "print(f\"  Years Employed: {inputs[2]}\")\n",
    "print(f\"  Existing Debt: â‚¬{inputs[3]:,}\")\n",
    "print()\n",
    "\n",
    "# Run the neuron\n",
    "weighted_sum, pre_activation, output = single_neuron_step_by_step(inputs, weights, bias)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(f\"FINAL DECISION: {'APPROVED' if output == 1 else 'REJECTED'}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084b7f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def neuron(x, w, b, activation_fn):\n",
    "    \"\"\"\n",
    "    A single neuron in compact form.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : numpy array, shape (n_features,)\n",
    "        Input values\n",
    "    w : numpy array, shape (n_features,)\n",
    "        Weights (learned parameters)\n",
    "    b : float\n",
    "        Bias (learned parameter)\n",
    "    activation_fn : function\n",
    "        The activation function to apply\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float: The neuron's output\n",
    "    \"\"\"\n",
    "    # np.dot computes the dot product (sum of element-wise products)\n",
    "    # This is the \"weighted sum\" step in one line\n",
    "    z = np.dot(w, x) + b\n",
    "    \n",
    "    # Apply activation function to get final output\n",
    "    y = activation_fn(z)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "# Simple step activation function\n",
    "def step_activation(z):\n",
    "    \"\"\"Return 1 if z > 0, else 0\"\"\"\n",
    "    return 1 if z > 0 else 0\n",
    "\n",
    "\n",
    "# Example usage\n",
    "x = np.array([45000, 680, 3, 12000])\n",
    "w = np.array([0.00001, 0.01, 0.1, -0.00005])\n",
    "b = -3.0\n",
    "\n",
    "output = neuron(x, w, b, step_activation)\n",
    "print(f\"Output: {output}\")  # Output: 1 (approved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3465c7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid examples:\n",
      "  sigmoid(-10) = 0.000045\n",
      "  sigmoid(-2)  = 0.119203\n",
      "  sigmoid(0)   = 0.500000\n",
      "  sigmoid(2)   = 0.880797\n",
      "  sigmoid(10)  = 0.999955\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \n",
    "    What it does:\n",
    "    - Takes any real number as input\n",
    "    - Outputs a value between 0 and 1\n",
    "    - Smooth, S-shaped curve\n",
    "    \n",
    "    When to use:\n",
    "    - Output layer for binary classification (probability of class 1)\n",
    "    - Historically used in hidden layers, but rarely now (see ReLU)\n",
    "    \n",
    "    Why it works:\n",
    "    - Large negative z â†’ output â‰ˆ 0\n",
    "    - Large positive z â†’ output â‰ˆ 1\n",
    "    - z = 0 â†’ output = 0.5\n",
    "    \n",
    "    Problem:\n",
    "    - For very large or very small z, the gradient is nearly 0\n",
    "    - This causes \"vanishing gradients\" in deep networks (explained later)\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Examples\n",
    "print(\"Sigmoid examples:\")\n",
    "print(f\"  sigmoid(-10) = {sigmoid(-10):.6f}\")  # â‰ˆ 0 (very negative â†’ near 0)\n",
    "print(f\"  sigmoid(-2)  = {sigmoid(-2):.6f}\")   # = 0.119\n",
    "print(f\"  sigmoid(0)   = {sigmoid(0):.6f}\")    # = 0.5 (midpoint)\n",
    "print(f\"  sigmoid(2)   = {sigmoid(2):.6f}\")    # = 0.881\n",
    "print(f\"  sigmoid(10)  = {sigmoid(10):.6f}\")   # â‰ˆ 1 (very positive â†’ near 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c52b1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ReLU examples:\n",
      "  relu(-10) = 0\n",
      "  relu(-2)  = 0\n",
      "  relu(0)   = 0\n",
      "  relu(2)   = 2\n",
      "  relu(10)  = 10\n"
     ]
    }
   ],
   "source": [
    "def relu(z):\n",
    "    \"\"\"\n",
    "    ReLU (Rectified Linear Unit) activation function.\n",
    "    \n",
    "    What it does:\n",
    "    - If z > 0, output z (pass through unchanged)\n",
    "    - If z â‰¤ 0, output 0 (block negative values)\n",
    "    \n",
    "    When to use:\n",
    "    - Default choice for hidden layers in modern networks\n",
    "    - Works well in deep networks\n",
    "    \n",
    "    Why it's great:\n",
    "    - Computationally efficient (just a comparison)\n",
    "    - Doesn't saturate for positive values (gradient = 1)\n",
    "    - Creates sparse representations (many zeros)\n",
    "    \n",
    "    Problem:\n",
    "    - \"Dead neurons\": If a neuron's output is always negative,\n",
    "      it will always output 0 and never update (gradient = 0)\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# Examples\n",
    "print(\"\\nReLU examples:\")\n",
    "print(f\"  relu(-10) = {relu(-10)}\")   # 0 (negative â†’ blocked)\n",
    "print(f\"  relu(-2)  = {relu(-2)}\")    # 0\n",
    "print(f\"  relu(0)   = {relu(0)}\")     # 0\n",
    "print(f\"  relu(2)   = {relu(2)}\")     # 2 (positive â†’ passed through)\n",
    "print(f\"  relu(10)  = {relu(10)}\")    # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "131c7d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Softmax example (digit classification):\n",
      "Raw scores: [1.2 0.5 0.1 3.8 0.2 0.1 0.3 0.9 0.4 0.2]\n",
      "Probabilities: [0.056 0.028 0.019 0.75  0.02  0.019 0.023 0.041 0.025 0.02 ]\n",
      "Sum of probabilities: 1.000000\n",
      "Predicted class: 3 (highest probability)\n"
     ]
    }
   ],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Softmax activation function.\n",
    "    \n",
    "    What it does:\n",
    "    - Takes a vector of K raw scores (one per class)\n",
    "    - Converts to K probabilities that sum to 1\n",
    "    - Higher scores get higher probabilities\n",
    "    \n",
    "    When to use:\n",
    "    - Output layer for multi-class classification\n",
    "    - Example: Classifying images into 10 digit classes (0-9)\n",
    "    \n",
    "    Why it works:\n",
    "    - Exponential makes all values positive\n",
    "    - Division by sum ensures they add to 1\n",
    "    - Preserves ranking (highest score â†’ highest probability)\n",
    "    \n",
    "    Note: We subtract max(z) for numerical stability to prevent\n",
    "    overflow when computing exp() of large numbers.\n",
    "    \"\"\"\n",
    "    # Subtract max for numerical stability (doesn't change result)\n",
    "    exp_z = np.exp(z - np.max(z))\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "# Example: Classifying a digit image\n",
    "# Raw scores from a network for classes 0-9\n",
    "scores = np.array([1.2, 0.5, 0.1, 3.8, 0.2, 0.1, 0.3, 0.9, 0.4, 0.2])\n",
    "\n",
    "probabilities = softmax(scores)\n",
    "print(\"\\nSoftmax example (digit classification):\")\n",
    "print(\"Raw scores:\", scores)\n",
    "print(\"Probabilities:\", np.round(probabilities, 3))\n",
    "print(f\"Sum of probabilities: {np.sum(probabilities):.6f}\")  # Should be 1.0\n",
    "print(f\"Predicted class: {np.argmax(probabilities)} (highest probability)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe916df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FORWARD PASS DEMONSTRATION\n",
      "======================================================================\n",
      "Network initialized:\n",
      "  Input size: 2\n",
      "  Hidden size: 4\n",
      "  Output size: 1\n",
      "  W1 shape: (4, 2) (hidden Ã— input)\n",
      "  b1 shape: (4, 1)\n",
      "  W2 shape: (1, 4) (output Ã— hidden)\n",
      "  b2 shape: (1, 1)\n",
      "  Total parameters: 17\n",
      "\n",
      "======================================================================\n",
      "Processing 3 loan applications...\n",
      "======================================================================\n",
      "\n",
      "============================================================\n",
      "FORWARD PASS\n",
      "============================================================\n",
      "\n",
      "Input X shape: (2, 3)\n",
      "Input X:\n",
      "[[0.45 0.8  0.25]\n",
      " [0.68 0.9  0.55]]\n",
      "\n",
      "--- Layer 1 (Input â†’ Hidden) ---\n",
      "W1 @ X + b1 = Z1\n",
      "((4, 2)) @ ((2, 3)) + ((4, 1)) = ((4, 3))\n",
      "Z1 (pre-activation):\n",
      "[[ 0.12950164  0.27293345  0.04813317]\n",
      " [ 1.32712014  1.8888777   0.99958856]\n",
      " [-0.26458215 -0.39804596 -0.18731367]\n",
      " [ 1.23250138  1.95406151  0.8168923 ]]\n",
      "A1 = ReLU(Z1):\n",
      "[[0.12950164 0.27293345 0.04813317]\n",
      " [1.32712014 1.8888777  0.99958856]\n",
      " [0.         0.         0.        ]\n",
      " [1.23250138 1.95406151 0.8168923 ]]\n",
      "\n",
      "--- Layer 2 (Hidden â†’ Output) ---\n",
      "W2 @ A1 + b2 = Z2\n",
      "((1, 4)) @ ((4, 3)) + ((1, 1)) = ((1, 3))\n",
      "Z2 (pre-activation):\n",
      "[[ 0.06026819 -0.00945422  0.09849182]]\n",
      "A2 = sigmoid(Z2):\n",
      "[[0.51506249 0.49763646 0.52460307]]\n",
      "\n",
      "Final output shape: (1, 3)\n",
      "Predictions (probabilities): [0.51506249 0.49763646 0.52460307]\n",
      "\n",
      "======================================================================\n",
      "INTERPRETATION\n",
      "======================================================================\n",
      "Application 1 (income=0.45, credit=0.68): 51.5% approval probability\n",
      "Application 2 (income=0.80, credit=0.90): 49.8% approval probability\n",
      "Application 3 (income=0.25, credit=0.55): 52.5% approval probability\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleNetwork:\n",
    "    \"\"\"\n",
    "    A simple 2-layer neural network for binary classification.\n",
    "    \n",
    "    Architecture: Input â†’ Hidden Layer (ReLU) â†’ Output Layer (Sigmoid)\n",
    "    \n",
    "    This network can learn non-linear decision boundaries because:\n",
    "    1. The hidden layer creates multiple non-linear features\n",
    "    2. The output layer combines them for the final decision\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the network with random weights.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            Number of input features (e.g., 2 for [income, credit_score])\n",
    "        hidden_size : int  \n",
    "            Number of neurons in hidden layer (more = more capacity)\n",
    "        output_size : int\n",
    "            Number of output neurons (1 for binary classification)\n",
    "        \"\"\"\n",
    "        # =====================================================================\n",
    "        # Weight Initialization\n",
    "        # =====================================================================\n",
    "        # Weights connect neurons between layers.\n",
    "        # Shape: (neurons_in_this_layer, neurons_in_previous_layer)\n",
    "        # \n",
    "        # We initialize with small random values. The scaling factor\n",
    "        # (np.sqrt(2/n)) is called \"He initialization\" and helps training.\n",
    "        # We'll explain WHY this matters in the training section.\n",
    "        \n",
    "        # Hidden layer weights: connects input to hidden\n",
    "        # Shape: (hidden_size, input_size) = (4, 2) if hidden=4, input=2\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        \n",
    "        # Hidden layer biases: one per hidden neuron\n",
    "        # Shape: (hidden_size, 1) = (4, 1)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Output layer weights: connects hidden to output  \n",
    "        # Shape: (output_size, hidden_size) = (1, 4)\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "        \n",
    "        # Output layer biases: one per output neuron\n",
    "        # Shape: (output_size, 1) = (1, 1)\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "        \n",
    "        print(f\"Network initialized:\")\n",
    "        print(f\"  Input size: {input_size}\")\n",
    "        print(f\"  Hidden size: {hidden_size}\")\n",
    "        print(f\"  Output size: {output_size}\")\n",
    "        print(f\"  W1 shape: {self.W1.shape} (hidden Ã— input)\")\n",
    "        print(f\"  b1 shape: {self.b1.shape}\")\n",
    "        print(f\"  W2 shape: {self.W2.shape} (output Ã— hidden)\")\n",
    "        print(f\"  b2 shape: {self.b2.shape}\")\n",
    "        print(f\"  Total parameters: {self.W1.size + self.b1.size + self.W2.size + self.b2.size}\")\n",
    "    \n",
    "    def forward(self, X, verbose=False):\n",
    "        \"\"\"\n",
    "        Forward pass: compute output given input.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy array, shape (input_size, batch_size)\n",
    "            Input data. Each column is one example.\n",
    "            Example: shape (2, 32) = 2 features, 32 examples\n",
    "            \n",
    "        verbose : bool\n",
    "            If True, print intermediate steps\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        numpy array, shape (output_size, batch_size)\n",
    "            Output predictions (probabilities for each example)\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"FORWARD PASS\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"\\nInput X shape: {X.shape}\")\n",
    "            print(f\"Input X:\\n{X}\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # LAYER 1: Input â†’ Hidden\n",
    "        # =====================================================================\n",
    "        \n",
    "        # Step 1a: Compute weighted sum for hidden layer\n",
    "        # Z1 = W1 @ X + b1\n",
    "        # Matrix multiplication: (hidden, input) @ (input, batch) = (hidden, batch)\n",
    "        # This computes ALL hidden neurons for ALL examples at once!\n",
    "        Z1 = np.dot(self.W1, X) + self.b1\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- Layer 1 (Input â†’ Hidden) ---\")\n",
    "            print(f\"W1 @ X + b1 = Z1\")\n",
    "            print(f\"({self.W1.shape}) @ ({X.shape}) + ({self.b1.shape}) = ({Z1.shape})\")\n",
    "            print(f\"Z1 (pre-activation):\\n{Z1}\")\n",
    "        \n",
    "        # Step 1b: Apply ReLU activation\n",
    "        # ReLU(z) = max(0, z) for each element\n",
    "        A1 = np.maximum(0, Z1)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"A1 = ReLU(Z1):\\n{A1}\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # LAYER 2: Hidden â†’ Output\n",
    "        # =====================================================================\n",
    "        \n",
    "        # Step 2a: Compute weighted sum for output layer\n",
    "        Z2 = np.dot(self.W2, A1) + self.b2\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- Layer 2 (Hidden â†’ Output) ---\")\n",
    "            print(f\"W2 @ A1 + b2 = Z2\")\n",
    "            print(f\"({self.W2.shape}) @ ({A1.shape}) + ({self.b2.shape}) = ({Z2.shape})\")\n",
    "            print(f\"Z2 (pre-activation):\\n{Z2}\")\n",
    "        \n",
    "        # Step 2b: Apply sigmoid activation\n",
    "        # Sigmoid squashes to (0, 1) for probability output\n",
    "        A2 = 1 / (1 + np.exp(-Z2))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"A2 = sigmoid(Z2):\\n{A2}\")\n",
    "            print(f\"\\nFinal output shape: {A2.shape}\")\n",
    "            print(f\"Predictions (probabilities): {A2.flatten()}\")\n",
    "        \n",
    "        # Store intermediate values (needed for backpropagation later)\n",
    "        self.cache = {\n",
    "            'X': X, 'Z1': Z1, 'A1': A1, 'Z2': Z2, 'A2': A2\n",
    "        }\n",
    "        \n",
    "        return A2\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE: Forward pass demonstration\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FORWARD PASS DEMONSTRATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create a small network\n",
    "np.random.seed(42)  # For reproducible results\n",
    "net = SimpleNetwork(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "# Create sample input: 3 loan applications\n",
    "# Each column is one application: [income (scaled), credit_score (scaled)]\n",
    "X = np.array([\n",
    "    [0.45, 0.80, 0.25],   # Incomes (scaled to ~0-1)\n",
    "    [0.68, 0.90, 0.55]    # Credit scores (scaled to ~0-1)\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Processing 3 loan applications...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run forward pass with detailed output\n",
    "predictions = net.forward(X, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Application 1 (income=0.45, credit=0.68): {predictions[0,0]:.1%} approval probability\")\n",
    "print(f\"Application 2 (income=0.80, credit=0.90): {predictions[0,1]:.1%} approval probability\")\n",
    "print(f\"Application 3 (income=0.25, credit=0.55): {predictions[0,2]:.1%} approval probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bfd7036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.1267\n",
      "Typical error magnitude: $35,590\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss for regression problems.\n",
    "    \n",
    "    How it works:\n",
    "    1. For each prediction, compute error: (actual - predicted)\n",
    "    2. Square each error (makes all errors positive, penalizes big errors more)\n",
    "    3. Average across all examples\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : numpy array\n",
    "        Actual values (ground truth)\n",
    "    y_pred : numpy array  \n",
    "        Model's predictions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : The average squared error\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    Predicting house prices (in $100,000s):\n",
    "    Actual:    [3.0, 4.5, 2.0]  ($300K, $450K, $200K)\n",
    "    Predicted: [2.8, 4.2, 2.5]  ($280K, $420K, $250K)\n",
    "    \n",
    "    Errors:    [0.2, 0.3, -0.5]\n",
    "    Squared:   [0.04, 0.09, 0.25]\n",
    "    MSE:       0.1267\n",
    "    \n",
    "    Interpretation: On average, our squared error is 0.127, meaning\n",
    "    typical error is roughly sqrt(0.127) â‰ˆ 0.36 ($36,000)\n",
    "    \"\"\"\n",
    "    # Step 1: Compute errors\n",
    "    errors = y_true - y_pred\n",
    "    \n",
    "    # Step 2: Square the errors\n",
    "    # This makes negative errors positive AND penalizes large errors more\n",
    "    squared_errors = errors ** 2\n",
    "    \n",
    "    # Step 3: Average across all examples\n",
    "    mse = np.mean(squared_errors)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "\n",
    "# Example: House price prediction\n",
    "y_true = np.array([3.0, 4.5, 2.0])  # Actual prices\n",
    "y_pred = np.array([2.8, 4.2, 2.5])  # Predicted prices\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"Typical error magnitude: ${np.sqrt(mse) * 100000:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ffd0cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross-Entropy Examples:\n",
      "--------------------------------------------------\n",
      "True=1, Pred=0.95 (confident, correct): Loss = 0.0513\n",
      "True=1, Pred=0.55 (uncertain, correct): Loss = 0.5978\n",
      "True=1, Pred=0.05 (confident, WRONG): Loss = 2.9957\n",
      "\n",
      "Notice: Confident wrong predictions have MUCH higher loss!\n",
      "This teaches the model to be uncertain when it doesn't know.\n"
     ]
    }
   ],
   "source": [
    "def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy loss for binary classification.\n",
    "    \n",
    "    How it works:\n",
    "    - If true label is 1, we want predicted probability to be HIGH\n",
    "      Loss = -log(predicted). If predicted=0.9, loss=-log(0.9)=0.1 (low, good!)\n",
    "                              If predicted=0.1, loss=-log(0.1)=2.3 (high, bad!)\n",
    "    \n",
    "    - If true label is 0, we want predicted probability to be LOW\n",
    "      Loss = -log(1-predicted). If predicted=0.1, loss=-log(0.9)=0.1 (low, good!)\n",
    "                                If predicted=0.9, loss=-log(0.1)=2.3 (high, bad!)\n",
    "    \n",
    "    Key insight: The -log function heavily penalizes confident WRONG predictions.\n",
    "    - Predicting 0.99 when true label is 0 â†’ loss â‰ˆ 4.6 (very bad!)\n",
    "    - Predicting 0.51 when true label is 0 â†’ loss â‰ˆ 0.7 (not great, but not terrible)\n",
    "    \n",
    "    This encourages the model to only be confident when it's right.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : numpy array\n",
    "        Actual labels (0 or 1)\n",
    "    y_pred : numpy array\n",
    "        Predicted probabilities (between 0 and 1)\n",
    "    epsilon : float\n",
    "        Small value to prevent log(0) which is undefined\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : The average binary cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Clip predictions to prevent log(0) or log(1)\n",
    "    # log(0) = -infinity, which would break our computation\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Compute loss for each example\n",
    "    # When y_true=1: loss = -log(y_pred)\n",
    "    # When y_true=0: loss = -log(1 - y_pred)\n",
    "    # This formula handles both cases elegantly:\n",
    "    individual_losses = -(\n",
    "        y_true * np.log(y_pred) +           # Active when y_true=1\n",
    "        (1 - y_true) * np.log(1 - y_pred)   # Active when y_true=0\n",
    "    )\n",
    "    \n",
    "    # Average across all examples\n",
    "    bce = np.mean(individual_losses)\n",
    "    \n",
    "    return bce\n",
    "\n",
    "\n",
    "# Example: Loan approval classification\n",
    "print(\"Binary Cross-Entropy Examples:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Case 1: Correct and confident prediction\n",
    "y_true = np.array([1])       # Should be approved\n",
    "y_pred = np.array([0.95])    # Model is confident: 95% approval\n",
    "loss = binary_cross_entropy(y_true, y_pred)\n",
    "print(f\"True=1, Pred=0.95 (confident, correct): Loss = {loss:.4f}\")\n",
    "\n",
    "# Case 2: Correct but uncertain prediction\n",
    "y_pred = np.array([0.55])    # Model is uncertain: 55% approval\n",
    "loss = binary_cross_entropy(y_true, y_pred)\n",
    "print(f\"True=1, Pred=0.55 (uncertain, correct): Loss = {loss:.4f}\")\n",
    "\n",
    "# Case 3: Wrong and confident prediction (VERY BAD)\n",
    "y_pred = np.array([0.05])    # Model is confident it should be rejected!\n",
    "loss = binary_cross_entropy(y_true, y_pred)\n",
    "print(f\"True=1, Pred=0.05 (confident, WRONG): Loss = {loss:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Notice: Confident wrong predictions have MUCH higher loss!\")\n",
    "print(\"This teaches the model to be uncertain when it doesn't know.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b9cbef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BACKPROPAGATION STEP BY STEP\n",
      "======================================================================\n",
      "\n",
      "--- NETWORK PARAMETERS ---\n",
      "W1 (hidden layer weights):\n",
      "[[0.1 0.2]\n",
      " [0.3 0.4]]\n",
      "b1 (hidden layer biases): [0.1 0.2]\n",
      "W2 (output layer weights): [[0.5 0.6]]\n",
      "b2 (output layer bias): [0.3]\n",
      "\n",
      "--- INPUT ---\n",
      "X (input features): [0.5 0.8]\n",
      "Y (true label): [1.]\n",
      "\n",
      "======================================================================\n",
      "FORWARD PASS\n",
      "======================================================================\n",
      "\n",
      "[Hidden Layer] Z1 = W1 @ X + b1\n",
      "  W1 @ X = [[0.1 0.2]\n",
      " [0.3 0.4]] @ [0.5 0.8] = [0.21 0.47]\n",
      "  Z1 = [0.21 0.47] + [0.1 0.2] = [0.31 0.67]\n",
      "\n",
      "[Hidden Layer] A1 = ReLU(Z1) = max(0, Z1)\n",
      "  A1 = max(0, [0.31 0.67]) = [0.31 0.67]\n",
      "\n",
      "[Output Layer] Z2 = W2 @ A1 + b2\n",
      "  Z2 = [[0.5 0.6]] @ [0.31 0.67] + [0.3] = [0.857]\n",
      "\n",
      "[Output Layer] A2 = sigmoid(Z2)\n",
      "  A2 = sigmoid([0.857]) = [0.70203349]\n",
      "\n",
      "[Loss] Binary Cross-Entropy\n",
      "  L = 0.353774\n",
      "  Interpretation: Predicted 0.7020, true label is 1.0\n",
      "  Error: We predicted 70.2% probability, should be 100%\n",
      "\n",
      "======================================================================\n",
      "BACKWARD PASS\n",
      "======================================================================\n",
      "\n",
      "We now compute: How does changing each weight affect the loss?\n",
      "\n",
      "[Step 1] Gradient of Loss w.r.t. Output (dL/dA2)\n",
      "  Formula: dL/dA2 = -Y/A2 + (1-Y)/(1-A2)\n",
      "  dL/dA2 = -1.424433\n",
      "  Meaning: If we increase A2 slightly, loss changes by this amount\n",
      "\n",
      "[Step 2] Chain through Sigmoid (dL/dZ2 = dL/dA2 Ã— dA2/dZ2)\n",
      "  Sigmoid derivative: dA2/dZ2 = A2 Ã— (1-A2) = 0.7020 Ã— 0.2980 = 0.209182\n",
      "  Chain rule: dL/dZ2 = -1.424433 Ã— 0.209182 = -0.297967\n",
      "\n",
      "[Step 3] Gradients for Output Layer (W2, b2)\n",
      "  dL/dW2 = dL/dZ2 Ã— A1áµ€ = -0.297967 Ã— [0.31 0.67] = [-0.09236962 -0.19963756]\n",
      "  dL/db2 = dL/dZ2 = [-0.29796651]\n",
      "  Meaning: These tell us how to adjust W2 and b2 to reduce loss\n",
      "\n",
      "[Step 4] Backpropagate error to Hidden Layer (dL/dA1)\n",
      "  dL/dA1 = W2áµ€ Ã— dL/dZ2 = [0.5 0.6] Ã— -0.297967 = [-0.14898326 -0.17877991]\n",
      "  Meaning: This is how much each hidden neuron contributed to the error\n",
      "\n",
      "[Step 5] Chain through ReLU (dL/dZ1 = dL/dA1 Ã— dA1/dZ1)\n",
      "  ReLU derivative: dA1/dZ1 = [1. 1.] (1 if Z1>0, else 0)\n",
      "  dL/dZ1 = [-0.14898326 -0.17877991] Ã— [1. 1.] = [-0.14898326 -0.17877991]\n",
      "\n",
      "[Step 6] Gradients for Hidden Layer (W1, b1)\n",
      "  dL/dW1 = dL/dZ1 Ã— Xáµ€:\n",
      "    [[-0.07449163 -0.1191866 ]\n",
      " [-0.08938995 -0.14302393]]\n",
      "  dL/db1 = [-0.14898326 -0.17877991]\n",
      "\n",
      "======================================================================\n",
      "WEIGHT UPDATE (Gradient Descent)\n",
      "======================================================================\n",
      "\n",
      "Learning rate: 0.1\n",
      "Update rule: W_new = W_old - learning_rate Ã— gradient\n",
      "\n",
      "W2: [0.5 0.6] â†’ [0.50923696 0.61996376]\n",
      "b2: [0.3] â†’ [0.32979665]\n",
      "W1:\n",
      "[[0.1 0.2]\n",
      " [0.3 0.4]] â†’\n",
      "[[0.10744916 0.21191866]\n",
      " [0.308939   0.41430239]]\n",
      "b1: [0.1 0.2] â†’ [0.11489833 0.21787799]\n",
      "\n",
      "======================================================================\n",
      "VERIFY: Did the update help?\n",
      "======================================================================\n",
      "\n",
      "Before update:\n",
      "  Prediction: 0.7020 (target: 1.0)\n",
      "  Loss: 0.353774\n",
      "\n",
      "After update:\n",
      "  Prediction: 0.7188 (target: 1.0)\n",
      "  Loss: 0.330227\n",
      "\n",
      "Improvement: Loss decreased by 0.023548 (6.7%)\n",
      "The prediction moved closer to the target! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def backprop_step_by_step():\n",
    "    \"\"\"\n",
    "    Demonstrate backpropagation with concrete numbers.\n",
    "    \n",
    "    Network: 2 inputs â†’ 2 hidden neurons â†’ 1 output\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"BACKPROPAGATION STEP BY STEP\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SETUP: Define a simple network with known weights\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Network architecture: 2 â†’ 2 â†’ 1\n",
    "    \n",
    "    # Hidden layer weights (2 neurons, each receiving 2 inputs)\n",
    "    W1 = np.array([\n",
    "        [0.1, 0.2],   # Neuron 1 weights\n",
    "        [0.3, 0.4]    # Neuron 2 weights\n",
    "    ])\n",
    "    b1 = np.array([[0.1], [0.2]])  # Biases for hidden neurons\n",
    "    \n",
    "    # Output layer weights (1 neuron receiving 2 inputs from hidden)\n",
    "    W2 = np.array([[0.5, 0.6]])  # Single output neuron\n",
    "    b2 = np.array([[0.3]])       # Bias for output\n",
    "    \n",
    "    print(\"\\n--- NETWORK PARAMETERS ---\")\n",
    "    print(f\"W1 (hidden layer weights):\\n{W1}\")\n",
    "    print(f\"b1 (hidden layer biases): {b1.flatten()}\")\n",
    "    print(f\"W2 (output layer weights): {W2}\")\n",
    "    print(f\"b2 (output layer bias): {b2.flatten()}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # INPUT DATA\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Single training example\n",
    "    X = np.array([[0.5], [0.8]])  # Input features\n",
    "    Y = np.array([[1.0]])          # True label (should output 1)\n",
    "    \n",
    "    print(f\"\\n--- INPUT ---\")\n",
    "    print(f\"X (input features): {X.flatten()}\")\n",
    "    print(f\"Y (true label): {Y.flatten()}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FORWARD PASS (compute predictions)\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FORWARD PASS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # --- Hidden Layer ---\n",
    "    # Z1 = W1 @ X + b1\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    print(f\"\\n[Hidden Layer] Z1 = W1 @ X + b1\")\n",
    "    print(f\"  W1 @ X = {W1} @ {X.flatten()} = {np.dot(W1, X).flatten()}\")\n",
    "    print(f\"  Z1 = {np.dot(W1, X).flatten()} + {b1.flatten()} = {Z1.flatten()}\")\n",
    "    \n",
    "    # Apply ReLU: A1 = max(0, Z1)\n",
    "    A1 = np.maximum(0, Z1)\n",
    "    print(f\"\\n[Hidden Layer] A1 = ReLU(Z1) = max(0, Z1)\")\n",
    "    print(f\"  A1 = max(0, {Z1.flatten()}) = {A1.flatten()}\")\n",
    "    \n",
    "    # --- Output Layer ---\n",
    "    # Z2 = W2 @ A1 + b2\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    print(f\"\\n[Output Layer] Z2 = W2 @ A1 + b2\")\n",
    "    print(f\"  Z2 = {W2} @ {A1.flatten()} + {b2.flatten()} = {Z2.flatten()}\")\n",
    "    \n",
    "    # Apply Sigmoid: A2 = 1 / (1 + exp(-Z2))\n",
    "    A2 = 1 / (1 + np.exp(-Z2))\n",
    "    print(f\"\\n[Output Layer] A2 = sigmoid(Z2)\")\n",
    "    print(f\"  A2 = sigmoid({Z2.flatten()}) = {A2.flatten()}\")\n",
    "    \n",
    "    # --- Compute Loss ---\n",
    "    # Binary Cross-Entropy: L = -[Y*log(A2) + (1-Y)*log(1-A2)]\n",
    "    epsilon = 1e-15\n",
    "    L = -np.mean(Y * np.log(A2 + epsilon) + (1 - Y) * np.log(1 - A2 + epsilon))\n",
    "    print(f\"\\n[Loss] Binary Cross-Entropy\")\n",
    "    print(f\"  L = {L:.6f}\")\n",
    "    print(f\"  Interpretation: Predicted {A2[0,0]:.4f}, true label is {Y[0,0]}\")\n",
    "    print(f\"  Error: We predicted {A2[0,0]:.1%} probability, should be {Y[0,0]:.0%}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # BACKWARD PASS (compute gradients)\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BACKWARD PASS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nWe now compute: How does changing each weight affect the loss?\")\n",
    "    \n",
    "    m = 1  # Number of examples (batch size)\n",
    "    \n",
    "    # --- Step 1: Gradient of Loss w.r.t. A2 ---\n",
    "    # For BCE: dL/dA2 = -Y/A2 + (1-Y)/(1-A2)\n",
    "    dL_dA2 = -(Y / (A2 + epsilon)) + (1 - Y) / (1 - A2 + epsilon)\n",
    "    print(f\"\\n[Step 1] Gradient of Loss w.r.t. Output (dL/dA2)\")\n",
    "    print(f\"  Formula: dL/dA2 = -Y/A2 + (1-Y)/(1-A2)\")\n",
    "    print(f\"  dL/dA2 = {dL_dA2.flatten()[0]:.6f}\")\n",
    "    print(f\"  Meaning: If we increase A2 slightly, loss changes by this amount\")\n",
    "    \n",
    "    # --- Step 2: Gradient through Sigmoid ---\n",
    "    # Sigmoid derivative: dA2/dZ2 = A2 * (1 - A2)\n",
    "    dA2_dZ2 = A2 * (1 - A2)\n",
    "    dL_dZ2 = dL_dA2 * dA2_dZ2  # Chain rule!\n",
    "    print(f\"\\n[Step 2] Chain through Sigmoid (dL/dZ2 = dL/dA2 Ã— dA2/dZ2)\")\n",
    "    print(f\"  Sigmoid derivative: dA2/dZ2 = A2 Ã— (1-A2) = {A2[0,0]:.4f} Ã— {1-A2[0,0]:.4f} = {dA2_dZ2[0,0]:.6f}\")\n",
    "    print(f\"  Chain rule: dL/dZ2 = {dL_dA2[0,0]:.6f} Ã— {dA2_dZ2[0,0]:.6f} = {dL_dZ2[0,0]:.6f}\")\n",
    "    \n",
    "    # --- Step 3: Gradients for Output Layer Weights ---\n",
    "    # dL/dW2 = dL/dZ2 Ã— dZ2/dW2 = dL/dZ2 Ã— A1\n",
    "    dL_dW2 = np.dot(dL_dZ2, A1.T) / m\n",
    "    dL_db2 = np.sum(dL_dZ2, axis=1, keepdims=True) / m\n",
    "    print(f\"\\n[Step 3] Gradients for Output Layer (W2, b2)\")\n",
    "    print(f\"  dL/dW2 = dL/dZ2 Ã— A1áµ€ = {dL_dZ2[0,0]:.6f} Ã— {A1.flatten()} = {dL_dW2.flatten()}\")\n",
    "    print(f\"  dL/db2 = dL/dZ2 = {dL_db2.flatten()}\")\n",
    "    print(f\"  Meaning: These tell us how to adjust W2 and b2 to reduce loss\")\n",
    "    \n",
    "    # --- Step 4: Backpropagate to Hidden Layer ---\n",
    "    # dL/dA1 = W2áµ€ Ã— dL/dZ2\n",
    "    dL_dA1 = np.dot(W2.T, dL_dZ2)\n",
    "    print(f\"\\n[Step 4] Backpropagate error to Hidden Layer (dL/dA1)\")\n",
    "    print(f\"  dL/dA1 = W2áµ€ Ã— dL/dZ2 = {W2.T.flatten()} Ã— {dL_dZ2[0,0]:.6f} = {dL_dA1.flatten()}\")\n",
    "    print(f\"  Meaning: This is how much each hidden neuron contributed to the error\")\n",
    "    \n",
    "    # --- Step 5: Gradient through ReLU ---\n",
    "    # ReLU derivative: 1 if Z1 > 0, else 0\n",
    "    dA1_dZ1 = (Z1 > 0).astype(float)  # 1 where Z1 > 0, else 0\n",
    "    dL_dZ1 = dL_dA1 * dA1_dZ1  # Chain rule\n",
    "    print(f\"\\n[Step 5] Chain through ReLU (dL/dZ1 = dL/dA1 Ã— dA1/dZ1)\")\n",
    "    print(f\"  ReLU derivative: dA1/dZ1 = {dA1_dZ1.flatten()} (1 if Z1>0, else 0)\")\n",
    "    print(f\"  dL/dZ1 = {dL_dA1.flatten()} Ã— {dA1_dZ1.flatten()} = {dL_dZ1.flatten()}\")\n",
    "    \n",
    "    # --- Step 6: Gradients for Hidden Layer Weights ---\n",
    "    dL_dW1 = np.dot(dL_dZ1, X.T) / m\n",
    "    dL_db1 = np.sum(dL_dZ1, axis=1, keepdims=True) / m\n",
    "    print(f\"\\n[Step 6] Gradients for Hidden Layer (W1, b1)\")\n",
    "    print(f\"  dL/dW1 = dL/dZ1 Ã— Xáµ€:\")\n",
    "    print(f\"    {dL_dW1}\")\n",
    "    print(f\"  dL/db1 = {dL_db1.flatten()}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # WEIGHT UPDATE\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"WEIGHT UPDATE (Gradient Descent)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    learning_rate = 0.1\n",
    "    print(f\"\\nLearning rate: {learning_rate}\")\n",
    "    print(f\"Update rule: W_new = W_old - learning_rate Ã— gradient\")\n",
    "    \n",
    "    W2_new = W2 - learning_rate * dL_dW2\n",
    "    b2_new = b2 - learning_rate * dL_db2\n",
    "    W1_new = W1 - learning_rate * dL_dW1\n",
    "    b1_new = b1 - learning_rate * dL_db1\n",
    "    \n",
    "    print(f\"\\nW2: {W2.flatten()} â†’ {W2_new.flatten()}\")\n",
    "    print(f\"b2: {b2.flatten()} â†’ {b2_new.flatten()}\")\n",
    "    print(f\"W1:\\n{W1} â†’\\n{W1_new}\")\n",
    "    print(f\"b1: {b1.flatten()} â†’ {b1_new.flatten()}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # VERIFY IMPROVEMENT\n",
    "    # =========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VERIFY: Did the update help?\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Forward pass with new weights\n",
    "    Z1_new = np.dot(W1_new, X) + b1_new\n",
    "    A1_new = np.maximum(0, Z1_new)\n",
    "    Z2_new = np.dot(W2_new, A1_new) + b2_new\n",
    "    A2_new = 1 / (1 + np.exp(-Z2_new))\n",
    "    L_new = -np.mean(Y * np.log(A2_new + epsilon) + (1 - Y) * np.log(1 - A2_new + epsilon))\n",
    "    \n",
    "    print(f\"\\nBefore update:\")\n",
    "    print(f\"  Prediction: {A2[0,0]:.4f} (target: {Y[0,0]:.1f})\")\n",
    "    print(f\"  Loss: {L:.6f}\")\n",
    "    \n",
    "    print(f\"\\nAfter update:\")\n",
    "    print(f\"  Prediction: {A2_new[0,0]:.4f} (target: {Y[0,0]:.1f})\")\n",
    "    print(f\"  Loss: {L_new:.6f}\")\n",
    "    \n",
    "    print(f\"\\nImprovement: Loss decreased by {L - L_new:.6f} ({(L-L_new)/L*100:.1f}%)\")\n",
    "    print(\"The prediction moved closer to the target! ðŸŽ‰\")\n",
    "    \n",
    "    return {\n",
    "        'W1': W1_new, 'b1': b1_new, 'W2': W2_new, 'b2': b2_new,\n",
    "        'final_loss': L_new\n",
    "    }\n",
    "\n",
    "\n",
    "# Run the demonstration\n",
    "result = backprop_step_by_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696b468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LEARNING RATE COMPARISON\n",
      "============================================================\n",
      "Minimizing f(x) = xÂ², starting at x = 10\n",
      "Optimal solution: x = 0\n",
      "\n",
      "Learning rate = 0.01:\n",
      "  After 20 steps: x = 6.676080, f(x) = 44.570040\n",
      "  Result: STILL CONVERGING (learning rate may be too low)\n",
      "\n",
      "Learning rate = 0.1:\n",
      "  After 20 steps: x = 0.115292, f(x) = 0.013292\n",
      "  Result: STILL CONVERGING (learning rate may be too low)\n",
      "\n",
      "Learning rate = 0.5:\n",
      "  After 20 steps: x = 0.000000, f(x) = 0.000000\n",
      "  Result: CONVERGED (good learning rate)\n",
      "\n",
      "Learning rate = 0.9:\n",
      "  After 20 steps: x = 0.115292, f(x) = 0.013292\n",
      "  Result: STILL CONVERGING (learning rate may be too low)\n",
      "\n",
      "Learning rate = 5.0:\n",
      "  Step 10: DIVERGED! (x = 3.49e+10)\n",
      "  Result: DIVERGED (learning rate too high)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def learning_rate_demo():\n",
    "    \"\"\"\n",
    "    Demonstrate the effect of learning rate.\n",
    "    \n",
    "    We'll minimize a simple function: f(x) = xÂ²\n",
    "    The minimum is at x = 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    def f(x):\n",
    "        return x ** 2\n",
    "    \n",
    "    def gradient(x):\n",
    "        return 2 * x  # Derivative of xÂ²\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"LEARNING RATE COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Minimizing f(x) = xÂ², starting at x = 10\")\n",
    "    print(\"Optimal solution: x = 0\")\n",
    "    print()\n",
    "    \n",
    "    learning_rates = [0.01, 0.1, 0.5, 0.9, 5.0]\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        x = 10.0  # Starting point\n",
    "        history = [x]\n",
    "        \n",
    "        print(f\"Learning rate = {lr}:\")\n",
    "        \n",
    "        for step in range(20):\n",
    "            grad = gradient(x)\n",
    "            x = x - lr * grad  # Gradient descent update\n",
    "            history.append(x)\n",
    "            \n",
    "            # Check for divergence\n",
    "            if abs(x) > 1e10:\n",
    "                print(f\"  Step {step+1}: DIVERGED! (x = {x:.2e})\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"  After 20 steps: x = {x:.6f}, f(x) = {f(x):.6f}\")\n",
    "        \n",
    "        # Categorize behavior\n",
    "        if abs(x) > 1e10:\n",
    "            print(f\"  Result: DIVERGED (learning rate too high)\")\n",
    "        elif abs(x) < 0.01:\n",
    "            print(f\"  Result: CONVERGED (good learning rate)\")\n",
    "        else:\n",
    "            print(f\"  Result: STILL CONVERGING (learning rate may be too low)\")\n",
    "        print()\n",
    "\n",
    "\n",
    "learning_rate_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c6721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    \"\"\"\n",
    "    Adam optimizer - the practical default for most neural networks.\n",
    "    \n",
    "    Key ideas:\n",
    "    1. Momentum: Keep a running average of past gradients.\n",
    "       If gradients consistently point the same direction, speed up.\n",
    "       This helps escape flat regions and smooth out noise.\n",
    "    \n",
    "    2. Adaptive learning rates: Keep track of how much each parameter's\n",
    "       gradient varies. Parameters with wildly varying gradients get\n",
    "       smaller learning rates. Parameters with consistent gradients\n",
    "       get larger learning rates.\n",
    "    \n",
    "    Default hyperparameters (rarely need changing):\n",
    "    - learning_rate = 0.001\n",
    "    - beta1 = 0.9 (momentum decay)\n",
    "    - beta2 = 0.999 (squared gradient decay)\n",
    "    - epsilon = 1e-8 (prevents division by zero)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        learning_rate : float\n",
    "            Base step size. 0.001 is good for most problems.\n",
    "            \n",
    "        beta1 : float\n",
    "            Decay rate for first moment (gradient average).\n",
    "            0.9 means \"remember 90% of past gradients\".\n",
    "            \n",
    "        beta2 : float\n",
    "            Decay rate for second moment (squared gradient average).\n",
    "            0.999 means \"remember 99.9% of past squared gradients\".\n",
    "            \n",
    "        epsilon : float\n",
    "            Small constant to prevent division by zero.\n",
    "        \"\"\"\n",
    "        self.lr = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # These store running averages for each parameter\n",
    "        self.m = {}  # First moment (gradient average)\n",
    "        self.v = {}  # Second moment (squared gradient average)\n",
    "        self.t = 0   # Timestep counter\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        \"\"\"\n",
    "        Update parameters using Adam algorithm.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        params : dict\n",
    "            Dictionary of parameters, e.g., {'W1': array, 'b1': array, ...}\n",
    "        grads : dict\n",
    "            Dictionary of gradients, e.g., {'dW1': array, 'db1': array, ...}\n",
    "        \"\"\"\n",
    "        self.t += 1  # Increment timestep\n",
    "        \n",
    "        for key in params:\n",
    "            # Get gradient for this parameter\n",
    "            # Convert 'W1' to 'dW1', 'b1' to 'db1', etc.\n",
    "            grad_key = 'd' + key\n",
    "            if grad_key not in grads:\n",
    "                continue\n",
    "            \n",
    "            grad = grads[grad_key]\n",
    "            \n",
    "            # Initialize moments on first update\n",
    "            if key not in self.m:\n",
    "                self.m[key] = np.zeros_like(params[key])\n",
    "                self.v[key] = np.zeros_like(params[key])\n",
    "            \n",
    "            # ================================================================\n",
    "            # Step 1: Update first moment (momentum)\n",
    "            # ================================================================\n",
    "            # m = Î²â‚ Ã— m + (1 - Î²â‚) Ã— gradient\n",
    "            # This is a weighted average of past gradients\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grad\n",
    "            \n",
    "            # ================================================================\n",
    "            # Step 2: Update second moment (adaptive learning rate)\n",
    "            # ================================================================\n",
    "            # v = Î²â‚‚ Ã— v + (1 - Î²â‚‚) Ã— gradientÂ²\n",
    "            # This tracks how much the gradient varies\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grad ** 2)\n",
    "            \n",
    "            # ================================================================\n",
    "            # Step 3: Bias correction\n",
    "            # ================================================================\n",
    "            # The moments are biased toward zero initially (they start at 0)\n",
    "            # This correction compensates for that bias\n",
    "            m_corrected = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "            v_corrected = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # ================================================================\n",
    "            # Step 4: Update parameter\n",
    "            # ================================================================\n",
    "            # The key insight: we divide by sqrt(v), which gives each parameter\n",
    "            # its own effective learning rate based on gradient history\n",
    "            params[key] -= self.lr * m_corrected / (np.sqrt(v_corrected) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b0ddb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ADAM vs VANILLA GRADIENT DESCENT\n",
      "============================================================\n",
      "Minimizing f(x) = xÂ², starting at x = 10\n",
      "\n",
      "Vanilla GD (lr=0.1):\n",
      "  Step 5:  x = 3.276800\n",
      "  Step 10: x = 1.073742\n",
      "  Step 20: x = 0.115292\n",
      "\n",
      "Adam (lr=0.5):\n",
      "  Step 5:  x = 7.514779\n",
      "  Step 10: x = 5.122934\n",
      "  Step 20: x = 1.112262\n",
      "\n",
      "Adam adapts its effective learning rate based on gradient history.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def adam_demo():\n",
    "    \"\"\"\n",
    "    Compare Adam vs vanilla gradient descent on f(x) = xÂ²\n",
    "    \"\"\"\n",
    "    def f(x):\n",
    "        return x ** 2\n",
    "    \n",
    "    def gradient(x):\n",
    "        return 2 * x\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ADAM vs VANILLA GRADIENT DESCENT\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Minimizing f(x) = xÂ², starting at x = 10\\n\")\n",
    "    \n",
    "    # Vanilla GD with a conservative learning rate\n",
    "    x_gd = 10.0\n",
    "    lr = 0.1\n",
    "    gd_history = [x_gd]\n",
    "    \n",
    "    for _ in range(20):\n",
    "        x_gd = x_gd - lr * gradient(x_gd)\n",
    "        gd_history.append(x_gd)\n",
    "    \n",
    "    # Adam optimizer\n",
    "    x_adam = 10.0\n",
    "    adam = AdamOptimizer(learning_rate=0.5)  # Can use larger lr with Adam\n",
    "    adam_history = [x_adam]\n",
    "    \n",
    "    for _ in range(20):\n",
    "        # Adam expects dict format, we'll simulate it\n",
    "        params = {'x': np.array([x_adam])}\n",
    "        grads = {'dx': np.array([gradient(x_adam)])}\n",
    "        adam.update(params, grads)\n",
    "        x_adam = params['x'][0]\n",
    "        adam_history.append(x_adam)\n",
    "    \n",
    "    print(f\"Vanilla GD (lr={lr}):\")\n",
    "    print(f\"  Step 5:  x = {gd_history[5]:.6f}\")\n",
    "    print(f\"  Step 10: x = {gd_history[10]:.6f}\")\n",
    "    print(f\"  Step 20: x = {gd_history[20]:.6f}\")\n",
    "    print()\n",
    "    print(f\"Adam (lr=0.5):\")\n",
    "    print(f\"  Step 5:  x = {adam_history[5]:.6f}\")\n",
    "    print(f\"  Step 10: x = {adam_history[10]:.6f}\")\n",
    "    print(f\"  Step 20: x = {adam_history[20]:.6f}\")\n",
    "    print()\n",
    "    print(\"Adam adapts its effective learning rate based on gradient history.\")\n",
    "\n",
    "adam_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bc7a28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient magnitude through layers with Sigmoid:\n",
      "--------------------------------------------------\n",
      "  Layer  1: gradient magnitude = 2.50e-01\n",
      "  Layer  2: gradient magnitude = 6.25e-02\n",
      "  Layer  3: gradient magnitude = 1.56e-02\n",
      "  Layer  4: gradient magnitude = 3.91e-03\n",
      "  Layer  5: gradient magnitude = 9.77e-04\n",
      "  Layer  6: gradient magnitude = 2.44e-04\n",
      "  Layer  7: gradient magnitude = 6.10e-05\n",
      "  Layer  8: gradient magnitude = 1.53e-05\n",
      "  Layer  9: gradient magnitude = 3.81e-06\n",
      "  Layer 10: gradient magnitude = 9.54e-07\n",
      "\n",
      "After 10 layers: gradient is 9.54e-07\n",
      "This is essentially ZERO - early layers learn nothing!\n",
      "\n",
      "This is why deep networks historically couldn't be trained,\n",
      "until ReLU and other techniques came along.\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_vanishing_gradient():\n",
    "    \"\"\"\n",
    "    Show why deep networks with sigmoid activations are hard to train.\n",
    "    \n",
    "    Sigmoid derivative: Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))\n",
    "    Maximum value: 0.25 (when z = 0)\n",
    "    \n",
    "    In backpropagation, we multiply by this derivative at each layer.\n",
    "    After many layers: 0.25 Ã— 0.25 Ã— 0.25 Ã— ... â†’ ~0\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Gradient magnitude through layers with Sigmoid:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Maximum sigmoid derivative\n",
    "    sigmoid_deriv_max = 0.25\n",
    "    \n",
    "    gradient = 1.0  # Start with gradient = 1 from the loss\n",
    "    \n",
    "    for layer in range(1, 11):\n",
    "        # Each layer multiplies gradient by sigmoid derivative\n",
    "        gradient = gradient * sigmoid_deriv_max\n",
    "        print(f\"  Layer {layer:2d}: gradient magnitude = {gradient:.2e}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"After 10 layers: gradient is {gradient:.2e}\")\n",
    "    print(\"This is essentially ZERO - early layers learn nothing!\")\n",
    "    print()\n",
    "    print(\"This is why deep networks historically couldn't be trained,\")\n",
    "    print(\"until ReLU and other techniques came along.\")\n",
    "\n",
    "\n",
    "demonstrate_vanishing_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10890aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WHY WEIGHT INITIALIZATION MATTERS\n",
      "============================================================\n",
      "\n",
      "In deep networks, signals pass through many layers via matrix multiplications.\n",
      "Each layer multiplies by weights, so after N layers: signal â‰ˆ (weight_scale)^N\n",
      "\n",
      "If weights are too small (e.g., 0.001):\n",
      "  - Signal shrinks exponentially â†’ activations become ~0 â†’ \"dead network\"\n",
      "  - Gradients also vanish â†’ no learning happens\n",
      "\n",
      "If weights are too large (e.g., 1.0):\n",
      "  - Signal grows exponentially â†’ activations explode to infinity\n",
      "  - Gradients also explode â†’ NaN values, training crashes\n",
      "\n",
      "The fix: Scale weights so variance stays ~constant across layers.\n",
      "  - Xavier init: scale = sqrt(1/n)  â€” for tanh/sigmoid\n",
      "  - He init:     scale = sqrt(2/n)  â€” for ReLU (accounts for ReLU killing ~half the values)\n",
      "\n",
      "Let's see this in action with a 10-layer network:\n",
      "\n",
      "\n",
      "Too small (0.001):\n",
      "  Initial activation std: 1.0372\n",
      "  After 10 layers, activation std: 0.000000\n",
      "  â†’ VANISHED! Network is effectively dead.\n",
      "\n",
      "Too large (1.0):\n",
      "  Initial activation std: 1.0061\n",
      "  After 10 layers, activation std: 27069794333.318287\n",
      "  â†’ EXPLODED! Network outputs are meaningless.\n",
      "\n",
      "He init (sqrt(2/n)):\n",
      "  Initial activation std: 1.0650\n",
      "  After 10 layers, activation std: 0.656139\n",
      "  â†’ STABLE. Good initialization!\n"
     ]
    }
   ],
   "source": [
    "def initialization_comparison():\n",
    "    \"\"\"\n",
    "    Demonstrate why initialization matters.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"WHY WEIGHT INITIALIZATION MATTERS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "In deep networks, signals pass through many layers via matrix multiplications.\n",
    "Each layer multiplies by weights, so after N layers: signal â‰ˆ (weight_scale)^N\n",
    "\n",
    "If weights are too small (e.g., 0.001):\n",
    "  - Signal shrinks exponentially â†’ activations become ~0 â†’ \"dead network\"\n",
    "  - Gradients also vanish â†’ no learning happens\n",
    "\n",
    "If weights are too large (e.g., 1.0):\n",
    "  - Signal grows exponentially â†’ activations explode to infinity\n",
    "  - Gradients also explode â†’ NaN values, training crashes\n",
    "\n",
    "The fix: Scale weights so variance stays ~constant across layers.\n",
    "  - Xavier init: scale = sqrt(1/n)  â€” for tanh/sigmoid\n",
    "  - He init:     scale = sqrt(2/n)  â€” for ReLU (accounts for ReLU killing ~half the values)\n",
    "\n",
    "Let's see this in action with a 10-layer network:\n",
    "\"\"\")\n",
    "    \n",
    "    # Simulate a 10-layer network\n",
    "    n_layers = 10\n",
    "    layer_size = 256\n",
    "    \n",
    "    for init_name, init_scale in [(\"Too small (0.001)\", 0.001),\n",
    "                                   (\"Too large (1.0)\", 1.0),\n",
    "                                   (\"He init (sqrt(2/n))\", np.sqrt(2/layer_size))]:\n",
    "        \n",
    "        # Start with input of reasonable magnitude\n",
    "        activations = np.random.randn(layer_size, 1)\n",
    "        \n",
    "        print(f\"\\n{init_name}:\")\n",
    "        print(f\"  Initial activation std: {np.std(activations):.4f}\")\n",
    "        \n",
    "        for layer in range(n_layers):\n",
    "            # Random weights with given initialization scale\n",
    "            W = np.random.randn(layer_size, layer_size) * init_scale\n",
    "            \n",
    "            # Forward pass: z = Wx, then ReLU\n",
    "            z = np.dot(W, activations)\n",
    "            activations = np.maximum(0, z)  # ReLU\n",
    "        \n",
    "        print(f\"  After {n_layers} layers, activation std: {np.std(activations):.6f}\")\n",
    "        \n",
    "        if np.std(activations) < 1e-10:\n",
    "            print(f\"  â†’ VANISHED! Network is effectively dead.\")\n",
    "        elif np.std(activations) > 1e10:\n",
    "            print(f\"  â†’ EXPLODED! Network outputs are meaningless.\")\n",
    "        else:\n",
    "            print(f\"  â†’ STABLE. Good initialization!\")\n",
    "\n",
    "\n",
    "initialization_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6138ee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DROPOUT â€” REGULARIZATION BY RANDOM NEURON DISABLING\n",
      "============================================================\n",
      "\n",
      "THE PROBLEM: Co-adaptation & Overfitting\n",
      "Neural networks can \"memorize\" training data by having neurons form brittle,\n",
      "complex dependencies on each other. Neuron A only works if B fires in a\n",
      "specific way â†’ fails on new data.\n",
      "\n",
      "THE SOLUTION: Dropout\n",
      "During TRAINING: Randomly \"drop\" neurons (set to 0) with probability p.\n",
      "During INFERENCE: Use all neurons, but scale outputs by (1-p).\n",
      "  (Or equivalently: scale during training by 1/(1-p), use as-is during inference)\n",
      "\n",
      "WHY IT WORKS:\n",
      "- Forces redundancy: No neuron can rely on another always being present\n",
      "- Implicit ensemble: Each forward pass uses a different \"sub-network\"\n",
      "- Like training 2^N networks (N = neurons) and averaging predictions\n",
      "\n",
      "TYPICAL VALUES: p = 0.2 to 0.5 (higher for larger layers)\n",
      "\n",
      "Let's see it in action:\n",
      "\n",
      "Original activations: [0.5 0.8 0.3 0.9 0.2]\n",
      "Dropout probability: 0.3 (30% of neurons dropped)\n",
      "Dropout mask: [1 0 1 1 0] (0 = dropped)\n",
      "After dropout: [0.714 0.    0.429 1.286 0.   ]\n",
      "\n",
      "Why this helps:\n",
      "- Prevents neurons from relying too much on specific other neurons\n",
      "- Forces network to learn redundant representations\n",
      "- Acts like training many different networks and averaging\n"
     ]
    }
   ],
   "source": [
    "def dropout_example():\n",
    "    \"\"\"\n",
    "    Demonstrate dropout - a regularization technique.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"DROPOUT â€” REGULARIZATION BY RANDOM NEURON DISABLING\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\"\"\n",
    "THE PROBLEM: Co-adaptation & Overfitting\n",
    "Neural networks can \"memorize\" training data by having neurons form brittle,\n",
    "complex dependencies on each other. Neuron A only works if B fires in a\n",
    "specific way â†’ fails on new data.\n",
    "\n",
    "THE SOLUTION: Dropout\n",
    "During TRAINING: Randomly \"drop\" neurons (set to 0) with probability p.\n",
    "During INFERENCE: Use all neurons, but scale outputs by (1-p).\n",
    "  (Or equivalently: scale during training by 1/(1-p), use as-is during inference)\n",
    "\n",
    "WHY IT WORKS:\n",
    "- Forces redundancy: No neuron can rely on another always being present\n",
    "- Implicit ensemble: Each forward pass uses a different \"sub-network\"\n",
    "- Like training 2^N networks (N = neurons) and averaging predictions\n",
    "\n",
    "TYPICAL VALUES: p = 0.2 to 0.5 (higher for larger layers)\n",
    "\n",
    "Let's see it in action:\n",
    "\"\"\")\n",
    "    \n",
    "    # Simulated layer activations\n",
    "    activations = np.array([0.5, 0.8, 0.3, 0.9, 0.2])\n",
    "    drop_prob = 0.3  # 30% of neurons will be \"dropped\"\n",
    "    \n",
    "    print(f\"Original activations: {activations}\")\n",
    "    print(f\"Dropout probability: {drop_prob} (30% of neurons dropped)\")\n",
    "    \n",
    "    # Create dropout mask\n",
    "    mask = np.random.rand(len(activations)) > drop_prob\n",
    "    print(f\"Dropout mask: {mask.astype(int)} (0 = dropped)\")\n",
    "    \n",
    "    # Apply dropout\n",
    "    # Note: We scale by 1/(1-p) to keep expected value the same\n",
    "    dropped = activations * mask / (1 - drop_prob)\n",
    "    print(f\"After dropout: {np.round(dropped, 3)}\")\n",
    "    print()\n",
    "    print(\"Why this helps:\")\n",
    "    print(\"- Prevents neurons from relying too much on specific other neurons\")\n",
    "    print(\"- Forces network to learn redundant representations\")\n",
    "    print(\"- Acts like training many different networks and averaging\")\n",
    "\n",
    "\n",
    "dropout_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cc72f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "THE COMPLETE PICTURE: FROM CONCEPTS TO WORKING NETWORK\n",
      "================================================================================\n",
      "\n",
      "This is where everything we've learned comes together into a working system.\n",
      "\n",
      "WHAT WE'VE COVERED:\n",
      "  1. Single neuron      â†’ weighted sum + activation = basic decision unit\n",
      "  2. Activation funcs   â†’ ReLU (hidden layers), Sigmoid (output probabilities)\n",
      "  3. Loss function      â†’ Binary cross-entropy measures prediction quality\n",
      "  4. Backpropagation    â†’ Chain rule computes how each weight affects loss\n",
      "  5. Gradient descent   â†’ Move weights in direction that reduces loss\n",
      "  6. Adam optimizer     â†’ Smart learning rates + momentum for faster training\n",
      "  7. Weight init        â†’ He initialization prevents vanishing/exploding signals\n",
      "  8. Dropout            â†’ Regularization to prevent overfitting\n",
      "\n",
      "NOW WE BUILD A NETWORK:\n",
      "\n",
      "    Architecture: 2 inputs â†’ 64 hidden neurons â†’ 1 output\n",
      "\n",
      "    Input Layer        Hidden Layer (ReLU)        Output Layer (Sigmoid)\n",
      "         â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—                            \n",
      "         â”‚                  â—                            \n",
      "         â”‚                  â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â— â†’ P(class=1)\n",
      "         â”‚                  â—                            \n",
      "         â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—                            \n",
      "      (x, y)           (64 neurons)                  (probability)\n",
      "\n",
      "THE TRAINING LOOP (what happens every iteration):\n",
      "\n",
      "    1. FORWARD PASS\n",
      "       Feed input through network â†’ get prediction\n",
      "\n",
      "    2. COMPUTE LOSS  \n",
      "       Compare prediction to true label â†’ get error signal\n",
      "\n",
      "    3. BACKWARD PASS (Backpropagation)\n",
      "       Compute gradient of loss w.r.t. each weight\n",
      "\n",
      "    4. UPDATE WEIGHTS (Adam)\n",
      "       Nudge weights to reduce loss\n",
      "\n",
      "    Repeat 1000x until network learns the pattern.\n",
      "\n",
      "WHY SPIRALS?\n",
      "\n",
      "    We'll train on two interleaved spirals - a classic non-linear problem.\n",
      "    A single neuron (linear classifier) CANNOT separate spirals.\n",
      "    But a network with hidden layers can learn the curved decision boundary.\n",
      "    This demonstrates why depth matters.\n",
      "\n",
      "Let's build it:\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TRAINING THE NETWORK\n",
      "======================================================================\n",
      "\n",
      "What you'll see below:\n",
      "- Loss decreasing: The network is getting better at predicting\n",
      "- Accuracy increasing: More correct classifications\n",
      "- Every 100 epochs: A snapshot of progress\n",
      "\n",
      "Watch the loss drop from ~0.7 (random guessing) toward ~0.1 (learned pattern).\n",
      "\n",
      "\n",
      "Dataset: 500 points, 2 features\n",
      "Classes: 250 class 0, 250 class 1\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Training a 2-layer network (2 â†’ 64 â†’ 1)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Epoch 100/1000 - Loss: 0.5326 - Accuracy: 64.60%\n",
      "Epoch 200/1000 - Loss: 0.5232 - Accuracy: 67.20%\n",
      "Epoch 300/1000 - Loss: 0.5206 - Accuracy: 67.80%\n",
      "Epoch 400/1000 - Loss: 0.5243 - Accuracy: 65.40%\n",
      "Epoch 500/1000 - Loss: 0.5187 - Accuracy: 68.00%\n",
      "Epoch 600/1000 - Loss: 0.5190 - Accuracy: 68.00%\n",
      "Epoch 700/1000 - Loss: 0.5218 - Accuracy: 65.60%\n",
      "Epoch 800/1000 - Loss: 0.5197 - Accuracy: 67.40%\n",
      "Epoch 900/1000 - Loss: 0.5220 - Accuracy: 67.80%\n",
      "Epoch 1000/1000 - Loss: 0.5213 - Accuracy: 67.20%\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE\n",
      "======================================================================\n",
      "Final Loss: 0.5213\n",
      "Final Accuracy: 66.40%\n",
      "\n",
      "WHAT JUST HAPPENED:\n",
      "\n",
      "1. We created 500 points in two interleaved spirals (impossible to separate \n",
      "   with a straight line).\n",
      "\n",
      "2. The network started with random weights (He initialization) and made \n",
      "   random predictions (~50% accuracy, loss ~0.7).\n",
      "\n",
      "3. Over 1000 iterations, the network:\n",
      "   - Forward pass: computed predictions\n",
      "   - Loss: measured how wrong it was\n",
      "   - Backprop: computed gradients (which direction to adjust each weight)\n",
      "   - Adam: updated weights intelligently\n",
      "\n",
      "4. The hidden layer learned to create a NON-LINEAR decision boundary that \n",
      "   curves around the spirals.\n",
      "\n",
      "KEY INSIGHT: A single neuron can only draw straight lines. Hidden layers \n",
      "with non-linear activations (ReLU) allow the network to learn ANY shape \n",
      "of decision boundary. That's the power of neural networks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# =============================================================================\n",
    "# PUTTING IT ALL TOGETHER: A COMPLETE NEURAL NETWORK\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "================================================================================\n",
    "THE COMPLETE PICTURE: FROM CONCEPTS TO WORKING NETWORK\n",
    "================================================================================\n",
    "\n",
    "This is where everything we've learned comes together into a working system.\n",
    "\n",
    "WHAT WE'VE COVERED:\n",
    "  1. Single neuron      â†’ weighted sum + activation = basic decision unit\n",
    "  2. Activation funcs   â†’ ReLU (hidden layers), Sigmoid (output probabilities)\n",
    "  3. Loss function      â†’ Binary cross-entropy measures prediction quality\n",
    "  4. Backpropagation    â†’ Chain rule computes how each weight affects loss\n",
    "  5. Gradient descent   â†’ Move weights in direction that reduces loss\n",
    "  6. Adam optimizer     â†’ Smart learning rates + momentum for faster training\n",
    "  7. Weight init        â†’ He initialization prevents vanishing/exploding signals\n",
    "  8. Dropout            â†’ Regularization to prevent overfitting\n",
    "\n",
    "NOW WE BUILD A NETWORK:\n",
    "\n",
    "    Architecture: 2 inputs â†’ 64 hidden neurons â†’ 1 output\n",
    "    \n",
    "    Input Layer        Hidden Layer (ReLU)        Output Layer (Sigmoid)\n",
    "         â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—                            \n",
    "         â”‚                  â—                            \n",
    "         â”‚                  â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â— â†’ P(class=1)\n",
    "         â”‚                  â—                            \n",
    "         â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—                            \n",
    "      (x, y)           (64 neurons)                  (probability)\n",
    "\n",
    "THE TRAINING LOOP (what happens every iteration):\n",
    "\n",
    "    1. FORWARD PASS\n",
    "       Feed input through network â†’ get prediction\n",
    "       \n",
    "    2. COMPUTE LOSS  \n",
    "       Compare prediction to true label â†’ get error signal\n",
    "       \n",
    "    3. BACKWARD PASS (Backpropagation)\n",
    "       Compute gradient of loss w.r.t. each weight\n",
    "       \n",
    "    4. UPDATE WEIGHTS (Adam)\n",
    "       Nudge weights to reduce loss\n",
    "\n",
    "    Repeat 1000x until network learns the pattern.\n",
    "\n",
    "WHY SPIRALS?\n",
    "\n",
    "    We'll train on two interleaved spirals - a classic non-linear problem.\n",
    "    A single neuron (linear classifier) CANNOT separate spirals.\n",
    "    But a network with hidden layers can learn the curved decision boundary.\n",
    "    This demonstrates why depth matters.\n",
    "\n",
    "Let's build it:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A complete 2-layer neural network for binary classification.\n",
    "    \n",
    "    This brings together everything we've learned:\n",
    "    - Forward pass with ReLU and Sigmoid activations\n",
    "    - Binary cross-entropy loss\n",
    "    - Backpropagation for computing gradients\n",
    "    - Adam optimizer for weight updates\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int = 1):\n",
    "        \"\"\"Initialize network with He initialization.\"\"\"\n",
    "        \n",
    "        # He initialization: scale by sqrt(2/fan_in)\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "        \n",
    "        # Initialize Adam optimizer state\n",
    "        self.adam_t = 0\n",
    "        self.adam_m = {'W1': 0, 'b1': 0, 'W2': 0, 'b2': 0}\n",
    "        self.adam_v = {'W1': 0, 'b1': 0, 'W2': 0, 'b2': 0}\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        # Hidden layer: Linear â†’ ReLU\n",
    "        self.Z1 = np.dot(self.W1, X) + self.b1\n",
    "        self.A1 = np.maximum(0, self.Z1)\n",
    "        \n",
    "        # Output layer: Linear â†’ Sigmoid\n",
    "        self.Z2 = np.dot(self.W2, self.A1) + self.b2\n",
    "        self.A2 = 1 / (1 + np.exp(-np.clip(self.Z2, -500, 500)))\n",
    "        \n",
    "        # Store for backprop\n",
    "        self.X = X\n",
    "        \n",
    "        return self.A2\n",
    "    \n",
    "    def compute_loss(self, Y: np.ndarray) -> float:\n",
    "        \"\"\"Compute binary cross-entropy loss.\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        epsilon = 1e-15\n",
    "        loss = -(1/m) * np.sum(\n",
    "            Y * np.log(self.A2 + epsilon) + \n",
    "            (1 - Y) * np.log(1 - self.A2 + epsilon)\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, Y: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Backward pass to compute gradients.\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dZ2 = self.A2 - Y  # Derivative of BCE + sigmoid combined\n",
    "        dW2 = (1/m) * np.dot(dZ2, self.A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dA1 = np.dot(self.W2.T, dZ2)\n",
    "        dZ1 = dA1 * (self.Z1 > 0).astype(float)  # ReLU derivative\n",
    "        dW1 = (1/m) * np.dot(dZ1, self.X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        \n",
    "        return {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
    "    \n",
    "    def update_weights(self, grads: Dict, lr: float = 0.001, \n",
    "                       beta1: float = 0.9, beta2: float = 0.999):\n",
    "        \"\"\"Update weights using Adam optimizer.\"\"\"\n",
    "        self.adam_t += 1\n",
    "        epsilon = 1e-8\n",
    "        \n",
    "        for key in ['W1', 'b1', 'W2', 'b2']:\n",
    "            param = getattr(self, key)\n",
    "            grad = grads['d' + key]\n",
    "            \n",
    "            # Update moments\n",
    "            self.adam_m[key] = beta1 * self.adam_m[key] + (1 - beta1) * grad\n",
    "            self.adam_v[key] = beta2 * self.adam_v[key] + (1 - beta2) * (grad ** 2)\n",
    "            \n",
    "            # Bias correction\n",
    "            m_hat = self.adam_m[key] / (1 - beta1 ** self.adam_t)\n",
    "            v_hat = self.adam_v[key] / (1 - beta2 ** self.adam_t)\n",
    "            \n",
    "            # Update parameter\n",
    "            setattr(self, key, param - lr * m_hat / (np.sqrt(v_hat) + epsilon))\n",
    "    \n",
    "    def train(self, X: np.ndarray, Y: np.ndarray, \n",
    "              epochs: int = 1000, lr: float = 0.01,\n",
    "              verbose: bool = True) -> List[float]:\n",
    "        \"\"\"Train the network.\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            predictions = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(Y)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            grads = self.backward(Y)\n",
    "            \n",
    "            # Update weights\n",
    "            self.update_weights(grads, lr=lr)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                accuracy = np.mean((predictions > 0.5) == Y)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss:.4f} - Accuracy: {accuracy:.2%}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions (returns probabilities).\"\"\"\n",
    "        return self.forward(X)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING EXAMPLE\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING THE NETWORK\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\"\"\n",
    "What you'll see below:\n",
    "- Loss decreasing: The network is getting better at predicting\n",
    "- Accuracy increasing: More correct classifications\n",
    "- Every 100 epochs: A snapshot of progress\n",
    "\n",
    "Watch the loss drop from ~0.7 (random guessing) toward ~0.1 (learned pattern).\n",
    "\"\"\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate synthetic dataset: Two interleaved spirals (hard to separate!)\n",
    "    def generate_spirals(n_points=500, noise=0.1):\n",
    "        \"\"\"Generate two interleaved spirals - a classic non-linear problem.\"\"\"\n",
    "        n = n_points // 2\n",
    "        \n",
    "        # Spiral 1 (class 0)\n",
    "        theta1 = np.linspace(0, 4 * np.pi, n) + np.random.randn(n) * noise\n",
    "        r1 = theta1 / (4 * np.pi)\n",
    "        x1 = r1 * np.cos(theta1)\n",
    "        y1 = r1 * np.sin(theta1)\n",
    "        \n",
    "        # Spiral 2 (class 1) - offset by pi\n",
    "        theta2 = np.linspace(0, 4 * np.pi, n) + np.pi + np.random.randn(n) * noise\n",
    "        r2 = theta2 / (4 * np.pi)\n",
    "        x2 = r2 * np.cos(theta2)\n",
    "        y2 = r2 * np.sin(theta2)\n",
    "        \n",
    "        # Combine\n",
    "        X = np.vstack([\n",
    "            np.hstack([x1, x2]),\n",
    "            np.hstack([y1, y2])\n",
    "        ])\n",
    "        Y = np.hstack([np.zeros(n), np.ones(n)]).reshape(1, -1)\n",
    "        \n",
    "        return X, Y\n",
    "    \n",
    "    # Generate data\n",
    "    X, Y = generate_spirals(n_points=500, noise=0.1)\n",
    "    print(f\"\\nDataset: {X.shape[1]} points, {X.shape[0]} features\")\n",
    "    print(f\"Classes: {int(np.sum(Y==0))} class 0, {int(np.sum(Y==1))} class 1\")\n",
    "    \n",
    "    # Create and train network\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"Training a 2-layer network (2 â†’ 64 â†’ 1)\")\n",
    "    print(\"-\"*70 + \"\\n\")\n",
    "    \n",
    "    net = NeuralNetwork(input_size=2, hidden_size=64, output_size=1)\n",
    "    losses = net.train(X, Y, epochs=1000, lr=0.1, verbose=True)\n",
    "    \n",
    "    # Final evaluation\n",
    "    predictions = net.predict(X)\n",
    "    final_accuracy = np.mean((predictions > 0.5) == Y)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TRAINING COMPLETE\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Final Loss: {losses[-1]:.4f}\")\n",
    "    print(f\"Final Accuracy: {final_accuracy:.2%}\")\n",
    "    print(f\"\"\"\n",
    "WHAT JUST HAPPENED:\n",
    "\n",
    "1. We created 500 points in two interleaved spirals (impossible to separate \n",
    "   with a straight line).\n",
    "\n",
    "2. The network started with random weights (He initialization) and made \n",
    "   random predictions (~50% accuracy, loss ~0.7).\n",
    "\n",
    "3. Over 1000 iterations, the network:\n",
    "   - Forward pass: computed predictions\n",
    "   - Loss: measured how wrong it was\n",
    "   - Backprop: computed gradients (which direction to adjust each weight)\n",
    "   - Adam: updated weights intelligently\n",
    "\n",
    "4. The hidden layer learned to create a NON-LINEAR decision boundary that \n",
    "   curves around the spirals.\n",
    "\n",
    "KEY INSIGHT: A single neuron can only draw straight lines. Hidden layers \n",
    "with non-linear activations (ReLU) allow the network to learn ANY shape \n",
    "of decision boundary. That's the power of neural networks.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a733bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
