{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1B: Demos\n",
        "\n",
        "Companion notebook with runnable demos for Part 1B concepts.\n",
        "\n",
        "**Prerequisites:**\n",
        "- Ollama installed (`brew install ollama` or https://ollama.ai)\n",
        "- Model: `ollama pull qwen3:4b` (recommended, ~2.6GB)\n",
        "- `pip install instructor outlines[ollama]`\n",
        "\n",
        "**Demos in this notebook:**\n",
        "1. Structured Output with Instructor + Ollama\n",
        "2. Structured Generation with Outlines\n",
        "\n",
        "**Guardrails demos → See `guardrails_demo.ipynb`** (separate notebook for dependency isolation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo 1: Structured Output with Instructor\n",
        "\n",
        "Instructor wraps LLM clients to return validated Pydantic objects instead of raw text.\n",
        "\n",
        "**Why this matters:**\n",
        "- LLMs generate text; applications consume structured data\n",
        "- Prompt-only approach: ~85% reliable (model sometimes adds explanations, breaks JSON)\n",
        "- Instructor: ~95-99% reliable (auto-retries with validation feedback)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m[INFO]\u001b[0m Ollama is running\n",
            "\u001b[92m[INFO]\u001b[0m Recommended: ollama pull qwen3:4b   (~2.6GB, great for structured output)\n",
            "\u001b[92m[INFO]\u001b[0m Smaller:     ollama pull qwen3:1.7b (~1.4GB, still good)\n",
            "\u001b[92m[INFO]\u001b[0m Reliable:    ollama pull qwen3:8b   (~5GB, best quality)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Available models:\n",
            "NAME           ID              SIZE      MODIFIED   \n",
            "qwen3:4b       359d7dd4bcda    2.5 GB    2 days ago    \n",
            "llama3.2:1b    baf6a787fdff    1.3 GB    2 days ago\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Setup - Logging and Ollama check\n",
        "import subprocess\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "# Configure logging for notebooks - color-coded levels\n",
        "class ColoredFormatter(logging.Formatter):\n",
        "    COLORS = {\n",
        "        'DEBUG': '\\033[90m',     # Gray\n",
        "        'INFO': '\\033[92m',      # Green\n",
        "        'WARNING': '\\033[93m',   # Yellow\n",
        "        'ERROR': '\\033[91m',     # Red\n",
        "        'RESET': '\\033[0m'\n",
        "    }\n",
        "    \n",
        "    def format(self, record):\n",
        "        color = self.COLORS.get(record.levelname, self.COLORS['RESET'])\n",
        "        reset = self.COLORS['RESET']\n",
        "        record.msg = f\"{color}[{record.levelname}]{reset} {record.msg}\"\n",
        "        return super().format(record)\n",
        "\n",
        "# Setup logger\n",
        "logger = logging.getLogger(\"demos\")\n",
        "logger.setLevel(logging.DEBUG)\n",
        "if not logger.handlers:\n",
        "    handler = logging.StreamHandler()\n",
        "    handler.setFormatter(ColoredFormatter('%(message)s'))\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "def check_ollama():\n",
        "    \"\"\"Check if Ollama is running and has a model.\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"ollama\", \"list\"], \n",
        "            capture_output=True, \n",
        "            text=True, \n",
        "            timeout=5\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            models = result.stdout.strip()\n",
        "            logger.info(\"Ollama is running\")\n",
        "            print(f\"\\nAvailable models:\\n{models}\")\n",
        "            return True\n",
        "        else:\n",
        "            logger.error(\"Ollama not responding. Run: ollama serve\")\n",
        "            return False\n",
        "    except FileNotFoundError:\n",
        "        logger.error(\"Ollama not installed. Install: brew install ollama\")\n",
        "        return False\n",
        "    except subprocess.TimeoutExpired:\n",
        "        logger.error(\"Ollama timed out. Run: ollama serve\")\n",
        "        return False\n",
        "\n",
        "ollama_ready = check_ollama()\n",
        "\n",
        "if ollama_ready:\n",
        "    print()\n",
        "    logger.info(\"Recommended: ollama pull qwen3:4b   (~2.6GB, great for structured output)\")\n",
        "    logger.info(\"Smaller:     ollama pull qwen3:1.7b (~1.4GB, still good)\")\n",
        "    logger.info(\"Reliable:    ollama pull qwen3:8b   (~5GB, best quality)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Schema defined: SupportTicket\n",
            "Fields: ['category', 'priority', 'summary', 'entities', 'sentiment']\n"
          ]
        }
      ],
      "source": [
        "# STEP 2: Define Pydantic schema for structured extraction\n",
        "from pydantic import BaseModel, Field, field_validator\n",
        "from typing import List\n",
        "from enum import Enum\n",
        "\n",
        "class Priority(str, Enum):\n",
        "    HIGH = \"high\"\n",
        "    MEDIUM = \"medium\"\n",
        "    LOW = \"low\"\n",
        "\n",
        "class SupportTicket(BaseModel):\n",
        "    \"\"\"Structured representation of a customer support ticket.\"\"\"\n",
        "    \n",
        "    category: str = Field(description=\"Primary issue category (e.g., billing, technical, shipping)\")\n",
        "    priority: Priority = Field(description=\"Urgency level based on customer tone and issue severity\")\n",
        "    summary: str = Field(description=\"One-sentence summary of the issue\", max_length=200)\n",
        "    entities: List[str] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"Products, order numbers, or features mentioned\"\n",
        "    )\n",
        "    sentiment: float = Field(\n",
        "        ge=-1.0, le=1.0,\n",
        "        description=\"Sentiment from -1 (angry) to 1 (happy)\"\n",
        "    )\n",
        "\n",
        "print(\"Schema defined: SupportTicket\")\n",
        "print(f\"Fields: {list(SupportTicket.model_fields.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m[INFO]\u001b[0m Instructor client ready with model: qwen3:4b\n"
          ]
        }
      ],
      "source": [
        "# STEP 3: Initialize Instructor with Ollama\n",
        "import instructor\n",
        "from openai import OpenAI\n",
        "\n",
        "# Ollama exposes OpenAI-compatible API at localhost:11434/v1\n",
        "# This is why instructor.from_openai() works - same API shape\n",
        "ollama_client = OpenAI(\n",
        "    base_url=\"http://localhost:11434/v1\",\n",
        "    api_key=\"ollama\"  # Ollama ignores this, but OpenAI client requires it\n",
        ")\n",
        "\n",
        "# Instructor wraps the client to add structured output capabilities\n",
        "client = instructor.from_openai(\n",
        "    ollama_client,\n",
        "    mode=instructor.Mode.JSON  # JSON mode works best with Ollama\n",
        ")\n",
        "\n",
        "# Model selection - Qwen3 excels at instruction following and structured output\n",
        "MODEL = \"qwen3:4b\"  # Recommended. Alternatives: qwen3:1.7b (smaller), qwen3:8b (better)\n",
        "\n",
        "# Verify model availability\n",
        "try:\n",
        "    import subprocess\n",
        "    result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
        "    if MODEL.split(\":\")[0] in result.stdout:\n",
        "        logger.info(f\"Instructor client ready with model: {MODEL}\")\n",
        "    else:\n",
        "        logger.warning(f\"Model {MODEL} not found. Available models:\")\n",
        "        print(result.stdout)\n",
        "        logger.info(f\"Pull it with: ollama pull {MODEL}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Could not verify model: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[90m[DEBUG]\u001b[0m Extracting from: \n",
            "    I've been trying to reset my password for 3 d...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting structured tickets from raw messages...\n",
            "=================================================================\n",
            "\n",
            "[Message 1]\n",
            "Input: I've been trying to reset my password for 3 days now!\n",
            "    The mobile a...\n"
          ]
        }
      ],
      "source": [
        "# STEP 4: Extract structured data from raw customer messages\n",
        "\n",
        "# More explicit system prompt - critical for smaller models\n",
        "SYSTEM_PROMPT = \"\"\"You are a support ticket classifier. Extract information from customer messages.\n",
        "\n",
        "RESPOND WITH JSON ONLY. No explanations. Use this exact structure:\n",
        "{\n",
        "  \"category\": \"billing\" or \"technical\" or \"shipping\" or \"account\" or \"feedback\",\n",
        "  \"priority\": \"high\" or \"medium\" or \"low\",\n",
        "  \"summary\": \"one sentence describing the issue\",\n",
        "  \"entities\": [\"list\", \"of\", \"mentioned\", \"items\"],\n",
        "  \"sentiment\": -1.0 to 1.0 (negative to positive)\n",
        "}\"\"\"\n",
        "\n",
        "def extract_ticket(raw_message: str) -> SupportTicket:\n",
        "    \"\"\"Extract structured ticket from raw customer message.\"\"\"\n",
        "    logger.debug(f\"Extracting from: {raw_message[:50]}...\")\n",
        "    \n",
        "    try:\n",
        "        result = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            response_model=SupportTicket,\n",
        "            max_retries=3,  # More retries for smaller models\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": f\"Customer message:\\n{raw_message}\"}\n",
        "            ]\n",
        "        )\n",
        "        logger.info(\"Extraction successful\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Extraction failed: {type(e).__name__}\")\n",
        "        raise\n",
        "\n",
        "# Test messages\n",
        "test_messages = [\n",
        "    \"\"\"\n",
        "    I've been trying to reset my password for 3 days now!\n",
        "    The mobile app keeps crashing when I tap \"Forgot Password\".\n",
        "    This is ridiculous - I need access to my account for work.\n",
        "    Order #12345 is stuck and I can't track it.\n",
        "    \"\"\",\n",
        "    \n",
        "    \"\"\"\n",
        "    Hi, just wanted to say the new Pro subscription is amazing!\n",
        "    The export feature saved me hours. Quick question - can I add \n",
        "    more team members to my account?\n",
        "    \"\"\",\n",
        "    \n",
        "    \"\"\"\n",
        "    My invoice shows €299 but I was charged €399. Please fix this \n",
        "    immediately. Account: ACC-2024-789. This is the third billing \n",
        "    error this year.\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "print(\"Extracting structured tickets from raw messages...\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "for i, msg in enumerate(test_messages, 1):\n",
        "    print(f\"\\n[Message {i}]\")\n",
        "    print(f\"Input: {msg.strip()[:70]}...\")\n",
        "    \n",
        "    try:\n",
        "        ticket = extract_ticket(msg)\n",
        "        print(f\"\\n  ✓ Extracted SupportTicket:\")\n",
        "        print(f\"    category:  {ticket.category}\")\n",
        "        print(f\"    priority:  {ticket.priority.value}\")\n",
        "        print(f\"    summary:   {ticket.summary[:60]}...\")\n",
        "        print(f\"    entities:  {ticket.entities}\")\n",
        "        print(f\"    sentiment: {ticket.sentiment:+.2f}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed: {e}\")\n",
        "    \n",
        "    print(\"-\" * 65)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m[INFO]\u001b[0m Extracting ticket from angry customer message...\n",
            "\u001b[90m[DEBUG]\u001b[0m Extracting from: Cancel my subscription NOW. Order #99999 never arr...\n",
            "\u001b[92m[INFO]\u001b[0m Extraction successful\n",
            "\u001b[92m[INFO]\u001b[0m This is what 'structured output' means - no JSON parsing, no try/except\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=================================================================\n",
            "INSTRUCTOR GIVES YOU A PYDANTIC OBJECT, NOT A STRING\n",
            "=================================================================\n",
            "\n",
            "1. Type-safe attribute access:\n",
            "   ticket.category  → 'billing'\n",
            "   ticket.priority  → Priority.HIGH  (Enum, not string)\n",
            "   ticket.sentiment → -1.0  (float, constrained to [-1, 1])\n",
            "   ticket.entities  → ['Order #99999']  (List[str])\n",
            "\n",
            "2. Export to JSON (for APIs):\n",
            "{\n",
            "  \"category\": \"billing\",\n",
            "  \"priority\": \"high\",\n",
            "  \"summary\": \"Customer wants to cancel subscription immediately as order #99999 never arrived and is fraudulent.\",\n",
            "  \"entities\": [\n",
            "    \"Order #99999\"\n",
            "  ],\n",
            "  \"sentiment\": -1.0\n",
            "}\n",
            "\n",
            "3. Export to dict (for databases):\n",
            "{'category': 'billing', 'priority': <Priority.HIGH: 'high'>, 'summary': 'Customer wants to cancel subscription immediately as order #99999 never arrived and is fraudulent.', 'entities': ['Order #99999'], 'sentiment': -1.0}\n"
          ]
        }
      ],
      "source": [
        "# STEP 5: Instructor output demo - what you get back\n",
        "\n",
        "sample_msg = \"Cancel my subscription NOW. Order #99999 never arrived. This is fraud!\"\n",
        "\n",
        "logger.info(\"Extracting ticket from angry customer message...\")\n",
        "ticket = extract_ticket(sample_msg)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 65)\n",
        "print(\"INSTRUCTOR GIVES YOU A PYDANTIC OBJECT, NOT A STRING\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "print(\"\\n1. Type-safe attribute access:\")\n",
        "print(f\"   ticket.category  → {ticket.category!r}\")\n",
        "print(f\"   ticket.priority  → {ticket.priority}  (Enum, not string)\")\n",
        "print(f\"   ticket.sentiment → {ticket.sentiment}  (float, constrained to [-1, 1])\")\n",
        "print(f\"   ticket.entities  → {ticket.entities}  (List[str])\")\n",
        "\n",
        "print(\"\\n2. Export to JSON (for APIs):\")\n",
        "print(ticket.model_dump_json(indent=2))\n",
        "\n",
        "print(\"\\n3. Export to dict (for databases):\")\n",
        "print(ticket.model_dump())\n",
        "\n",
        "logger.info(\"This is what 'structured output' means - no JSON parsing, no try/except\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[91m[ERROR]\u001b[0m Pydantic caught invalid data:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "PYDANTIC VALIDATION BASICS\n",
            "=================================================================\n",
            "\n",
            "Attempting: SupportTicket(**bad_data)\n",
            "Where bad_data = {'category': 'test', 'priority': 'urgent', 'summary': 'x', 'sentiment': 5.0}\n",
            "\n",
            "2 validation errors for SupportTicket\n",
            "priority\n",
            "  Input should be 'high', 'medium' or 'low' [type=enum, input_value='urgent', input_type=str]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/enum\n",
            "sentiment\n",
            "  Input should be less than or equal to 1 [type=less_than_equal, input_value=5.0, input_type=float]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/less_than_equal\n"
          ]
        }
      ],
      "source": [
        "# STEP 6: Validation demo - what happens when LLM returns bad data\n",
        "\n",
        "from pydantic import ValidationError\n",
        "\n",
        "print(\"=\" * 65)\n",
        "print(\"PYDANTIC VALIDATION BASICS\")\n",
        "print(\"=\" * 65)\n",
        "\n",
        "# **bad_data is Python's dict unpacking syntax:\n",
        "# SupportTicket(**{\"a\": 1, \"b\": 2}) == SupportTicket(a=1, b=2)\n",
        "bad_data = {\"category\": \"test\", \"priority\": \"urgent\", \"summary\": \"x\", \"sentiment\": 5.0}\n",
        "\n",
        "print(f\"\\nAttempting: SupportTicket(**bad_data)\")\n",
        "print(f\"Where bad_data = {bad_data}\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    SupportTicket(**bad_data)  # ** unpacks dict as keyword arguments\n",
        "except ValidationError as e:\n",
        "    logger.error(\"Pydantic caught invalid data:\")\n",
        "    print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m[INFO]\u001b[0m Testing: 'I love your product! Best purchase ever!...'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "INSTRUCTOR RETRY MECHANISM - Error Feedback to LLM\n",
            "=================================================================\n",
            "\n",
            "Setup: Schema allows only ['billing', 'technical', 'shipping']\n",
            "       BUT we don't tell the model this constraint!\n",
            "\n",
            "When the model outputs an invalid category, Instructor:\n",
            "1. Catches the ValidationError\n",
            "2. Appends error to messages: \"category must be one of [...], got 'account'\"\n",
            "3. Asks LLM to try again\n",
            "4. LLM corrects its output\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m[INFO]\u001b[0m Testing: 'Please delete my account and all my data...'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ passed → category: technical\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m[INFO]\u001b[0m Testing: 'I want to upgrade to the premium plan...'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ passed → category: technical\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m[INFO]\u001b[0m Testing: 'My package arrived damaged...'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ passed → category: billing\n",
            "\n",
            "  ✓ passed → category: shipping\n",
            "\n",
            "=================================================================\n",
            "NOTE: If all pass on first try, that's a GOOD sign!\n",
            "\n",
            "Better models = fewer retries needed. Qwen3:4b is smart enough to \n",
            "infer valid categories from context even without explicit constraints.\n",
            "\n",
            "In production, this means:\n",
            "- Strong models (GPT-4o, Claude Sonnet, Qwen3) rarely trigger retries\n",
            "- Weaker models benefit more from Instructor's retry mechanism\n",
            "- The retry logic is your safety net, not your primary path\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# STEP 7: Demo - Instructor's retry mechanism with error feedback\n",
        "\n",
        "# Create a STRICT schema - DON'T tell the model the valid categories!\n",
        "class StrictTicket(BaseModel):\n",
        "    \"\"\"Schema with strict constraints to trigger validation failures.\"\"\"\n",
        "    category: str = Field(description=\"The issue category\")  # Vague on purpose\n",
        "    priority: Priority\n",
        "    confidence: float = Field(ge=0.0, le=1.0, description=\"Model's confidence 0-1\")\n",
        "    \n",
        "    @field_validator('category')\n",
        "    @classmethod\n",
        "    def validate_category(cls, v):\n",
        "        # Very restrictive - only 3 allowed values\n",
        "        allowed = ['billing', 'technical', 'shipping']\n",
        "        if v.lower() not in allowed:\n",
        "            raise ValueError(f\"category must be one of {allowed}, got '{v}'\")\n",
        "        return v.lower()\n",
        "\n",
        "# Track retries\n",
        "retry_count = 0\n",
        "\n",
        "def extract_with_retry_logging(message: str) -> StrictTicket:\n",
        "    \"\"\"Extract with visible retry tracking.\"\"\"\n",
        "    global retry_count\n",
        "    retry_count = 0\n",
        "    \n",
        "    import functools\n",
        "    original_create = client.chat.completions.create\n",
        "    \n",
        "    @functools.wraps(original_create)\n",
        "    def tracked_create(*args, **kwargs):\n",
        "        global retry_count\n",
        "        retry_count += 1\n",
        "        if retry_count > 1:\n",
        "            logger.warning(f\"Retry #{retry_count} - LLM correcting based on validation error\")\n",
        "        return original_create(*args, **kwargs)\n",
        "    \n",
        "    client.chat.completions.create = tracked_create\n",
        "    \n",
        "    try:\n",
        "        # KEY: Don't tell the model the valid categories - let it fail naturally\n",
        "        result = client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            response_model=StrictTicket,\n",
        "            max_retries=4,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"Extract ticket info as JSON. Classify the category.\"},\n",
        "                {\"role\": \"user\", \"content\": message}\n",
        "            ]\n",
        "        )\n",
        "        return result\n",
        "    finally:\n",
        "        client.chat.completions.create = original_create\n",
        "\n",
        "print(\"=\" * 65)\n",
        "print(\"INSTRUCTOR RETRY MECHANISM - Error Feedback to LLM\")\n",
        "print(\"=\" * 65)\n",
        "print(\"\"\"\n",
        "Setup: Schema allows only ['billing', 'technical', 'shipping']\n",
        "       BUT we don't tell the model this constraint!\n",
        "       \n",
        "When the model outputs an invalid category, Instructor:\n",
        "1. Catches the ValidationError\n",
        "2. Appends error to messages: \"category must be one of [...], got 'account'\"\n",
        "3. Asks LLM to try again\n",
        "4. LLM corrects its output\n",
        "\"\"\")\n",
        "\n",
        "# Messages that naturally map to invalid categories\n",
        "test_cases = [\n",
        "    \"I love your product! Best purchase ever!\",           # → \"feedback\" or \"general\" (invalid)\n",
        "    \"Please delete my account and all my data\",           # → \"account\" (invalid)\n",
        "    \"I want to upgrade to the premium plan\",              # → \"sales\" or \"upgrade\" (invalid)\n",
        "    \"My package arrived damaged\",                          # → \"shipping\" (valid - should pass first try)\n",
        "]\n",
        "\n",
        "for msg in test_cases:\n",
        "    logger.info(f\"Testing: '{msg[:45]}...'\")\n",
        "    \n",
        "    try:\n",
        "        ticket = extract_with_retry_logging(msg)\n",
        "        status = \"✓ passed\" if retry_count == 1 else f\"✓ corrected after {retry_count} attempts\"\n",
        "        print(f\"  {status} → category: {ticket.category}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed after max retries: {type(e).__name__}\")\n",
        "    print()\n",
        "\n",
        "# Note on results\n",
        "print(\"=\" * 65)\n",
        "print(\"NOTE: If all pass on first try, that's a GOOD sign!\")\n",
        "print(\"\"\"\n",
        "Better models = fewer retries needed. Qwen3:4b is smart enough to \n",
        "infer valid categories from context even without explicit constraints.\n",
        "\n",
        "In production, this means:\n",
        "- Strong models (GPT-4o, Claude Sonnet, Qwen3) rarely trigger retries\n",
        "- Weaker models benefit more from Instructor's retry mechanism\n",
        "- The retry logic is your safety net, not your primary path\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instructor Takeaways\n",
        "\n",
        "**Instructor provides:**\n",
        "- Pydantic schema → automatic prompt injection\n",
        "- Response parsing with type coercion\n",
        "- Validation with auto-retry on failure\n",
        "- Works with any OpenAI-compatible API (OpenAI, Anthropic, Ollama, etc.)\n",
        "\n",
        "**Cloud API setup** (when not using Ollama):\n",
        "```python\n",
        "from openai import OpenAI\n",
        "import instructor\n",
        "\n",
        "client = instructor.from_openai(OpenAI())  # Uses OPENAI_API_KEY env var\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo 2: Structured Generation with Outlines\n",
        "\n",
        "Outlines is a structured generation library that works differently depending on the backend:\n",
        "- **API backends (Ollama, OpenAI, vLLM server):** Uses provider's JSON mode\n",
        "- **Local backends (HuggingFace, vLLM offline):** True token masking during generation\n",
        "\n",
        "**Key insight:** Token masking (regex, grammar constraints) only works with LOCAL models!\n",
        "\n",
        "**When to use Outlines:**\n",
        "- Self-hosting models + need regex/grammar constraints\n",
        "- High-volume GPU inference with vLLM offline\n",
        "- Want unified API across local and cloud models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m[INFO]\u001b[0m Connecting to Ollama model: qwen3:4b\n",
            "\u001b[92m[INFO]\u001b[0m Ollama model connected!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "OUTLINES: Structured Generation Library\n",
            "=================================================================\n",
            "\n",
            "Outlines supports many backends with different capabilities:\n",
            "  • API backends (Ollama, OpenAI, vLLM server): JSON schema via native API\n",
            "  • Local backends (HuggingFace, vLLM offline): True token masking\n",
            "\n",
            "With APIs, Outlines uses the provider's JSON mode - reliable but no regex.\n",
            "With local models, Outlines masks tokens during sampling - full control.\n",
            "\n",
            "We'll demo with Ollama (JSON mode) since we already have it running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# STEP 8: Setup Outlines with Ollama (same model we used for Instructor!)\n",
        "# pip install outlines[ollama]\n",
        "\n",
        "print(\"=\" * 65)\n",
        "print(\"OUTLINES: Structured Generation Library\")\n",
        "print(\"=\" * 65)\n",
        "print(\"\"\"\n",
        "Outlines supports many backends with different capabilities:\n",
        "  • API backends (Ollama, OpenAI, vLLM server): JSON schema via native API\n",
        "  • Local backends (HuggingFace, vLLM offline): True token masking\n",
        "\n",
        "With APIs, Outlines uses the provider's JSON mode - reliable but no regex.\n",
        "With local models, Outlines masks tokens during sampling - full control.\n",
        "\n",
        "We'll demo with Ollama (JSON mode) since we already have it running.\n",
        "\"\"\")\n",
        "\n",
        "import outlines\n",
        "import ollama\n",
        "\n",
        "# Outlines 1.2.x: Use Ollama (same model as Instructor demo!)\n",
        "MODEL_NAME = \"qwen3:4b\"\n",
        "\n",
        "logger.info(f\"Connecting to Ollama model: {MODEL_NAME}\")\n",
        "\n",
        "model = None\n",
        "try:\n",
        "    # Create Ollama client, then wrap with Outlines\n",
        "    ollama_client = ollama.Client()\n",
        "    model = outlines.from_ollama(ollama_client, model_name=MODEL_NAME)\n",
        "    logger.info(\"Ollama model connected!\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to connect: {type(e).__name__}: {e}\")\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"  1. pip install 'outlines[ollama]'\")\n",
        "    print(\"  2. Ensure Ollama is running: ollama serve\")\n",
        "    print(f\"  3. Pull model if needed: ollama pull {MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m[INFO]\u001b[0m Prompt: Classify this support ticket: My payment was decli...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "CONSTRAINED GENERATION RESULTS\n",
            "=================================================================\n",
            "Schema enforces: category ∈ {billing, technical, shipping}\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m[INFO]\u001b[0m Prompt: Classify this support ticket: Package never arrive...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output: {\n",
            "  \"category\": \"shipping\",\n",
            "  \"priority\": 2,\n",
            "  \"summary\": \"Payment declined twice and locked out - urgent account access issue\"\n",
            "}\n",
            "\n",
            "\n",
            "  Type: str\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m[INFO]\u001b[0m Prompt: Classify this support ticket: App crashes when I t...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Output: {\n",
            "  \"category\": \"shipping\",\n",
            "  \"priority\": 2,\n",
            "  \"summary\": \"Package never arrived despite tracking showing delivery status as 'delivered' - tracking discrepancy detected\"\n",
            "}\n",
            "\n",
            "\n",
            "  Type: str\n",
            "\n",
            "  Output: {\n",
            "  \"category\": \"technical\",\n",
            "  \"priority\": 2,\n",
            "  \"summary\": \"App crashes when attempting to export reports (feature-specific crash during report export operation)\"\n",
            "}\n",
            "\n",
            "  Type: str\n",
            "\n",
            "=================================================================\n",
            "NOTE: With Ollama, Outlines uses Ollama's JSON mode (not token masking).\n",
            "Output is JSON string, not Pydantic object. Still reliable, but different mechanism.\n",
            "For true grammar-level constraints, use HuggingFace transformers backend.\n"
          ]
        }
      ],
      "source": [
        "# STEP 9: Constrained generation with Pydantic schema\n",
        "\n",
        "if model is not None:\n",
        "    from pydantic import BaseModel as PydanticBase\n",
        "    from typing import Literal\n",
        "    \n",
        "    # Define output schema as Pydantic model\n",
        "    class TicketClassification(PydanticBase):\n",
        "        category: Literal[\"billing\", \"technical\", \"shipping\"]\n",
        "        priority: int  # 1-5\n",
        "        summary: str\n",
        "    \n",
        "    print(\"=\" * 65)\n",
        "    print(\"CONSTRAINED GENERATION RESULTS\")\n",
        "    print(\"=\" * 65)\n",
        "    print(\"Schema enforces: category ∈ {billing, technical, shipping}\")\n",
        "    print()\n",
        "    \n",
        "    # Test messages\n",
        "    test_prompts = [\n",
        "        \"Classify this support ticket: My payment was declined twice and I'm locked out\",\n",
        "        \"Classify this support ticket: Package never arrived, tracking shows delivered\",\n",
        "        \"Classify this support ticket: App crashes when I try to export reports\",\n",
        "    ]\n",
        "    \n",
        "    for prompt in test_prompts:\n",
        "        logger.info(f\"Prompt: {prompt[:50]}...\")\n",
        "        \n",
        "        try:\n",
        "            # Outlines 1.2.x simple API: model(prompt, output_type)\n",
        "            result = model(prompt, TicketClassification)\n",
        "            print(f\"  Output: {result}\")\n",
        "            print(f\"  Type: {type(result).__name__}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Generation failed: {type(e).__name__}: {e}\")\n",
        "        print()\n",
        "    \n",
        "    print(\"=\" * 65)\n",
        "    print(\"NOTE: With Ollama, Outlines uses Ollama's JSON mode (not token masking).\")\n",
        "    print(\"Output is JSON string, not Pydantic object. Still reliable, but different mechanism.\")\n",
        "    print(\"For true grammar-level constraints, use HuggingFace transformers backend.\")\n",
        "else:\n",
        "    logger.warning(\"Skipping demo - model not loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "OUTLINES BACKEND CAPABILITIES\n",
            "=================================================================\n",
            "\n",
            "Regex/grammar constraints require token-level control during generation.\n",
            "This depends on HOW you connect to the model:\n",
            "\n",
            "┌────────────────────────────┬──────────────┬─────────────────┐\n",
            "│ Backend                    │ JSON Schemas │ Regex/Grammar   │\n",
            "├────────────────────────────┼──────────────┼─────────────────┤\n",
            "│ Ollama (from_ollama)       │ ✓            │ ✗ (black-box)   │\n",
            "│ OpenAI (from_openai)       │ ✓            │ ✗ (black-box)   │\n",
            "│ vLLM server (from_vllm)    │ ✓            │ ✗ (API mode)    │\n",
            "│ vLLM local (from_vllm_offline) │ ✓        │ ✓ Full support  │\n",
            "│ HuggingFace (from_transformers)│ ✓        │ ✓ Full support  │\n",
            "│ llama.cpp (from_llamacpp)  │ ✓            │ ✓ Full support  │\n",
            "└────────────────────────────┴──────────────┴─────────────────┘\n",
            "\n",
            "For production with full grammar control:\n",
            "  # vLLM (offline mode - fastest for GPUs)\n",
            "  from vllm import LLM\n",
            "  model = outlines.from_vllm_offline(LLM(\"meta-llama/Llama-3-8B\"))\n",
            "\n",
            "  # HuggingFace (simpler setup)\n",
            "  model = outlines.from_transformers(hf_model, tokenizer)\n",
            "\n",
            "  regex_type = outlines.types.regex(r\"PRD-[0-9]{3}\")\n",
            "  result = model(\"Generate code:\", regex_type)  # Guaranteed PRD-XXX\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# STEP 10: Regex constraints - which backends support them?\n",
        "\n",
        "print(\"=\" * 65)\n",
        "print(\"OUTLINES BACKEND CAPABILITIES\")\n",
        "print(\"=\" * 65)\n",
        "print(\"\"\"\n",
        "Regex/grammar constraints require token-level control during generation.\n",
        "This depends on HOW you connect to the model:\n",
        "\n",
        "┌────────────────────────────┬──────────────┬─────────────────┐\n",
        "│ Backend                    │ JSON Schemas │ Regex/Grammar   │\n",
        "├────────────────────────────┼──────────────┼─────────────────┤\n",
        "│ Ollama (from_ollama)       │ ✓            │ ✗ (black-box)   │\n",
        "│ OpenAI (from_openai)       │ ✓            │ ✗ (black-box)   │\n",
        "│ vLLM server (from_vllm)    │ ✓            │ ✗ (API mode)    │\n",
        "│ vLLM local (from_vllm_offline) │ ✓        │ ✓ Full support  │\n",
        "│ HuggingFace (from_transformers)│ ✓        │ ✓ Full support  │\n",
        "│ llama.cpp (from_llamacpp)  │ ✓            │ ✓ Full support  │\n",
        "└────────────────────────────┴──────────────┴─────────────────┘\n",
        "\n",
        "For production with full grammar control:\n",
        "  # vLLM (offline mode - fastest for GPUs)\n",
        "  from vllm import LLM\n",
        "  model = outlines.from_vllm_offline(LLM(\"meta-llama/Llama-3-8B\"))\n",
        "  \n",
        "  # HuggingFace (simpler setup)\n",
        "  model = outlines.from_transformers(hf_model, tokenizer)\n",
        "  \n",
        "  regex_type = outlines.types.regex(r\"PRD-[0-9]{3}\")\n",
        "  result = model(\"Generate code:\", regex_type)  # Guaranteed PRD-XXX\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "INSTRUCTOR vs OUTLINES: When to use which\n",
            "=================================================================\n",
            "\n",
            "┌──────────────────┬─────────────────────┬─────────────────────────────┐\n",
            "│                  │ INSTRUCTOR          │ OUTLINES                    │\n",
            "├──────────────────┼─────────────────────┼─────────────────────────────┤\n",
            "│ Mechanism        │ Validate + retry    │ API: JSON mode              │\n",
            "│                  │                     │ Local: Token masking        │\n",
            "│ APIs (Ollama,    │ ✓ Full support      │ ✓ JSON schemas only         │\n",
            "│   OpenAI, vLLM)  │                     │                             │\n",
            "│ Local models     │ ✓ Via API wrapper   │ ✓ Full regex/grammar        │\n",
            "│ (HF, vLLM offline)│                    │                             │\n",
            "│ Returns          │ Pydantic object     │ API: str, Local: object     │\n",
            "│ Retry on fail    │ ✓ With error context│ N/A (constraints prevent)   │\n",
            "│ Custom validators│ ✓ Full Pydantic     │ Limited                     │\n",
            "└──────────────────┴─────────────────────┴─────────────────────────────┘\n",
            "\n",
            "DECISION GUIDE:\n",
            "  • Using APIs (Ollama, OpenAI, vLLM server)? → Instructor (simpler DX)\n",
            "  • Need Pydantic validators, retry with error feedback? → Instructor\n",
            "  • Self-hosting + need regex/grammar constraints? → Outlines (local mode)\n",
            "  • High-volume GPU inference? → Outlines + vLLM offline (fastest)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# STEP 11: Comparison summary\n",
        "\n",
        "print(\"=\" * 65)\n",
        "print(\"INSTRUCTOR vs OUTLINES: When to use which\")\n",
        "print(\"=\" * 65)\n",
        "print(\"\"\"\n",
        "┌──────────────────┬─────────────────────┬─────────────────────────────┐\n",
        "│                  │ INSTRUCTOR          │ OUTLINES                    │\n",
        "├──────────────────┼─────────────────────┼─────────────────────────────┤\n",
        "│ Mechanism        │ Validate + retry    │ API: JSON mode              │\n",
        "│                  │                     │ Local: Token masking        │\n",
        "│ APIs (Ollama,    │ ✓ Full support      │ ✓ JSON schemas only         │\n",
        "│   OpenAI, vLLM)  │                     │                             │\n",
        "│ Local models     │ ✓ Via API wrapper   │ ✓ Full regex/grammar        │\n",
        "│ (HF, vLLM offline)│                    │                             │\n",
        "│ Returns          │ Pydantic object     │ API: str, Local: object     │\n",
        "│ Retry on fail    │ ✓ With error context│ N/A (constraints prevent)   │\n",
        "│ Custom validators│ ✓ Full Pydantic     │ Limited                     │\n",
        "└──────────────────┴─────────────────────┴─────────────────────────────┘\n",
        "\n",
        "DECISION GUIDE:\n",
        "  • Using APIs (Ollama, OpenAI, vLLM server)? → Instructor (simpler DX)\n",
        "  • Need Pydantic validators, retry with error feedback? → Instructor\n",
        "  • Self-hosting + need regex/grammar constraints? → Outlines (local mode)\n",
        "  • High-volume GPU inference? → Outlines + vLLM offline (fastest)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demos 3-4: Guardrails (Separate Notebook)\n",
        "\n",
        "Guardrails demos have been moved to **`guardrails_demo.ipynb`** for:\n",
        "\n",
        "- **Dependency isolation:** `guardrails-ai` requires `openai<2.0.0` (conflicts with Instructor)\n",
        "- **Comprehensive coverage:** NeMo, Guardrails AI, and Haystack demos\n",
        "- **Separate environment:** Can run in isolated env if needed\n",
        "\n",
        "**See `1B/guardrails_demo.ipynb` for:**\n",
        "1. NeMo Guardrails - Dialog flow with Colang\n",
        "2. Guardrails AI - Field-level validation with Hub\n",
        "3. Haystack - Pipeline-native components\n",
        "4. Complete comparison and decision guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "guardrails-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
