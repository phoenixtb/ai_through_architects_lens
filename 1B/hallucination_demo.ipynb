{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hallucination Detection Demo\n",
        "\n",
        "Functional demos for hallucination detection in LLM outputs:\n",
        "\n",
        "1. **LettuceDetect** - Token-level detection with ModernBERT\n",
        "2. **NLI-Based Detection** - Natural Language Inference for contradiction detection\n",
        "3. **Self-Consistency** - Multi-sample agreement check\n",
        "4. **HaluGate-Style Pipeline** - Sentinel → Detector → Explainer\n",
        "5. **Grounded Prompting** - Mitigation via prompt engineering\n",
        "\n",
        "## Hallucination Types\n",
        "\n",
        "- **Intrinsic**: Contradicts provided context\n",
        "- **Extrinsic**: Fabricates information not in context\n",
        "\n",
        "## Setup\n",
        "\n",
        "```bash\n",
        "pip install lettucedetect transformers torch\n",
        "```\n",
        "\n",
        "**Ollama:** `ollama pull qwen3:4b`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[92m[INFO]\u001b[0m Ollama is running\n",
            "\u001b[92m[INFO]\u001b[0m Using model: qwen3:4b\n",
            "\u001b[92m[INFO]\u001b[0m Helper functions ready\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Available models:\n",
            "NAME           ID              SIZE      MODIFIED     \n",
            "qwen3:4b       359d7dd4bcda    2.5 GB    44 hours ago    \n",
            "llama3.2:1b    baf6a787fdff    1.3 GB    44 hours ago    \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Setup: Logging, environment checks, and Ollama client\n",
        "import subprocess\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "import re\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "# Color-coded logging\n",
        "class ColoredFormatter(logging.Formatter):\n",
        "    COLORS = {'DEBUG': '\\033[90m', 'INFO': '\\033[92m', 'WARNING': '\\033[93m', 'ERROR': '\\033[91m', 'RESET': '\\033[0m'}\n",
        "    def format(self, record):\n",
        "        color = self.COLORS.get(record.levelname, self.COLORS['RESET'])\n",
        "        record.msg = f\"{color}[{record.levelname}]{self.COLORS['RESET']} {record.msg}\"\n",
        "        return super().format(record)\n",
        "\n",
        "logger = logging.getLogger(\"hallucination_demo\")\n",
        "logger.setLevel(logging.DEBUG)\n",
        "if not logger.handlers:\n",
        "    handler = logging.StreamHandler()\n",
        "    handler.setFormatter(ColoredFormatter('%(message)s'))\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "# Check Ollama\n",
        "def check_ollama():\n",
        "    try:\n",
        "        result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True, timeout=5)\n",
        "        if result.returncode == 0:\n",
        "            logger.info(\"Ollama is running\")\n",
        "            print(f\"\\nAvailable models:\\n{result.stdout}\")\n",
        "            return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ollama check failed: {e}\")\n",
        "    return False\n",
        "\n",
        "ollama_ready = check_ollama()\n",
        "MODEL = \"qwen3:4b\"\n",
        "OLLAMA_URL = \"http://localhost:11434\"\n",
        "\n",
        "if ollama_ready:\n",
        "    logger.info(f\"Using model: {MODEL}\")\n",
        "\n",
        "# Ollama generate function\n",
        "def ollama_generate(prompt: str, model: str = MODEL, temperature: float = 0.7) -> str:\n",
        "    \"\"\"Generate response from Ollama.\"\"\"\n",
        "    response = requests.post(\n",
        "        f\"{OLLAMA_URL}/api/generate\",\n",
        "        json={\n",
        "            \"model\": model,\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False,\n",
        "            \"options\": {\"temperature\": temperature}\n",
        "        },\n",
        "        timeout=120\n",
        "    )\n",
        "    return response.json().get(\"response\", \"\")\n",
        "\n",
        "def ollama_chat(messages: List[Dict], model: str = MODEL, temperature: float = 0.7) -> str:\n",
        "    \"\"\"Chat completion from Ollama.\"\"\"\n",
        "    response = requests.post(\n",
        "        f\"{OLLAMA_URL}/api/chat\",\n",
        "        json={\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"stream\": False,\n",
        "            \"options\": {\"temperature\": temperature}\n",
        "        },\n",
        "        timeout=120\n",
        "    )\n",
        "    return response.json().get(\"message\", {}).get(\"content\", \"\")\n",
        "\n",
        "def clean_response(text: str) -> str:\n",
        "    \"\"\"Remove thinking tags from qwen3 responses.\"\"\"\n",
        "    return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
        "\n",
        "logger.info(\"Helper functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Demo 1: LettuceDetect (Token-Level Detection)\n",
        "\n",
        "**LettuceDetect** uses ModernBERT for token-level hallucination detection in RAG pipelines.\n",
        "\n",
        "**Key features:**\n",
        "- Fast: Runs at inference time (~50-100ms)\n",
        "- Token-level: Flags specific unsupported tokens\n",
        "- ModernBERT-based: Supports up to 8,192 tokens\n",
        "- Local execution: No external API calls\n",
        "\n",
        "**Install:** `pip install lettucedetect`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "LETTUCEDETECT: Token-Level Hallucination Detection\n",
            "=================================================================\n",
            "\n",
            "Loading LettuceDetect model...\n",
            "(First run downloads ~500MB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[91m[ERROR]\u001b[0m LettuceDetect initialization failed: KRLabsOrg/lettuce-detect-base-modernbert-en-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✗ Error: KRLabsOrg/lettuce-detect-base-modernbert-en-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n",
            "\n",
            "Note: First run downloads the model (~500MB)\n"
          ]
        }
      ],
      "source": [
        "# LettuceDetect: Setup and initialization\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"LETTUCEDETECT: Token-Level Hallucination Detection\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "lettucedetect_available = False\n",
        "detector = None\n",
        "\n",
        "try:\n",
        "    from lettucedetect.models.inference import HallucinationDetector\n",
        "    \n",
        "    # Initialize the detector with ModernBERT model\n",
        "    # Model: KRLabsOrg/lettuce-detect-base-modernbert-en-v1\n",
        "    print(\"\\nLoading LettuceDetect model...\")\n",
        "    print(\"(First run downloads ~500MB)\")\n",
        "    \n",
        "    detector = HallucinationDetector(\n",
        "        model_path=\"KRLabsOrg/lettuce-detect-base-modernbert-en-v1\",\n",
        "        model_type=\"token\"\n",
        "    )\n",
        "    \n",
        "    lettucedetect_available = True\n",
        "    logger.info(\"LettuceDetect loaded successfully\")\n",
        "    print(\"\\n✓ Model: KRLabsOrg/lettuce-detect-base-modernbert-en-v1\")\n",
        "    print(\"✓ Type: Token-level classification\")\n",
        "    print(\"✓ Context window: 8,192 tokens (ModernBERT)\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    logger.warning(f\"LettuceDetect not installed: {e}\")\n",
        "    print(\"\\n✗ LettuceDetect not available\")\n",
        "    print(\"\\nTo install:\")\n",
        "    print(\"  pip install lettucedetect\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"LettuceDetect initialization failed: {e}\")\n",
        "    print(f\"\\n✗ Error: {e}\")\n",
        "    print(\"\\nNote: First run downloads the model (~500MB)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "LETTUCEDETECT: RAG Hallucination Detection Demo\n",
            "=================================================================\n",
            "\n",
            "⚠ LettuceDetect not available - showing conceptual demo\n",
            "\n",
            "CONCEPTUAL USAGE:\n",
            "\n",
            "    from lettucedetect.models.inference import HallucinationDetector\n",
            "\n",
            "    detector = HallucinationDetector(\n",
            "        model_path=\"KRLabsOrg/lettuce-detect-base-modernbert-en-v1\",\n",
            "        model_type=\"token\"\n",
            "    )\n",
            "\n",
            "    # Detect hallucinations - returns list of span predictions\n",
            "    predictions = detector.predict(\n",
            "        context=[\"Paris is the capital of France.\"],\n",
            "        question=\"What is the capital of France?\",\n",
            "        answer=\"The capital of France is Lyon.\"\n",
            "    )\n",
            "\n",
            "    # Each prediction has: start, end, label, confidence\n",
            "    for pred in predictions:\n",
            "        if pred.label == \"hallucinated\":\n",
            "            print(f\"Hallucinated span: {answer[pred.start:pred.end]}\")\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# LettuceDetect: Demo with RAG-style hallucination detection\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"LETTUCEDETECT: RAG Hallucination Detection Demo\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "# Test cases: context + question + answer with potential hallucinations\n",
        "test_cases = [\n",
        "    {\n",
        "        \"name\": \"Faithful Response (No Hallucination)\",\n",
        "        \"context\": [\"The Eiffel Tower is located in Paris, France. It was completed in 1889 and stands 330 meters tall.\"],\n",
        "        \"question\": \"Where is the Eiffel Tower located?\",\n",
        "        \"answer\": \"The Eiffel Tower is located in Paris, France.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Intrinsic Hallucination (Contradicts Context)\",\n",
        "        \"context\": [\"The Eiffel Tower is located in Paris, France. It was completed in 1889.\"],\n",
        "        \"question\": \"Where is the Eiffel Tower located?\",\n",
        "        \"answer\": \"The Eiffel Tower is located in Berlin, Germany.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Extrinsic Hallucination (Fabricated Info)\",\n",
        "        \"context\": [\"The Eiffel Tower is located in Paris, France.\"],\n",
        "        \"question\": \"Tell me about the Eiffel Tower.\",\n",
        "        \"answer\": \"The Eiffel Tower is in Paris. It was designed by Alexander Graham Bell in 1920.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Partial Hallucination\",\n",
        "        \"context\": [\"Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976.\"],\n",
        "        \"question\": \"Who founded Apple?\",\n",
        "        \"answer\": \"Apple was founded by Steve Jobs and Bill Gates in 1976.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "if not lettucedetect_available:\n",
        "    print(\"\\n⚠ LettuceDetect not available - showing conceptual demo\")\n",
        "    print(\"\"\"\n",
        "CONCEPTUAL USAGE:\n",
        "\n",
        "    from lettucedetect.models.inference import HallucinationDetector\n",
        "    \n",
        "    detector = HallucinationDetector(\n",
        "        model_path=\"KRLabsOrg/lettuce-detect-base-modernbert-en-v1\",\n",
        "        model_type=\"token\"\n",
        "    )\n",
        "    \n",
        "    # Detect hallucinations - returns list of span predictions\n",
        "    predictions = detector.predict(\n",
        "        context=[\"Paris is the capital of France.\"],\n",
        "        question=\"What is the capital of France?\",\n",
        "        answer=\"The capital of France is Lyon.\"\n",
        "    )\n",
        "    \n",
        "    # Each prediction has: start, end, label, confidence\n",
        "    for pred in predictions:\n",
        "        if pred.label == \"hallucinated\":\n",
        "            print(f\"Hallucinated span: {answer[pred.start:pred.end]}\")\n",
        "    \"\"\")\n",
        "else:\n",
        "    for case in test_cases:\n",
        "        print(f\"\\n{'─'*65}\")\n",
        "        print(f\"[{case['name']}]\")\n",
        "        print(f\"{'─'*65}\")\n",
        "        ctx_display = case['context'][0][:80] + \"...\" if len(case['context'][0]) > 80 else case['context'][0]\n",
        "        print(f\"Context: {ctx_display}\")\n",
        "        print(f\"Question: {case['question']}\")\n",
        "        print(f\"Answer: {case['answer']}\")\n",
        "        \n",
        "        try:\n",
        "            # Get token-level predictions\n",
        "            predictions = detector.predict(\n",
        "                context=case['context'],\n",
        "                question=case['question'],\n",
        "                answer=case['answer']\n",
        "            )\n",
        "            \n",
        "            # Analyze predictions - predictions is a list of spans with labels\n",
        "            hallucinated_spans = []\n",
        "            for pred in predictions:\n",
        "                if hasattr(pred, 'label') and pred.label == 'hallucinated':\n",
        "                    # Extract the hallucinated text from the answer\n",
        "                    span_text = case['answer'][pred.start:pred.end] if hasattr(pred, 'start') else str(pred)\n",
        "                    hallucinated_spans.append(span_text)\n",
        "            \n",
        "            if hallucinated_spans:\n",
        "                print(f\"\\n⚠ HALLUCINATION DETECTED\")\n",
        "                print(f\"  Flagged spans: {hallucinated_spans}\")\n",
        "            else:\n",
        "                print(f\"\\n✓ No hallucination detected\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Detection failed: {e}\")\n",
        "            print(f\"\\n✗ Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "LETTUCEDETECT: Live Demo with Ollama\n",
            "=================================================================\n",
            "\n",
            "⚠ LettuceDetect not available - skipping live demo\n"
          ]
        }
      ],
      "source": [
        "# LettuceDetect: Live demo with LLM-generated responses\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"LETTUCEDETECT: Live Demo with Ollama\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "if not ollama_ready:\n",
        "    print(\"\\n⚠ Ollama not running - skipping live demo\")\n",
        "elif not lettucedetect_available:\n",
        "    print(\"\\n⚠ LettuceDetect not available - skipping live demo\")\n",
        "else:\n",
        "    # RAG-style context and questions\n",
        "    rag_scenarios = [\n",
        "        {\n",
        "            \"name\": \"Technical Documentation\",\n",
        "            \"context\": \"\"\"TechCorp Cloud offers three pricing tiers: \n",
        "            - Basic: $10/month, 100GB storage, 5 users\n",
        "            - Pro: $50/month, 1TB storage, 25 users  \n",
        "            - Enterprise: Custom pricing, unlimited storage, unlimited users\n",
        "            All plans include 24/7 email support. Phone support is only available for Enterprise.\"\"\",\n",
        "            \"question\": \"What's included in the Pro plan?\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Historical Facts\",\n",
        "            \"context\": \"\"\"The Apollo 11 mission landed on the Moon on July 20, 1969. \n",
        "            Neil Armstrong was the first human to walk on the Moon, followed by Buzz Aldrin. \n",
        "            Michael Collins remained in lunar orbit aboard the command module.\"\"\",\n",
        "            \"question\": \"Who walked on the Moon during Apollo 11?\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    for scenario in rag_scenarios:\n",
        "        print(f\"\\n{'─'*65}\")\n",
        "        print(f\"[{scenario['name']}]\")\n",
        "        print(f\"{'─'*65}\")\n",
        "        print(f\"Context: {scenario['context'][:100]}...\")\n",
        "        print(f\"Question: {scenario['question']}\")\n",
        "        \n",
        "        # Generate response with Ollama\n",
        "        prompt = f\"\"\"Based ONLY on the following context, answer the question.\n",
        "        \n",
        "Context: {scenario['context']}\n",
        "\n",
        "Question: {scenario['question']}\n",
        "\n",
        "Answer concisely based only on the context:\"\"\"\n",
        "        \n",
        "        print(\"\\nGenerating response...\")\n",
        "        response = ollama_generate(prompt, temperature=0.3)\n",
        "        response = clean_response(response)\n",
        "        \n",
        "        resp_display = response[:200] + \"...\" if len(response) > 200 else response\n",
        "        print(f\"Response: {resp_display}\")\n",
        "        \n",
        "        # Detect hallucinations\n",
        "        print(\"\\nRunning hallucination detection...\")\n",
        "        try:\n",
        "            predictions = detector.predict(\n",
        "                context=[scenario['context']],\n",
        "                question=scenario['question'],\n",
        "                answer=response\n",
        "            )\n",
        "            \n",
        "            # Check for hallucinations in predictions\n",
        "            has_hallucination = False\n",
        "            flagged = []\n",
        "            for pred in predictions:\n",
        "                if hasattr(pred, 'label') and pred.label == 'hallucinated':\n",
        "                    has_hallucination = True\n",
        "                    span_text = response[pred.start:pred.end] if hasattr(pred, 'start') else str(pred)\n",
        "                    flagged.append(span_text)\n",
        "            \n",
        "            if has_hallucination:\n",
        "                print(f\"⚠ HALLUCINATION DETECTED: {flagged}\")\n",
        "            else:\n",
        "                print(\"✓ Response appears grounded in context\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Detection error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Demo 2: NLI-Based Hallucination Detection\n",
        "\n",
        "Natural Language Inference (NLI) models classify text pairs as:\n",
        "- **Entailment**: Response follows from context\n",
        "- **Contradiction**: Response contradicts context (hallucination!)\n",
        "- **Neutral**: Neither follows nor contradicts\n",
        "\n",
        "**Pros:** Simple, well-understood, catches intrinsic hallucinations\n",
        "**Cons:** Sentence-level (not token-level), may miss subtle issues\n",
        "\n",
        "**Models:** `cross-encoder/nli-deberta-v3-base`, `facebook/bart-large-mnli`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "NLI-BASED HALLUCINATION DETECTION: Setup\n",
            "=================================================================\n",
            "\n",
            "Loading NLI model: cross-encoder/nli-deberta-v3-base...\n",
            "(First run downloads model ~300MB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps\n",
            "\u001b[92m[INFO]\u001b[0m NLI model loaded\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Model loaded: cross-encoder/nli-deberta-v3-base\n",
            "✓ Device: MPS\n"
          ]
        }
      ],
      "source": [
        "# NLI-Based Detection: Setup\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"NLI-BASED HALLUCINATION DETECTION: Setup\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "nli_available = False\n",
        "nli_pipeline = None\n",
        "\n",
        "try:\n",
        "    from transformers import pipeline\n",
        "    import torch\n",
        "    \n",
        "    # Use a smaller, faster NLI model for demo\n",
        "    # Options: cross-encoder/nli-deberta-v3-base, facebook/bart-large-mnli\n",
        "    NLI_MODEL = \"cross-encoder/nli-deberta-v3-base\"\n",
        "    \n",
        "    print(f\"\\nLoading NLI model: {NLI_MODEL}...\")\n",
        "    print(\"(First run downloads model ~300MB)\")\n",
        "    \n",
        "    # Determine device\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "    elif torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    \n",
        "    nli_pipeline = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=NLI_MODEL,\n",
        "        device=device\n",
        "    )\n",
        "    \n",
        "    nli_available = True\n",
        "    logger.info(\"NLI model loaded\")\n",
        "    print(f\"\\n✓ Model loaded: {NLI_MODEL}\")\n",
        "    print(f\"✓ Device: {device.upper()}\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    logger.warning(f\"Transformers not installed: {e}\")\n",
        "    print(\"\\n✗ Transformers not available\")\n",
        "    print(\"\\nTo install:\")\n",
        "    print(\"  pip install transformers torch\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"NLI setup failed: {e}\")\n",
        "    print(f\"\\n✗ Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "NLI-BASED DETECTION: Contradiction Analysis\n",
            "=================================================================\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "[Faithful Response]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "Context: The capital of France is Paris. Paris has a population of about 2 million people.\n",
            "Response: Paris is the capital of France with approximately 2 million residents.\n",
            "\n",
            "Claim Analysis:\n",
            "  ✓ [entailment  ] (1.00) \"Paris is the capital of France with approximately ...\"\n",
            "\n",
            "✓ No contradictions detected\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "[Contradiction (Intrinsic Hallucination)]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "Context: The capital of France is Paris. Paris has a population of about 2 million people.\n",
            "Response: London is the capital of France. It has a population of 10 million.\n",
            "\n",
            "Claim Analysis:\n",
            "  ⚠ [contradiction] (1.00) \"London is the capital of France\"\n",
            "  ⚠ [contradiction] (1.00) \"It has a population of 10 million\"\n",
            "\n",
            "⚠ HALLUCINATION DETECTED: 2 contradicting claim(s)\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "[Mixed: Correct + Fabricated]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "Context: Python was created by Guido van Rossum and first released in 1991.\n",
            "Response: Python was created by Guido van Rossum. It was developed at MIT in 1985.\n",
            "\n",
            "Claim Analysis:\n",
            "  ✓ [entailment  ] (1.00) \"Python was created by Guido van Rossum\"\n",
            "  ⚠ [contradiction] (0.79) \"It was developed at MIT in 1985\"\n",
            "\n",
            "⚠ HALLUCINATION DETECTED: 1 contradicting claim(s)\n"
          ]
        }
      ],
      "source": [
        "# NLI-Based Detection: Demo\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"NLI-BASED DETECTION: Contradiction Analysis\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "def extract_claims(text: str) -> List[str]:\n",
        "    \"\"\"Split text into claim sentences.\"\"\"\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    return [s.strip() for s in sentences if len(s.strip()) > 10]\n",
        "\n",
        "def nli_detect_hallucination(context: str, response: str, threshold: float = 0.7) -> Dict[str, Any]:\n",
        "    \"\"\"Detect hallucinations using NLI contradiction detection.\"\"\"\n",
        "    if not nli_available:\n",
        "        return {\"error\": \"NLI not available\"}\n",
        "    \n",
        "    claims = extract_claims(response)\n",
        "    results = []\n",
        "    \n",
        "    for claim in claims:\n",
        "        # NLI input format for cross-encoder models\n",
        "        prediction = nli_pipeline(f\"{context}</s></s>{claim}\")[0]\n",
        "        \n",
        "        results.append({\n",
        "            \"claim\": claim,\n",
        "            \"label\": prediction[\"label\"],\n",
        "            \"score\": prediction[\"score\"]\n",
        "        })\n",
        "    \n",
        "    # Find contradictions\n",
        "    contradictions = [r for r in results if r[\"label\"] == \"contradiction\" and r[\"score\"] >= threshold]\n",
        "    \n",
        "    return {\n",
        "        \"has_hallucination\": len(contradictions) > 0,\n",
        "        \"contradictions\": contradictions,\n",
        "        \"all_claims\": results\n",
        "    }\n",
        "\n",
        "# Test cases\n",
        "nli_test_cases = [\n",
        "    {\n",
        "        \"name\": \"Faithful Response\",\n",
        "        \"context\": \"The capital of France is Paris. Paris has a population of about 2 million people.\",\n",
        "        \"response\": \"Paris is the capital of France with approximately 2 million residents.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Contradiction (Intrinsic Hallucination)\",\n",
        "        \"context\": \"The capital of France is Paris. Paris has a population of about 2 million people.\",\n",
        "        \"response\": \"London is the capital of France. It has a population of 10 million.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Mixed: Correct + Fabricated\",\n",
        "        \"context\": \"Python was created by Guido van Rossum and first released in 1991.\",\n",
        "        \"response\": \"Python was created by Guido van Rossum. It was developed at MIT in 1985.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "if nli_available:\n",
        "    for case in nli_test_cases:\n",
        "        print(f\"\\n{'─'*65}\")\n",
        "        print(f\"[{case['name']}]\")\n",
        "        print(f\"{'─'*65}\")\n",
        "        print(f\"Context: {case['context']}\")\n",
        "        print(f\"Response: {case['response']}\")\n",
        "        \n",
        "        result = nli_detect_hallucination(case['context'], case['response'])\n",
        "        \n",
        "        print(f\"\\nClaim Analysis:\")\n",
        "        for claim_result in result['all_claims']:\n",
        "            symbol = \"⚠\" if claim_result['label'] == 'contradiction' else \"✓\" if claim_result['label'] == 'entailment' else \"○\"\n",
        "            claim_display = claim_result['claim'][:50] + \"...\" if len(claim_result['claim']) > 50 else claim_result['claim']\n",
        "            print(f\"  {symbol} [{claim_result['label']:12}] ({claim_result['score']:.2f}) \\\"{claim_display}\\\"\")\n",
        "        \n",
        "        if result['has_hallucination']:\n",
        "            print(f\"\\n⚠ HALLUCINATION DETECTED: {len(result['contradictions'])} contradicting claim(s)\")\n",
        "        else:\n",
        "            print(f\"\\n✓ No contradictions detected\")\n",
        "else:\n",
        "    print(\"\\n⚠ NLI model not available - showing conceptual usage\")\n",
        "    print(\"\"\"\n",
        "    from transformers import pipeline\n",
        "    \n",
        "    nli = pipeline(\"text-classification\", model=\"cross-encoder/nli-deberta-v3-base\")\n",
        "    \n",
        "    # Format: premise </s></s> hypothesis\n",
        "    result = nli(\"Paris is in France.</s></s>Paris is in Germany.\")\n",
        "    # Returns: {'label': 'contradiction', 'score': 0.98}\n",
        "    \"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "NLI DETECTION: Live Demo with LLM-Generated Responses\n",
            "=================================================================\n",
            "Context: TechCorp was founded in 2015 by Jane Smith in Austin, Texas. \n",
            "    The company specializes in cloud s...\n",
            "Question: Tell me everything about TechCorp including its founding, products, and any acquisitions.\n",
            "\n",
            "Generating response...\n",
            "\n",
            "Response: Based solely on the provided context, here is everything about TechCorp:\n",
            "\n",
            "- TechCorp was founded in 2015 by Jane Smith in Austin, Texas.\n",
            "- The company specializes in cloud security software.\n",
            "- As of 2024, TechCorp has 500 employees and annual revenue of $50 million.\n",
            "- The context does not mention any specific products beyond the specialization in cloud security software.\n",
            "- The context does not mention any acquisitions.\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "NLI Analysis:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "  ✓ [entailment  ] Based solely on the provided context, here is everything abo...\n",
            "  ✓ [entailment  ] - The company specializes in cloud security software\n",
            "  ✓ [entailment  ] - As of 2024, TechCorp has 500 employees and annual revenue ...\n",
            "  ○ [neutral     ] - The context does not mention any specific products beyond ...\n",
            "  ○ [neutral     ] - The context does not mention any acquisitions\n",
            "\n",
            "✓ Response appears consistent with context\n",
            "  Note: NLI catches contradictions but may miss fabricated details\n"
          ]
        }
      ],
      "source": [
        "# NLI-Based Detection: Live demo with Ollama\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"NLI DETECTION: Live Demo with LLM-Generated Responses\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "if not ollama_ready:\n",
        "    print(\"\\n⚠ Ollama not running - skipping live demo\")\n",
        "elif not nli_available:\n",
        "    print(\"\\n⚠ NLI model not available - skipping live demo\")\n",
        "else:\n",
        "    # Test with a prompt likely to cause hallucination\n",
        "    context = \"\"\"TechCorp was founded in 2015 by Jane Smith in Austin, Texas. \n",
        "    The company specializes in cloud security software. \n",
        "    As of 2024, TechCorp has 500 employees and annual revenue of $50 million.\"\"\"\n",
        "    \n",
        "    # Question that might tempt the LLM to hallucinate\n",
        "    question = \"Tell me everything about TechCorp including its founding, products, and any acquisitions.\"\n",
        "    \n",
        "    prompt = f\"\"\"Answer the question based on the context. Only include information from the context.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    print(f\"Context: {context[:100]}...\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(\"\\nGenerating response...\")\n",
        "    \n",
        "    response = ollama_generate(prompt, temperature=0.7)\n",
        "    response = clean_response(response)\n",
        "    \n",
        "    print(f\"\\nResponse: {response}\")\n",
        "    \n",
        "    # Run NLI detection\n",
        "    print(f\"\\n{'─'*65}\")\n",
        "    print(\"NLI Analysis:\")\n",
        "    print(f\"{'─'*65}\")\n",
        "    \n",
        "    result = nli_detect_hallucination(context, response)\n",
        "    \n",
        "    for claim_result in result['all_claims']:\n",
        "        symbol = \"⚠\" if claim_result['label'] == 'contradiction' else \"✓\" if claim_result['label'] == 'entailment' else \"○\"\n",
        "        claim_display = claim_result['claim'][:60] + \"...\" if len(claim_result['claim']) > 60 else claim_result['claim']\n",
        "        print(f\"  {symbol} [{claim_result['label']:12}] {claim_display}\")\n",
        "    \n",
        "    if result['has_hallucination']:\n",
        "        print(f\"\\n⚠ POTENTIAL HALLUCINATIONS: {len(result['contradictions'])} contradicting claim(s)\")\n",
        "    else:\n",
        "        print(f\"\\n✓ Response appears consistent with context\")\n",
        "        print(\"  Note: NLI catches contradictions but may miss fabricated details\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Demo 3: Self-Consistency Check\n",
        "\n",
        "Generate multiple responses and check agreement on factual claims.\n",
        "\n",
        "**Principle:** If the model is uncertain/hallucinating, multiple samples with temperature > 0 will disagree.\n",
        "\n",
        "**Pros:** No additional models needed, cheap\n",
        "**Cons:** Multiple LLM calls, only catches uncertainty-based hallucinations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "SELF-CONSISTENCY: Multi-Sample Agreement Check\n",
            "=================================================================\n",
            "\n",
            "Context: The Great Wall of China is approximately 21,196 kilometers long. Construction began in 7th century BC.\n",
            "\n",
            "Generating 3 samples with temperature=0.8...\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "Responses:\n",
            "\n",
            "  Sample 1: Approximately 21,196 kilometers long; construction began in the 7th century BC.\n",
            "\n",
            "  Sample 2: About 21,196 kilometers long; construction began in the 7th century BC.\n",
            "\n",
            "  Sample 3: Approximately 21,196 kilometers long; construction began in the 7th century BC.\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "Consistency Analysis:\n",
            "  Overall Score: 0.67\n",
            "  Number Agreement: 1.00\n",
            "  Entity Agreement: 0.33\n",
            "  Common Numbers: {'21,196'}\n",
            "  Common Entities: ['BC.']\n",
            "\n",
            "✓ Responses are reasonably consistent\n"
          ]
        }
      ],
      "source": [
        "# Self-Consistency: Implementation and Demo\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"SELF-CONSISTENCY: Multi-Sample Agreement Check\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "def self_consistency_check(\n",
        "    prompt: str,\n",
        "    n_samples: int = 3,\n",
        "    temperature: float = 0.8\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Generate multiple responses and check for consistency.\"\"\"\n",
        "    \n",
        "    responses = []\n",
        "    for i in range(n_samples):\n",
        "        resp = ollama_generate(prompt, temperature=temperature)\n",
        "        resp = clean_response(resp)\n",
        "        responses.append(resp)\n",
        "    \n",
        "    # Simple consistency check: extract key facts and compare\n",
        "    # In production, use embedding similarity or structured extraction\n",
        "    \n",
        "    # Extract numbers mentioned in each response\n",
        "    def extract_numbers(text):\n",
        "        return set(re.findall(r'\\b\\d+(?:\\.\\d+)?(?:,\\d+)*\\b', text))\n",
        "    \n",
        "    # Extract proper nouns (simplified)\n",
        "    def extract_entities(text):\n",
        "        words = text.split()\n",
        "        return set(w for w in words if w and w[0].isupper() and len(w) > 2)\n",
        "    \n",
        "    all_numbers = [extract_numbers(r) for r in responses]\n",
        "    all_entities = [extract_entities(r) for r in responses]\n",
        "    \n",
        "    # Check agreement\n",
        "    if all(len(nums) > 0 for nums in all_numbers):\n",
        "        number_agreement = len(set.intersection(*all_numbers)) / max(len(set.union(*all_numbers)), 1)\n",
        "    else:\n",
        "        number_agreement = 1.0  # No numbers to compare\n",
        "        \n",
        "    if all(len(ents) > 0 for ents in all_entities):\n",
        "        entity_agreement = len(set.intersection(*all_entities)) / max(len(set.union(*all_entities)), 1)\n",
        "    else:\n",
        "        entity_agreement = 1.0  # No entities to compare\n",
        "    \n",
        "    overall_consistency = (number_agreement + entity_agreement) / 2\n",
        "    \n",
        "    return {\n",
        "        \"responses\": responses,\n",
        "        \"consistency_score\": overall_consistency,\n",
        "        \"number_agreement\": number_agreement,\n",
        "        \"entity_agreement\": entity_agreement,\n",
        "        \"common_numbers\": set.intersection(*all_numbers) if all_numbers and all(all_numbers) else set(),\n",
        "        \"common_entities\": set.intersection(*all_entities) if all_entities and all(all_entities) else set()\n",
        "    }\n",
        "\n",
        "if not ollama_ready:\n",
        "    print(\"\\n⚠ Ollama not running - showing conceptual demo\")\n",
        "    print(\"\"\"\n",
        "CONCEPTUAL USAGE:\n",
        "\n",
        "    def self_consistency_check(prompt, n_samples=3, temperature=0.8):\n",
        "        responses = [llm.generate(prompt, temperature=temperature) for _ in range(n_samples)]\n",
        "        \n",
        "        # Extract and compare key facts\n",
        "        facts = [extract_facts(r) for r in responses]\n",
        "        \n",
        "        # Calculate agreement\n",
        "        agreement = intersection(facts) / union(facts)\n",
        "        \n",
        "        return agreement < 0.5  # Low agreement suggests hallucination\n",
        "    \"\"\")\n",
        "else:\n",
        "    # Test with a factual question\n",
        "    context = \"The Great Wall of China is approximately 21,196 kilometers long. Construction began in 7th century BC.\"\n",
        "    prompt = f\"\"\"Based on this context, answer the question.\n",
        "    \n",
        "Context: {context}\n",
        "\n",
        "Question: How long is the Great Wall of China and when was it built?\n",
        "\n",
        "Answer briefly:\"\"\"\n",
        "    \n",
        "    print(f\"\\nContext: {context}\")\n",
        "    print(\"\\nGenerating 3 samples with temperature=0.8...\")\n",
        "    \n",
        "    result = self_consistency_check(prompt, n_samples=3, temperature=0.8)\n",
        "    \n",
        "    print(f\"\\n{'─'*65}\")\n",
        "    print(\"Responses:\")\n",
        "    for i, resp in enumerate(result['responses'], 1):\n",
        "        resp_display = resp[:150] + \"...\" if len(resp) > 150 else resp\n",
        "        print(f\"\\n  Sample {i}: {resp_display}\")\n",
        "    \n",
        "    print(f\"\\n{'─'*65}\")\n",
        "    print(\"Consistency Analysis:\")\n",
        "    print(f\"  Overall Score: {result['consistency_score']:.2f}\")\n",
        "    print(f\"  Number Agreement: {result['number_agreement']:.2f}\")\n",
        "    print(f\"  Entity Agreement: {result['entity_agreement']:.2f}\")\n",
        "    print(f\"  Common Numbers: {result['common_numbers']}\")\n",
        "    common_ents = list(result['common_entities'])[:5]\n",
        "    print(f\"  Common Entities: {common_ents}{'...' if len(result['common_entities']) > 5 else ''}\")\n",
        "    \n",
        "    if result['consistency_score'] < 0.5:\n",
        "        print(f\"\\n⚠ LOW CONSISTENCY - Potential hallucination or uncertainty\")\n",
        "    else:\n",
        "        print(f\"\\n✓ Responses are reasonably consistent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Comparison Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "HALLUCINATION DETECTION: Method Comparison (Demos 1-3)\n",
            "======================================================================\n",
            "\n",
            "┌─────────────────────┬──────────────┬───────────────┬─────────────────────┐\n",
            "│ Method              │ Speed        │ Granularity   │ Best For            │\n",
            "├─────────────────────┼──────────────┼───────────────┼─────────────────────┤\n",
            "│ LettuceDetect       │ ~50-100ms    │ Token-level   │ RAG, real-time      │\n",
            "│ NLI-Based           │ ~20-50ms     │ Sentence      │ Contradiction check │\n",
            "│ Self-Consistency    │ N × LLM call │ Response      │ Uncertainty detect  │\n",
            "│ LLM-as-Judge        │ ~1-3s        │ Semantic      │ High-stakes, audit  │\n",
            "└─────────────────────┴──────────────┴───────────────┴─────────────────────┘\n",
            "\n",
            "DETECTION CAPABILITIES:\n",
            "\n",
            "  Intrinsic (Contradicts Context):\n",
            "    ✓ LettuceDetect - Excellent (token-level)\n",
            "    ✓ NLI-Based     - Good (sentence-level)\n",
            "    ○ Self-Consistency - Limited\n",
            "\n",
            "  Extrinsic (Fabricated Info):\n",
            "    ✓ LettuceDetect - Good (flags unsupported tokens)\n",
            "    ○ NLI-Based     - Limited (neutral ≠ false)\n",
            "    ○ Self-Consistency - Catches uncertainty\n",
            "\n",
            "→ See Demo 4 for HaluGate (vLLM's 3-stage pipeline)\n",
            "\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "DEMO 1-3 STATUS\n",
            "──────────────────────────────────────────────────────────────────────\n",
            "  ✓ Ollama (qwen3:4b)\n",
            "  ✗ pip install lettucedetect LettuceDetect\n",
            "  ✓ NLI Model\n"
          ]
        }
      ],
      "source": [
        "# Hallucination Detection: Comparison Summary (Demos 1-3)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"HALLUCINATION DETECTION: Method Comparison (Demos 1-3)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "┌─────────────────────┬──────────────┬───────────────┬─────────────────────┐\n",
        "│ Method              │ Speed        │ Granularity   │ Best For            │\n",
        "├─────────────────────┼──────────────┼───────────────┼─────────────────────┤\n",
        "│ LettuceDetect       │ ~50-100ms    │ Token-level   │ RAG, real-time      │\n",
        "│ NLI-Based           │ ~20-50ms     │ Sentence      │ Contradiction check │\n",
        "│ Self-Consistency    │ N × LLM call │ Response      │ Uncertainty detect  │\n",
        "│ LLM-as-Judge        │ ~1-3s        │ Semantic      │ High-stakes, audit  │\n",
        "└─────────────────────┴──────────────┴───────────────┴─────────────────────┘\n",
        "\n",
        "DETECTION CAPABILITIES:\n",
        "\n",
        "  Intrinsic (Contradicts Context):\n",
        "    ✓ LettuceDetect - Excellent (token-level)\n",
        "    ✓ NLI-Based     - Good (sentence-level)\n",
        "    ○ Self-Consistency - Limited\n",
        "    \n",
        "  Extrinsic (Fabricated Info):\n",
        "    ✓ LettuceDetect - Good (flags unsupported tokens)\n",
        "    ○ NLI-Based     - Limited (neutral ≠ false)\n",
        "    ○ Self-Consistency - Catches uncertainty\n",
        "\n",
        "→ See Demo 4 for HaluGate (vLLM's 3-stage pipeline)\n",
        "\"\"\")\n",
        "\n",
        "print(\"─\"*70)\n",
        "print(\"DEMO 1-3 STATUS\")\n",
        "print(\"─\"*70)\n",
        "status = [\n",
        "    (\"Ollama (qwen3:4b)\", \"✓\" if ollama_ready else \"✗\"),\n",
        "    (\"LettuceDetect\", \"✓\" if lettucedetect_available else \"✗ pip install lettucedetect\"),\n",
        "    (\"NLI Model\", \"✓\" if nli_available else \"✗ pip install transformers torch\"),\n",
        "]\n",
        "for name, stat in status:\n",
        "    print(f\"  {stat} {name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Demo 4: HaluGate-Style Pipeline\n",
        "\n",
        "A 3-stage pipeline for hallucination detection without LLM-as-judge:\n",
        "\n",
        "| Stage | Model | Function |\n",
        "|-------|-------|----------|\n",
        "| 1 | **Sentinel** | Classifies if prompt needs fact-checking |\n",
        "| 2 | **Detector** | Claim-level hallucination detection (NLI) |\n",
        "| 3 | **Explainer** | NLI explanation (contradiction/neutral/entailment) |\n",
        "\n",
        "**Models used:**\n",
        "- Sentinel: `llm-semantic-router/halugate-sentinel`\n",
        "- Detector/Explainer: `cross-encoder/nli-deberta-v3-base`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "HALUGATE-STYLE: 3-Stage Hallucination Detection Pipeline\n",
            "=================================================================\n",
            "\n",
            "Device: MPS (Apple Silicon)\n",
            "\n",
            "Loading HaluGate models...\n",
            "\n",
            "[1/3] Loading Sentinel (prompt classifier)...\n",
            "      Model: llm-semantic-router/halugate-sentinel\n",
            "  ✓ Sentinel loaded\n",
            "[2/3] Loading NLI model for detection...\n",
            "      Model: cross-encoder/nli-deberta-v3-base\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use mps\n",
            "\u001b[92m[INFO]\u001b[0m HaluGate-style pipeline ready\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✓ NLI model loaded (for detection + explanation)\n",
            "[3/3] Explainer uses same NLI model\n",
            "  ✓ Shared NLI pipeline\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "✓ Pipeline ready\n",
            "  • Sentinel: llm-semantic-router/halugate-sentinel\n",
            "  • Detector/Explainer: cross-encoder/nli-deberta-v3-base\n",
            "─────────────────────────────────────────────────────────────────\n"
          ]
        }
      ],
      "source": [
        "# HaluGate-Style Pipeline: Setup and Model Loading\n",
        "# \n",
        "# Models used:\n",
        "# - Sentinel: llm-semantic-router/halugate-sentinel\n",
        "# - Detector/Explainer: cross-encoder/nli-deberta-v3-base\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"HALUGATE-STYLE: 3-Stage Hallucination Detection Pipeline\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "halugate_available = False\n",
        "sentinel_model = None\n",
        "sentinel_tokenizer = None\n",
        "halugate_nli_pipeline = None\n",
        "\n",
        "try:\n",
        "    from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
        "    import torch\n",
        "    \n",
        "    # Determine device\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        device_name = \"MPS (Apple Silicon)\"\n",
        "    elif torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        device_name = \"CUDA\"\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        device_name = \"CPU\"\n",
        "    \n",
        "    print(f\"\\nDevice: {device_name}\")\n",
        "    print(\"\\nLoading HaluGate models...\")\n",
        "    \n",
        "    # Stage 1: Sentinel (prompt classifier)\n",
        "    print(\"\\n[1/3] Loading Sentinel (prompt classifier)...\")\n",
        "    print(\"      Model: llm-semantic-router/halugate-sentinel\")\n",
        "    sentinel_tokenizer = AutoTokenizer.from_pretrained(\"llm-semantic-router/halugate-sentinel\")\n",
        "    sentinel_model = AutoModelForSequenceClassification.from_pretrained(\"llm-semantic-router/halugate-sentinel\")\n",
        "    sentinel_model.to(device)\n",
        "    sentinel_model.eval()\n",
        "    sentinel_id2label = sentinel_model.config.id2label\n",
        "    print(\"  ✓ Sentinel loaded\")\n",
        "    \n",
        "    # Stage 2 & 3: Use NLI model for both detection and explanation\n",
        "    print(\"[2/3] Loading NLI model for detection...\")\n",
        "    print(\"      Model: cross-encoder/nli-deberta-v3-base\")\n",
        "    halugate_nli_pipeline = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=\"cross-encoder/nli-deberta-v3-base\",\n",
        "        device=device\n",
        "    )\n",
        "    print(\"  ✓ NLI model loaded (for detection + explanation)\")\n",
        "    \n",
        "    print(\"[3/3] Explainer uses same NLI model\")\n",
        "    print(\"  ✓ Shared NLI pipeline\")\n",
        "    \n",
        "    halugate_available = True\n",
        "    logger.info(\"HaluGate-style pipeline ready\")\n",
        "    print(\"\\n\" + \"─\"*65)\n",
        "    print(\"✓ Pipeline ready\")\n",
        "    print(\"  • Sentinel: llm-semantic-router/halugate-sentinel\")\n",
        "    print(\"  • Detector/Explainer: cross-encoder/nli-deberta-v3-base\")\n",
        "    print(\"─\"*65)\n",
        "    \n",
        "except ImportError as e:\n",
        "    logger.warning(f\"Transformers not installed: {e}\")\n",
        "    print(\"\\n✗ Transformers not available\")\n",
        "    print(\"\\nTo install:\")\n",
        "    print(\"  pip install transformers torch\")\n",
        "    \n",
        "except Exception as e:\n",
        "    logger.error(f\"HaluGate setup failed: {e}\")\n",
        "    print(f\"\\n✗ Error loading HaluGate models: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "HALUGATE-STYLE: Pipeline Functions\n",
            "=================================================================\n",
            "✓ Pipeline functions defined:\n",
            "  • halugate_sentinel(prompt) → needs_check, confidence\n",
            "  • halugate_detector(context, answer) → contradictions, unsupported\n",
            "  • halugate_explainer(context, claim) → entailment/neutral/contradiction\n",
            "  • halugate_full_pipeline(context, question, answer) → full analysis\n"
          ]
        }
      ],
      "source": [
        "# HaluGate-Style: Pipeline Implementation\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"HALUGATE-STYLE: Pipeline Functions\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "if not halugate_available:\n",
        "    print(\"\\n⚠ HaluGate models not loaded - showing conceptual implementation\")\n",
        "    print(\"\"\"\n",
        "HALUGATE PIPELINE ARCHITECTURE:\n",
        "\n",
        "    ┌─────────────────────────────────────────────────────────────┐\n",
        "    │                     USER PROMPT                             │\n",
        "    └─────────────────────────────────────────────────────────────┘\n",
        "                              │\n",
        "                              ▼\n",
        "    ┌─────────────────────────────────────────────────────────────┐\n",
        "    │  STAGE 1: SENTINEL                                          │\n",
        "    │  \"Does this prompt need fact-checking?\"                     │\n",
        "    │  → FACT_CHECK_NEEDED / NO_FACT_CHECK_NEEDED                 │\n",
        "    └─────────────────────────────────────────────────────────────┘\n",
        "                              │\n",
        "              ┌───────────────┴───────────────┐\n",
        "              │                               │\n",
        "    NO_FACT_CHECK_NEEDED              FACT_CHECK_NEEDED\n",
        "              │                               │\n",
        "              ▼                               ▼\n",
        "    ┌─────────────────┐         ┌─────────────────────────────────┐\n",
        "    │ Skip detection  │         │  STAGE 2: DETECTOR (NLI)        │\n",
        "    │ (72% of traffic)│         │  Claim-level classification     │\n",
        "    └─────────────────┘         │  → contradiction/neutral/entail │\n",
        "                                └─────────────────────────────────┘\n",
        "                                              │\n",
        "                                              ▼\n",
        "                                ┌─────────────────────────────────┐\n",
        "                                │  STAGE 3: EXPLAINER             │\n",
        "                                │  Detailed NLI explanation       │\n",
        "                                │  → contradiction/neutral/entail │\n",
        "                                └─────────────────────────────────┘\n",
        "    \"\"\")\n",
        "else:\n",
        "    import torch\n",
        "    import torch.nn.functional as F\n",
        "    \n",
        "    def halugate_sentinel(prompt: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Stage 1: Determine if prompt needs fact-checking.\n",
        "        Uses official halugate-sentinel model.\n",
        "        Returns classification and confidence.\n",
        "        \"\"\"\n",
        "        inputs = sentinel_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = sentinel_model(**inputs)\n",
        "            probs = F.softmax(outputs.logits, dim=-1)\n",
        "            pred_class = torch.argmax(probs, dim=-1).item()\n",
        "            confidence = probs[0][pred_class].item()\n",
        "        \n",
        "        label = sentinel_id2label.get(pred_class, str(pred_class))\n",
        "        needs_check = \"FACT_CHECK\" in label.upper() or \"CHECK\" in label.upper()\n",
        "        \n",
        "        return {\n",
        "            \"label\": label,\n",
        "            \"needs_check\": needs_check,\n",
        "            \"confidence\": confidence\n",
        "        }\n",
        "    \n",
        "    def halugate_detector(context: str, answer: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Stage 2: Claim-level hallucination detection using NLI.\n",
        "        Checks if answer claims are supported by context.\n",
        "        (Uses NLI model as halugate-detector not yet public)\n",
        "        \"\"\"\n",
        "        # Split answer into claims\n",
        "        claims = [s.strip() for s in re.split(r'[.!?]+', answer) if len(s.strip()) > 10]\n",
        "        \n",
        "        results = []\n",
        "        contradictions = []\n",
        "        unsupported = []\n",
        "        \n",
        "        for claim in claims:\n",
        "            # NLI: context </s></s> claim\n",
        "            nli_input = f\"{context}</s></s>{claim}\"\n",
        "            prediction = halugate_nli_pipeline(nli_input)[0]\n",
        "            \n",
        "            result = {\n",
        "                \"claim\": claim,\n",
        "                \"label\": prediction[\"label\"],\n",
        "                \"score\": prediction[\"score\"]\n",
        "            }\n",
        "            results.append(result)\n",
        "            \n",
        "            if prediction[\"label\"] == \"contradiction\" and prediction[\"score\"] > 0.7:\n",
        "                contradictions.append(claim)\n",
        "            elif prediction[\"label\"] == \"neutral\" and prediction[\"score\"] > 0.7:\n",
        "                unsupported.append(claim)\n",
        "        \n",
        "        return {\n",
        "            \"claims\": results,\n",
        "            \"contradictions\": contradictions,\n",
        "            \"unsupported\": unsupported,\n",
        "            \"has_hallucination\": len(contradictions) > 0\n",
        "        }\n",
        "    \n",
        "    def halugate_explainer(context: str, claim: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Stage 3: NLI explanation for detected hallucinations.\n",
        "        Classifies relationship between context and claim.\n",
        "        \"\"\"\n",
        "        nli_input = f\"{context}</s></s>{claim}\"\n",
        "        prediction = halugate_nli_pipeline(nli_input)[0]\n",
        "        \n",
        "        return {\n",
        "            \"label\": prediction[\"label\"],\n",
        "            \"confidence\": prediction[\"score\"],\n",
        "            \"is_contradiction\": prediction[\"label\"] == \"contradiction\"\n",
        "        }\n",
        "    \n",
        "    def halugate_full_pipeline(context: str, question: str, answer: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Full HaluGate pipeline: Sentinel → Detector → Explainer\n",
        "        \"\"\"\n",
        "        result = {\n",
        "            \"context\": context,\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"stages\": {}\n",
        "        }\n",
        "        \n",
        "        # Stage 1: Sentinel (official model)\n",
        "        sentinel_result = halugate_sentinel(question)\n",
        "        result[\"stages\"][\"sentinel\"] = sentinel_result\n",
        "        \n",
        "        if not sentinel_result[\"needs_check\"]:\n",
        "            result[\"final_verdict\"] = \"SKIPPED\"\n",
        "            result[\"explanation\"] = \"Prompt does not require fact-checking\"\n",
        "            return result\n",
        "        \n",
        "        # Stage 2: Detector (NLI-based)\n",
        "        detector_result = halugate_detector(context, answer)\n",
        "        result[\"stages\"][\"detector\"] = {\n",
        "            \"contradictions\": detector_result[\"contradictions\"],\n",
        "            \"unsupported\": detector_result[\"unsupported\"],\n",
        "            \"has_hallucination\": detector_result[\"has_hallucination\"]\n",
        "        }\n",
        "        \n",
        "        if not detector_result[\"has_hallucination\"] and not detector_result[\"unsupported\"]:\n",
        "            result[\"final_verdict\"] = \"PASSED\"\n",
        "            result[\"explanation\"] = \"All claims supported by context\"\n",
        "            return result\n",
        "        \n",
        "        # Stage 3: Explainer (provides verdict)\n",
        "        if detector_result[\"contradictions\"]:\n",
        "            # Explain the first contradiction\n",
        "            explainer_result = halugate_explainer(context, detector_result[\"contradictions\"][0])\n",
        "            result[\"stages\"][\"explainer\"] = explainer_result\n",
        "            result[\"final_verdict\"] = \"HALLUCINATION_DETECTED\"\n",
        "            result[\"explanation\"] = f\"Response contradicts context: '{detector_result['contradictions'][0][:50]}...'\"\n",
        "        elif detector_result[\"unsupported\"]:\n",
        "            explainer_result = halugate_explainer(context, detector_result[\"unsupported\"][0])\n",
        "            result[\"stages\"][\"explainer\"] = explainer_result\n",
        "            result[\"final_verdict\"] = \"UNSUPPORTED\"\n",
        "            result[\"explanation\"] = f\"Claim not in context: '{detector_result['unsupported'][0][:50]}...'\"\n",
        "        else:\n",
        "            result[\"final_verdict\"] = \"PASSED\"\n",
        "            result[\"explanation\"] = \"Response is supported by context\"\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    print(\"✓ Pipeline functions defined:\")\n",
        "    print(\"  • halugate_sentinel(prompt) → needs_check, confidence\")\n",
        "    print(\"  • halugate_detector(context, answer) → contradictions, unsupported\")\n",
        "    print(\"  • halugate_explainer(context, claim) → entailment/neutral/contradiction\")\n",
        "    print(\"  • halugate_full_pipeline(context, question, answer) → full analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "HALUGATE-STYLE: Full Pipeline Demo\n",
            "=================================================================\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "[Creative Prompt (Should Skip)]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "Question: Write a poem about autumn leaves\n",
            "Answer: Golden leaves fall gently down, painting nature's carpet on the ground.\n",
            "\n",
            "Pipeline Results:\n",
            "  [Sentinel] NO_FACT_CHECK_NEEDED (confidence: 1.00)\n",
            "  [Detector] ○ Unsupported claims: 1 found\n",
            "  [Explainer] ○ neutral (confidence: 0.99)\n",
            "\n",
            "  ⚠ VERDICT: UNSUPPORTED\n",
            "    Claim not in context: 'Golden leaves fall gently down, painting nature's ...'\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "[Factual - Correct Answer]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "Context: The Eiffel Tower is located in Paris, France. It was complet...\n",
            "Question: Where is the Eiffel Tower and how tall is it?\n",
            "Answer: The Eiffel Tower is located in Paris, France. It stands 330 meters tall.\n",
            "\n",
            "Pipeline Results:\n",
            "  [Sentinel] FACT_CHECK_NEEDED (confidence: 0.93)\n",
            "  [Detector] ✓ All claims supported\n",
            "\n",
            "  ✓ VERDICT: PASSED\n",
            "    All claims supported by context\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "[Factual - Intrinsic Hallucination]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "Context: The Eiffel Tower is located in Paris, France. It was complet...\n",
            "Question: Where is the Eiffel Tower located?\n",
            "Answer: The Eiffel Tower is located in Berlin, Germany.\n",
            "\n",
            "Pipeline Results:\n",
            "  [Sentinel] FACT_CHECK_NEEDED (confidence: 0.91)\n",
            "  [Detector] ⚠ Contradictions: 1 found\n",
            "  [Explainer] ⚠ contradiction (confidence: 1.00)\n",
            "\n",
            "  ⚠ VERDICT: HALLUCINATION_DETECTED\n",
            "    Response contradicts context: 'The Eiffel Tower is located in Berlin, Germany...'\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "[Factual - Extrinsic Hallucination]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "Context: Apple Inc. was founded in 1976 by Steve Jobs and Steve Wozni...\n",
            "Question: Who founded Apple?\n",
            "Answer: Apple was founded by Steve Jobs and Steve Wozniak in 1976. They started the company in Bill Gates' garage.\n",
            "\n",
            "Pipeline Results:\n",
            "  [Sentinel] FACT_CHECK_NEEDED (confidence: 0.78)\n",
            "  [Detector] ⚠ Contradictions: 1 found\n",
            "  [Explainer] ⚠ contradiction (confidence: 0.97)\n",
            "\n",
            "  ⚠ VERDICT: HALLUCINATION_DETECTED\n",
            "    Response contradicts context: 'They started the company in Bill Gates' garage...'\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "[RAG Scenario - Tool Response]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "Context: Order #12345 was placed on 2024-01-15. Status: Shipped. Expe...\n",
            "Question: What's the status of order #12345?\n",
            "Answer: Order #12345 was shipped on January 15th and will arrive by January 20th.\n",
            "\n",
            "Pipeline Results:\n",
            "  [Sentinel] FACT_CHECK_NEEDED (confidence: 0.94)\n",
            "  [Detector] ✓ All claims supported\n",
            "\n",
            "  ✓ VERDICT: PASSED\n",
            "    All claims supported by context\n"
          ]
        }
      ],
      "source": [
        "# HaluGate-Style: Demo with Test Cases\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"HALUGATE-STYLE: Full Pipeline Demo\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "# Test cases covering different scenarios\n",
        "halugate_test_cases = [\n",
        "    {\n",
        "        \"name\": \"Creative Prompt (Should Skip)\",\n",
        "        \"context\": \"\",\n",
        "        \"question\": \"Write a poem about autumn leaves\",\n",
        "        \"answer\": \"Golden leaves fall gently down, painting nature's carpet on the ground.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Factual - Correct Answer\",\n",
        "        \"context\": \"The Eiffel Tower is located in Paris, France. It was completed in 1889 and is 330 meters tall.\",\n",
        "        \"question\": \"Where is the Eiffel Tower and how tall is it?\",\n",
        "        \"answer\": \"The Eiffel Tower is located in Paris, France. It stands 330 meters tall.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Factual - Intrinsic Hallucination\",\n",
        "        \"context\": \"The Eiffel Tower is located in Paris, France. It was completed in 1889.\",\n",
        "        \"question\": \"Where is the Eiffel Tower located?\",\n",
        "        \"answer\": \"The Eiffel Tower is located in Berlin, Germany.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Factual - Extrinsic Hallucination\",\n",
        "        \"context\": \"Apple Inc. was founded in 1976 by Steve Jobs and Steve Wozniak.\",\n",
        "        \"question\": \"Who founded Apple?\",\n",
        "        \"answer\": \"Apple was founded by Steve Jobs and Steve Wozniak in 1976. They started the company in Bill Gates' garage.\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"RAG Scenario - Tool Response\",\n",
        "        \"context\": \"Order #12345 was placed on 2024-01-15. Status: Shipped. Expected delivery: 2024-01-20.\",\n",
        "        \"question\": \"What's the status of order #12345?\",\n",
        "        \"answer\": \"Order #12345 was shipped on January 15th and will arrive by January 20th.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "if not halugate_available:\n",
        "    print(\"\\n⚠ HaluGate models not loaded - showing expected outputs\")\n",
        "    print(\"\\nExpected behavior for test cases:\")\n",
        "    for case in halugate_test_cases:\n",
        "        print(f\"\\n[{case['name']}]\")\n",
        "        print(f\"  Question: {case['question'][:50]}...\")\n",
        "        if \"Creative\" in case['name']:\n",
        "            print(\"  → Sentinel: NO_FACT_CHECK_NEEDED → Skip detection\")\n",
        "        elif \"Correct\" in case['name'] or \"RAG\" in case['name']:\n",
        "            print(\"  → Sentinel: FACT_CHECK_NEEDED → Detector: No hallucination → PASSED\")\n",
        "        else:\n",
        "            print(\"  → Sentinel: FACT_CHECK_NEEDED → Detector: Hallucination → Explainer: contradiction\")\n",
        "else:\n",
        "    for case in halugate_test_cases:\n",
        "        print(f\"\\n{'─'*65}\")\n",
        "        print(f\"[{case['name']}]\")\n",
        "        print(f\"{'─'*65}\")\n",
        "        \n",
        "        if case['context']:\n",
        "            ctx_display = case['context'][:60] + \"...\" if len(case['context']) > 60 else case['context']\n",
        "            print(f\"Context: {ctx_display}\")\n",
        "        print(f\"Question: {case['question']}\")\n",
        "        print(f\"Answer: {case['answer']}\")\n",
        "        \n",
        "        # Run full pipeline\n",
        "        result = halugate_full_pipeline(\n",
        "            context=case['context'],\n",
        "            question=case['question'],\n",
        "            answer=case['answer']\n",
        "        )\n",
        "        \n",
        "        print(f\"\\nPipeline Results:\")\n",
        "        \n",
        "        # Stage 1\n",
        "        sentinel = result['stages']['sentinel']\n",
        "        print(f\"  [Sentinel] {sentinel['label']} (confidence: {sentinel['confidence']:.2f})\")\n",
        "        \n",
        "        # Stage 2 (if ran)\n",
        "        if 'detector' in result['stages']:\n",
        "            detector = result['stages']['detector']\n",
        "            if detector['contradictions']:\n",
        "                print(f\"  [Detector] ⚠ Contradictions: {len(detector['contradictions'])} found\")\n",
        "            elif detector['unsupported']:\n",
        "                print(f\"  [Detector] ○ Unsupported claims: {len(detector['unsupported'])} found\")\n",
        "            else:\n",
        "                print(f\"  [Detector] ✓ All claims supported\")\n",
        "        \n",
        "        # Stage 3 (if ran)\n",
        "        if 'explainer' in result['stages']:\n",
        "            explainer = result['stages']['explainer']\n",
        "            symbol = \"⚠\" if explainer['is_contradiction'] else \"○\" if explainer['label'] == 'neutral' else \"✓\"\n",
        "            print(f\"  [Explainer] {symbol} {explainer['label']} (confidence: {explainer['confidence']:.2f})\")\n",
        "        \n",
        "        # Final verdict\n",
        "        verdict_symbol = {\n",
        "            \"PASSED\": \"✓\",\n",
        "            \"SKIPPED\": \"○\",\n",
        "            \"HALLUCINATION_DETECTED\": \"⚠\",\n",
        "            \"UNSUPPORTED\": \"⚠\"\n",
        "        }.get(result['final_verdict'], \"?\")\n",
        "        \n",
        "        print(f\"\\n  {verdict_symbol} VERDICT: {result['final_verdict']}\")\n",
        "        print(f\"    {result['explanation']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "HALUGATE-STYLE: Live Demo with Ollama\n",
            "=================================================================\n",
            "\n",
            "═════════════════════════════════════════════════════════════════\n",
            "[Product Documentation]\n",
            "═════════════════════════════════════════════════════════════════\n",
            "Context: TechCorp Pro Plan: $50/month\n",
            "            - 1TB storage\n",
            "            - 25 user sea...\n",
            "Question: What does the Pro plan include?\n",
            "\n",
            "[Generating response with Ollama...]\n",
            "Response: 1TB storage  \n",
            "25 user seats  \n",
            "24/7 email support  \n",
            "API access\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "Pipeline Analysis:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "  [1] Sentinel: FACT_CHECK_NEEDED (1.00)\n",
            "  [2] Detector: ○ 1 unsupported claim(s)\n",
            "  [3] Explainer: neutral (0.98)\n",
            "\n",
            "  ══► VERDICT: UNSUPPORTED\n",
            "      Claim not in context: '1TB storage  \n",
            "25 user seats  \n",
            "24/7 email support  ...'\n",
            "\n",
            "═════════════════════════════════════════════════════════════════\n",
            "[Historical Query]\n",
            "═════════════════════════════════════════════════════════════════\n",
            "Context: The first iPhone was announced by Steve Jobs on January 9, 2007.\n",
            "            It ...\n",
            "Question: When was the first iPhone released and what were its specs?\n",
            "\n",
            "[Generating response with Ollama...]\n",
            "Response: The first iPhone was released on June 29, 2007, and it had a 3.5-inch screen and 2 megapixel camera.\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "Pipeline Analysis:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "  [1] Sentinel: FACT_CHECK_NEEDED (0.99)\n",
            "  [2] Detector: ○ 1 unsupported claim(s)\n",
            "  [3] Explainer: neutral (0.95)\n",
            "\n",
            "  ══► VERDICT: UNSUPPORTED\n",
            "      Claim not in context: '5-inch screen and 2 megapixel camera...'\n"
          ]
        }
      ],
      "source": [
        "# HaluGate-Style: Live Demo with Ollama-Generated Responses\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"HALUGATE-STYLE: Live Demo with Ollama\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "if not ollama_ready:\n",
        "    print(\"\\n⚠ Ollama not running - skipping live demo\")\n",
        "elif not halugate_available:\n",
        "    print(\"\\n⚠ Pipeline models not available - skipping live demo\")\n",
        "else:\n",
        "    # RAG-style scenarios\n",
        "    live_scenarios = [\n",
        "        {\n",
        "            \"name\": \"Product Documentation\",\n",
        "            \"context\": \"\"\"TechCorp Pro Plan: $50/month\n",
        "            - 1TB storage\n",
        "            - 25 user seats\n",
        "            - 24/7 email support\n",
        "            - API access included\n",
        "            Phone support is NOT included in Pro plan.\"\"\",\n",
        "            \"question\": \"What does the Pro plan include?\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Historical Query\",\n",
        "            \"context\": \"\"\"The first iPhone was announced by Steve Jobs on January 9, 2007.\n",
        "            It went on sale on June 29, 2007 in the United States.\n",
        "            The original iPhone had a 3.5-inch screen and 2 megapixel camera.\"\"\",\n",
        "            \"question\": \"When was the first iPhone released and what were its specs?\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    for scenario in live_scenarios:\n",
        "        print(f\"\\n{'═'*65}\")\n",
        "        print(f\"[{scenario['name']}]\")\n",
        "        print(f\"{'═'*65}\")\n",
        "        print(f\"Context: {scenario['context'][:80]}...\")\n",
        "        print(f\"Question: {scenario['question']}\")\n",
        "        \n",
        "        # Generate response with Ollama\n",
        "        prompt = f\"\"\"Based ONLY on the following context, answer the question.\n",
        "Do not add any information not present in the context.\n",
        "\n",
        "Context: {scenario['context']}\n",
        "\n",
        "Question: {scenario['question']}\n",
        "\n",
        "Answer:\"\"\"\n",
        "        \n",
        "        print(\"\\n[Generating response with Ollama...]\")\n",
        "        response = ollama_generate(prompt, temperature=0.5)\n",
        "        response = clean_response(response)\n",
        "        \n",
        "        resp_display = response[:150] + \"...\" if len(response) > 150 else response\n",
        "        print(f\"Response: {resp_display}\")\n",
        "        \n",
        "        # Run pipeline\n",
        "        print(f\"\\n{'─'*65}\")\n",
        "        print(\"Pipeline Analysis:\")\n",
        "        print(f\"{'─'*65}\")\n",
        "        \n",
        "        result = halugate_full_pipeline(\n",
        "            context=scenario['context'],\n",
        "            question=scenario['question'],\n",
        "            answer=response\n",
        "        )\n",
        "        \n",
        "        # Stage results\n",
        "        sentinel = result['stages']['sentinel']\n",
        "        print(f\"  [1] Sentinel: {sentinel['label']} ({sentinel['confidence']:.2f})\")\n",
        "        \n",
        "        if 'detector' in result['stages']:\n",
        "            detector = result['stages']['detector']\n",
        "            if detector['contradictions']:\n",
        "                print(f\"  [2] Detector: ⚠ {len(detector['contradictions'])} contradiction(s) found\")\n",
        "                for c in detector['contradictions'][:2]:\n",
        "                    print(f\"      → \\\"{c[:50]}...\\\"\")\n",
        "            elif detector['unsupported']:\n",
        "                print(f\"  [2] Detector: ○ {len(detector['unsupported'])} unsupported claim(s)\")\n",
        "            else:\n",
        "                print(f\"  [2] Detector: ✓ All claims grounded\")\n",
        "        \n",
        "        if 'explainer' in result['stages']:\n",
        "            explainer = result['stages']['explainer']\n",
        "            print(f\"  [3] Explainer: {explainer['label']} ({explainer['confidence']:.2f})\")\n",
        "        \n",
        "        # Verdict\n",
        "        print(f\"\\n  ══► VERDICT: {result['final_verdict']}\")\n",
        "        print(f\"      {result['explanation']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Demo 5: Grounded Prompting (Mitigation via Prompt Engineering)\n",
        "\n",
        "Reduce hallucinations through **prompt structure**, not additional models:\n",
        "\n",
        "| Technique | Effect |\n",
        "|-----------|--------|\n",
        "| Explicit grounding instruction | \"Answer ONLY based on context\" |\n",
        "| Context before question | Recency bias works in your favor |\n",
        "| \"I don't know\" permission | Model won't fabricate to please |\n",
        "| Citation requirement | Forces grounding in source |\n",
        "\n",
        "This is the cheapest mitigation — no extra models, just better prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "GROUNDED PROMPTING: Mitigation via Prompt Engineering\n",
            "=================================================================\n",
            "✓ Prompt builders defined:\n",
            "  • build_grounded_prompt(query, context) → Grounded prompt with rules\n",
            "  • build_ungrounded_prompt(query, context) → Simple prompt (baseline)\n"
          ]
        }
      ],
      "source": [
        "# Grounded Prompting: Implementation\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"GROUNDED PROMPTING: Mitigation via Prompt Engineering\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "def build_grounded_prompt(\n",
        "    query: str,\n",
        "    retrieved_context: str,\n",
        "    instructions: str = \"\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Build a prompt that encourages grounded responses.\n",
        "    \n",
        "    Key techniques:\n",
        "    1. Explicit grounding instruction\n",
        "    2. Context before question (recency bias)\n",
        "    3. \"I don't know\" permission\n",
        "    4. Citation requirement\n",
        "    \"\"\"\n",
        "    return f\"\"\"You are a helpful assistant that answers questions based ONLY on the provided context.\n",
        "\n",
        "RULES:\n",
        "- Answer ONLY based on information in the CONTEXT below\n",
        "- If the context doesn't contain the answer, say \"I don't have information about that in the provided documents\"\n",
        "- Quote or paraphrase directly from the context\n",
        "- Never make up information\n",
        "\n",
        "CONTEXT:\n",
        "{retrieved_context}\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "{instructions}\n",
        "\n",
        "Provide your answer, citing the relevant parts of the context:\"\"\"\n",
        "\n",
        "\n",
        "def build_ungrounded_prompt(query: str, context: str) -> str:\n",
        "    \"\"\"Simple prompt without grounding techniques.\"\"\"\n",
        "    return f\"\"\"Context: {context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "\n",
        "print(\"✓ Prompt builders defined:\")\n",
        "print(\"  • build_grounded_prompt(query, context) → Grounded prompt with rules\")\n",
        "print(\"  • build_ungrounded_prompt(query, context) → Simple prompt (baseline)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "GROUNDED vs UNGROUNDED: Side-by-Side Comparison\n",
            "=================================================================\n",
            "\n",
            "═════════════════════════════════════════════════════════════════\n",
            "[Question Beyond Context]\n",
            "═════════════════════════════════════════════════════════════════\n",
            "Context: TechCorp was founded in 2015 in Austin, Texas. The company has 500 employees....\n",
            "Question: Who is the CEO of TechCorp and what is the company's revenue?\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "UNGROUNDED PROMPT:\n",
            "  Response: Based **solely on the provided context**, **the answer cannot be determined**. Here's why:\n",
            "\n",
            "1.  **The context does not mention the CEO**: The context only states that \"TechCorp was founded in 2015 in ...\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "GROUNDED PROMPT:\n",
            "  Response: I don't have information about that in the provided documents\n",
            "\n",
            "The provided context only states: \"TechCorp was founded in 2015 in Austin, Texas. The company has 500 employees.\" There is no mention of ...\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "NLI ANALYSIS:\n",
            "  Ungrounded: 15 claims, 13 potentially ungrounded\n",
            "  Grounded:   3 claims, 1 potentially ungrounded\n",
            "  ✓ Grounded prompt produced more faithful response\n",
            "\n",
            "═════════════════════════════════════════════════════════════════\n",
            "[Tempting Fabrication]\n",
            "═════════════════════════════════════════════════════════════════\n",
            "Context: The Apollo 11 mission landed on the Moon on July 20, 1969. Neil Armstrong and Bu...\n",
            "Question: What did Neil Armstrong say when he first stepped on the Moon, and how long did the moonwalk last?\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "UNGROUNDED PROMPT:\n",
            "  Response: Based on historical records from NASA's Apollo 11 mission:\n",
            "\n",
            "1. **Neil Armstrong's first words on the Moon**:  \n",
            "   Neil Armstrong said:  \n",
            "   > **\"That's one small step for [a] man, one giant leap for m...\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "GROUNDED PROMPT:\n",
            "  Response: I don't have information about that in the provided documents\n",
            "\n",
            "The provided context states: \"The Apollo 11 mission landed on the Moon on July 20, 1969. Neil Armstrong and Buzz Aldrin walked on the lun...\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "NLI ANALYSIS:\n",
            "  Ungrounded: 15 claims, 15 potentially ungrounded\n",
            "  Grounded:   3 claims, 1 potentially ungrounded\n",
            "  ✓ Grounded prompt produced more faithful response\n",
            "\n",
            "═════════════════════════════════════════════════════════════════\n",
            "[Partial Information]\n",
            "═════════════════════════════════════════════════════════════════\n",
            "Context: Python 3.12 was released in October 2023. It includes performance improvements a...\n",
            "Question: What are all the new features in Python 3.12 and who developed them?\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "UNGROUNDED PROMPT:\n",
            "  Response: Here's a precise answer based on the **official Python 3.12 release notes** (https://docs.python.org/3.12/whatsnew/3.12.html) and community knowledge:\n",
            "\n",
            "---\n",
            "\n",
            "### 🔑 Key New Features in Python 3.12 (Octo...\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "GROUNDED PROMPT:\n",
            "  Response: I don't have information about that in the provided documents\n",
            "\n",
            "The context states: \"Python 3.12 was released in October 2023. It includes performance improvements and better error messages.\" \n",
            "\n",
            "This co...\n",
            "\n",
            "─────────────────────────────────────────────────────────────────\n",
            "NLI ANALYSIS:\n",
            "  Ungrounded: 40 claims, 39 potentially ungrounded\n",
            "  Grounded:   5 claims, 3 potentially ungrounded\n",
            "  ✓ Grounded prompt produced more faithful response\n"
          ]
        }
      ],
      "source": [
        "# Grounded Prompting: Comparison Demo\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"GROUNDED vs UNGROUNDED: Side-by-Side Comparison\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "if not ollama_ready:\n",
        "    print(\"\\n⚠ Ollama not running - skipping live demo\")\n",
        "else:\n",
        "    # Test cases designed to tempt hallucination\n",
        "    grounding_tests = [\n",
        "        {\n",
        "            \"name\": \"Question Beyond Context\",\n",
        "            \"context\": \"TechCorp was founded in 2015 in Austin, Texas. The company has 500 employees.\",\n",
        "            \"question\": \"Who is the CEO of TechCorp and what is the company's revenue?\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Tempting Fabrication\",\n",
        "            \"context\": \"The Apollo 11 mission landed on the Moon on July 20, 1969. Neil Armstrong and Buzz Aldrin walked on the lunar surface.\",\n",
        "            \"question\": \"What did Neil Armstrong say when he first stepped on the Moon, and how long did the moonwalk last?\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"Partial Information\",\n",
        "            \"context\": \"Python 3.12 was released in October 2023. It includes performance improvements and better error messages.\",\n",
        "            \"question\": \"What are all the new features in Python 3.12 and who developed them?\"\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    for test in grounding_tests:\n",
        "        print(f\"\\n{'═'*65}\")\n",
        "        print(f\"[{test['name']}]\")\n",
        "        print(f\"{'═'*65}\")\n",
        "        print(f\"Context: {test['context'][:80]}...\")\n",
        "        print(f\"Question: {test['question']}\")\n",
        "        \n",
        "        # Generate with ungrounded prompt\n",
        "        print(f\"\\n{'─'*65}\")\n",
        "        print(\"UNGROUNDED PROMPT:\")\n",
        "        ungrounded_prompt = build_ungrounded_prompt(test['question'], test['context'])\n",
        "        ungrounded_response = clean_response(ollama_generate(ungrounded_prompt, temperature=0.7))\n",
        "        ungrounded_display = ungrounded_response[:200] + \"...\" if len(ungrounded_response) > 200 else ungrounded_response\n",
        "        print(f\"  Response: {ungrounded_display}\")\n",
        "        \n",
        "        # Generate with grounded prompt\n",
        "        print(f\"\\n{'─'*65}\")\n",
        "        print(\"GROUNDED PROMPT:\")\n",
        "        grounded_prompt = build_grounded_prompt(test['question'], test['context'])\n",
        "        grounded_response = clean_response(ollama_generate(grounded_prompt, temperature=0.7))\n",
        "        grounded_display = grounded_response[:200] + \"...\" if len(grounded_response) > 200 else grounded_response\n",
        "        print(f\"  Response: {grounded_display}\")\n",
        "        \n",
        "        # Analyze with NLI if available\n",
        "        if nli_available:\n",
        "            print(f\"\\n{'─'*65}\")\n",
        "            print(\"NLI ANALYSIS:\")\n",
        "            \n",
        "            ungrounded_result = nli_detect_hallucination(test['context'], ungrounded_response)\n",
        "            grounded_result = nli_detect_hallucination(test['context'], grounded_response)\n",
        "            \n",
        "            ungrounded_issues = len([c for c in ungrounded_result['all_claims'] if c['label'] != 'entailment'])\n",
        "            grounded_issues = len([c for c in grounded_result['all_claims'] if c['label'] != 'entailment'])\n",
        "            \n",
        "            print(f\"  Ungrounded: {len(ungrounded_result['all_claims'])} claims, {ungrounded_issues} potentially ungrounded\")\n",
        "            print(f\"  Grounded:   {len(grounded_result['all_claims'])} claims, {grounded_issues} potentially ungrounded\")\n",
        "            \n",
        "            if grounded_issues < ungrounded_issues:\n",
        "                print(\"  ✓ Grounded prompt produced more faithful response\")\n",
        "            elif grounded_issues == ungrounded_issues:\n",
        "                print(\"  ○ Similar faithfulness (model may have behaved well in both)\")\n",
        "            else:\n",
        "                print(\"  ⚠ Unexpected: grounded prompt had more issues\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "HALLUCINATION DETECTION: Complete Summary\n",
            "======================================================================\n",
            "\n",
            "┌───────────────────┬─────────────┬──────────────┬──────────────────┐\n",
            "│ Method            │ Granularity │ Stages       │ Use Case         │\n",
            "├───────────────────┼─────────────┼──────────────┼──────────────────┤\n",
            "│ LettuceDetect     │ Token       │ 1 (detect)   │ RAG pipelines    │\n",
            "│ NLI-Based         │ Sentence    │ 1 (classify) │ Contradiction    │\n",
            "│ Self-Consistency  │ Response    │ N samples    │ Uncertainty      │\n",
            "│ HaluGate-Style    │ Claim+NLI   │ 3 (S→D→E)    │ Conditional RAG  │\n",
            "│ Grounded Prompt   │ Prompt      │ 0 (prevent)  │ All pipelines    │\n",
            "│ LLM-as-Judge      │ Semantic    │ 1 (judge)    │ High-stakes      │\n",
            "└───────────────────┴─────────────┴──────────────┴──────────────────┘\n",
            "\n",
            "PIPELINE ARCHITECTURE (Demo 4):\n",
            "\n",
            "  ┌─────────────────────────────────────────────────────────────────────┐\n",
            "  │   [User Query] ──► [Sentinel] ──► Skip? ──► Direct LLM             │\n",
            "  │                         │                                           │\n",
            "  │                         ▼ (needs fact-check)                        │\n",
            "  │                                                                     │\n",
            "  │   [LLM Response] ──► [Detector/NLI] ──► No issue? ──► ✓            │\n",
            "  │                         │                                           │\n",
            "  │                         ▼ (contradiction/neutral)                   │\n",
            "  │                                                                     │\n",
            "  │   [Explainer] ──► Flag/Block/Regenerate                            │\n",
            "  └─────────────────────────────────────────────────────────────────────┘\n",
            "\n",
            "MODELS USED:\n",
            "\n",
            "  Demo 1: KRLabsOrg/lettuce-detect-base-modernbert-en-v1\n",
            "  Demo 2: cross-encoder/nli-deberta-v3-base\n",
            "  Demo 3: (uses LLM directly)\n",
            "  Demo 4: llm-semantic-router/halugate-sentinel\n",
            "          cross-encoder/nli-deberta-v3-base\n",
            "  Demo 5: (prompt engineering, no model)\n",
            "\n",
            "\n",
            "======================================================================\n",
            "ALL DEMOS STATUS\n",
            "======================================================================\n",
            "  ✓ Ollama (qwen3:4b)\n",
            "  ✗ LettuceDetect\n",
            "  ✓ NLI Model\n",
            "  ✓ HaluGate (3 models)\n"
          ]
        }
      ],
      "source": [
        "# Final Summary: All Hallucination Detection Methods\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"HALLUCINATION DETECTION: Complete Summary\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "┌───────────────────┬─────────────┬──────────────┬──────────────────┐\n",
        "│ Method            │ Granularity │ Stages       │ Use Case         │\n",
        "├───────────────────┼─────────────┼──────────────┼──────────────────┤\n",
        "│ LettuceDetect     │ Token       │ 1 (detect)   │ RAG pipelines    │\n",
        "│ NLI-Based         │ Sentence    │ 1 (classify) │ Contradiction    │\n",
        "│ Self-Consistency  │ Response    │ N samples    │ Uncertainty      │\n",
        "│ HaluGate-Style    │ Claim+NLI   │ 3 (S→D→E)    │ Conditional RAG  │\n",
        "│ Grounded Prompt   │ Prompt      │ 0 (prevent)  │ All pipelines    │\n",
        "│ LLM-as-Judge      │ Semantic    │ 1 (judge)    │ High-stakes      │\n",
        "└───────────────────┴─────────────┴──────────────┴──────────────────┘\n",
        "\n",
        "PIPELINE ARCHITECTURE (Demo 4):\n",
        "\n",
        "  ┌─────────────────────────────────────────────────────────────────────┐\n",
        "  │   [User Query] ──► [Sentinel] ──► Skip? ──► Direct LLM             │\n",
        "  │                         │                                           │\n",
        "  │                         ▼ (needs fact-check)                        │\n",
        "  │                                                                     │\n",
        "  │   [LLM Response] ──► [Detector/NLI] ──► No issue? ──► ✓            │\n",
        "  │                         │                                           │\n",
        "  │                         ▼ (contradiction/neutral)                   │\n",
        "  │                                                                     │\n",
        "  │   [Explainer] ──► Flag/Block/Regenerate                            │\n",
        "  └─────────────────────────────────────────────────────────────────────┘\n",
        "\n",
        "MODELS USED:\n",
        "\n",
        "  Demo 1: KRLabsOrg/lettuce-detect-base-modernbert-en-v1\n",
        "  Demo 2: cross-encoder/nli-deberta-v3-base\n",
        "  Demo 3: (uses LLM directly)\n",
        "  Demo 4: llm-semantic-router/halugate-sentinel\n",
        "          cross-encoder/nli-deberta-v3-base\n",
        "  Demo 5: (prompt engineering, no model)\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ALL DEMOS STATUS\")\n",
        "print(\"=\"*70)\n",
        "all_status = [\n",
        "    (\"Ollama (qwen3:4b)\", \"✓\" if ollama_ready else \"✗\"),\n",
        "    (\"LettuceDetect\", \"✓\" if lettucedetect_available else \"✗\"),\n",
        "    (\"NLI Model\", \"✓\" if nli_available else \"✗\"),\n",
        "    (\"HaluGate (3 models)\", \"✓\" if halugate_available else \"✗\"),\n",
        "]\n",
        "for name, stat in all_status:\n",
        "    print(f\"  {stat} {name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "guardrails-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
