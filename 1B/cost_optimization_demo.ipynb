{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Cost Optimization Demo\n",
    "\n",
    "Demos aligned with Part 1B Section 3: Cost Optimization Beyond Caching\n",
    "\n",
    "1. **Routing Economics** - Calculate savings from intelligent model routing\n",
    "2. **LiteLLM** - The LLM Operations Layer (vendor-agnostic gateway)\n",
    "3. **Semantic Router** - Intent-based routing with local embeddings\n",
    "4. **Local Router Demo** - Ollama-based routing with keyword/length heuristics\n",
    "5. **Semantic Caching** - GPTCache for meaning-based caching\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "~70% of production traffic is simple enough for the smallest capable model.\n",
    "Route intelligently → 50-85% cost reduction.\n",
    "\n",
    "## Architecture ( Short Version)\n",
    "\n",
    "```\n",
    "Incoming Query → Semantic Router (local, ~5ms) → Route Config → LiteLLM Gateway → Providers\n",
    "```\n",
    "\n",
    "## Setup\n",
    "\n",
    "```bash\n",
    "pip install litellm semantic-router sentence-transformers gptcache torch\n",
    "```\n",
    "\n",
    "**Ollama:** `ollama pull qwen3:4b && ollama pull llama3.2:1b`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[92m[INFO]\u001b[0m Ollama is running\n",
      "2026-01-03 03:07:45,875 - 8495705152 - 1463702200.py-1463702200:30 - INFO: \u001b[92m[INFO]\u001b[0m Ollama is running\n",
      "\u001b[92m[INFO]\u001b[0m Strong model: qwen3:4b\n",
      "2026-01-03 03:07:45,876 - 8495705152 - 1463702200.py-1463702200:45 - INFO: \u001b[92m[INFO]\u001b[0m Strong model: qwen3:4b\n",
      "\u001b[92m[INFO]\u001b[0m Weak model: llama3.2:1b\n",
      "2026-01-03 03:07:45,877 - 8495705152 - 1463702200.py-1463702200:46 - INFO: \u001b[92m[INFO]\u001b[0m Weak model: llama3.2:1b\n",
      "\u001b[92m[INFO]\u001b[0m Helper functions ready\n",
      "2026-01-03 03:07:45,877 - 8495705152 - 1463702200.py-1463702200:69 - INFO: \u001b[92m[INFO]\u001b[0m Helper functions ready\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available models:\n",
      "NAME           ID              SIZE      MODIFIED   \n",
      "qwen3:4b       359d7dd4bcda    2.5 GB    2 days ago    \n",
      "llama3.2:1b    baf6a787fdff    1.3 GB    2 days ago    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup: Logging, environment checks, and Ollama client\n",
    "import subprocess\n",
    "import logging\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Color-coded logging\n",
    "class ColoredFormatter(logging.Formatter):\n",
    "    COLORS = {'DEBUG': '\\033[90m', 'INFO': '\\033[92m', 'WARNING': '\\033[93m', 'ERROR': '\\033[91m', 'RESET': '\\033[0m'}\n",
    "    def format(self, record):\n",
    "        color = self.COLORS.get(record.levelname, self.COLORS['RESET'])\n",
    "        record.msg = f\"{color}[{record.levelname}]{self.COLORS['RESET']} {record.msg}\"\n",
    "        return super().format(record)\n",
    "\n",
    "logger = logging.getLogger(\"cost_optimization_demo\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(ColoredFormatter('%(message)s'))\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# Check Ollama and available models\n",
    "def check_ollama():\n",
    "    try:\n",
    "        result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True, timeout=5)\n",
    "        if result.returncode == 0:\n",
    "            logger.info(\"Ollama is running\")\n",
    "            print(f\"\\nAvailable models:\\n{result.stdout}\")\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Ollama check failed: {e}\")\n",
    "    return False\n",
    "\n",
    "ollama_ready = check_ollama()\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Model configuration for routing demo\n",
    "STRONG_MODEL = \"qwen3:4b\"    # More capable, slower\n",
    "WEAK_MODEL = \"llama3.2:1b\"   # Faster, cheaper, less capable\n",
    "\n",
    "if ollama_ready:\n",
    "    logger.info(f\"Strong model: {STRONG_MODEL}\")\n",
    "    logger.info(f\"Weak model: {WEAK_MODEL}\")\n",
    "\n",
    "# Ollama helper functions\n",
    "def ollama_generate(prompt: str, model: str, temperature: float = 0.7) -> Tuple[str, float]:\n",
    "    \"\"\"Generate response from Ollama, return (response, latency_ms).\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    response = requests.post(\n",
    "        f\"{OLLAMA_URL}/api/generate\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": temperature}\n",
    "        },\n",
    "        timeout=120\n",
    "    )\n",
    "    latency_ms = (time.perf_counter() - start) * 1000\n",
    "    return response.json().get(\"response\", \"\"), latency_ms\n",
    "\n",
    "def clean_response(text: str) -> str:\n",
    "    \"\"\"Remove thinking tags from qwen3 responses.\"\"\"\n",
    "    return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
    "\n",
    "logger.info(\"Helper functions ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Demo 1: Routing Economics\n",
    "\n",
    "Calculate potential savings from intelligent model routing vs. using a single model for all requests.\n",
    "\n",
    "**Key insight:** Most production traffic (~70%) is simple enough for small models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "ROUTING ECONOMICS: Cost Savings Calculator\n",
      "=================================================================\n",
      "\n",
      "[Scenario 1: Customer Support - 100K requests/day]\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Daily requests:                 100,000\n",
      "  Daily cost (no routing):   €   3,000.00\n",
      "  Daily cost (with routing): €     441.00\n",
      "  Daily savings:             €   2,559.00\n",
      "  Monthly savings:           €  76,770.00\n",
      "  Savings:                           85.3%\n",
      "\n",
      "[Scenario 2: RAG Q&A - 50K requests/day]\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Daily requests:                  50,000\n",
      "  Daily cost (no routing):   €   2,250.00\n",
      "  Daily cost (with routing): €     373.50\n",
      "  Monthly savings:           €  56,295.00\n",
      "  Savings:                           83.4%\n"
     ]
    }
   ],
   "source": [
    "# Routing Economics: Cost Calculator\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"ROUTING ECONOMICS: Cost Savings Calculator\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "def calculate_routing_savings(\n",
    "    daily_requests: int,\n",
    "    complexity_distribution: dict,  # {\"simple\": 0.7, \"standard\": 0.2, \"complex\": 0.1}\n",
    "    model_costs: dict,              # {\"simple\": 0.0001, \"standard\": 0.001, \"complex\": 0.01}\n",
    "    frontier_cost: float = 0.01,    # Cost per 1K tokens if using frontier for everything\n",
    "    tokens_per_request: int = 2000\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate savings from intelligent routing vs. using frontier model for all.\n",
    "    \n",
    "    The key insight: ~70% of production traffic is simple enough for\n",
    "    the smallest capable model.\n",
    "    \"\"\"\n",
    "    # Cost without routing (frontier for everything)\n",
    "    daily_tokens = daily_requests * tokens_per_request\n",
    "    daily_frontier_cost = (daily_tokens / 1000) * frontier_cost\n",
    "    \n",
    "    # Cost with routing\n",
    "    daily_routed_cost = 0\n",
    "    for complexity, fraction in complexity_distribution.items():\n",
    "        tier_requests = daily_requests * fraction\n",
    "        tier_tokens = tier_requests * tokens_per_request\n",
    "        tier_cost = (tier_tokens / 1000) * model_costs[complexity]\n",
    "        daily_routed_cost += tier_cost\n",
    "    \n",
    "    daily_savings = daily_frontier_cost - daily_routed_cost\n",
    "    \n",
    "    return {\n",
    "        'daily_frontier_cost': round(daily_frontier_cost, 2),\n",
    "        'daily_routed_cost': round(daily_routed_cost, 2),\n",
    "        'daily_savings': round(daily_savings, 2),\n",
    "        'monthly_savings': round(daily_savings * 30, 2),\n",
    "        'savings_percent': round((daily_savings / daily_frontier_cost) * 100, 1)\n",
    "    }\n",
    "\n",
    "# Scenario 1: Customer support system with 100K daily queries\n",
    "print(\"\\n[Scenario 1: Customer Support - 100K requests/day]\")\n",
    "print(\"─\"*65)\n",
    "\n",
    "support_routing = calculate_routing_savings(\n",
    "    daily_requests=100000,\n",
    "    complexity_distribution={\n",
    "        \"simple\": 0.70,   # FAQ, status checks, simple questions\n",
    "        \"standard\": 0.20, # Explanations, multi-step answers\n",
    "        \"complex\": 0.10   # Analysis, debugging, complaints\n",
    "    },\n",
    "    model_costs={\n",
    "        \"simple\": 0.00015,   # GPT-4o-mini / Llama 8B\n",
    "        \"standard\": 0.003,   # Claude Sonnet / GPT-4o\n",
    "        \"complex\": 0.015     # Claude Opus\n",
    "    },\n",
    "    frontier_cost=0.015,\n",
    "    tokens_per_request=2000\n",
    ")\n",
    "\n",
    "print(f\"  Daily requests:            {100000:>12,}\")\n",
    "print(f\"  Daily cost (no routing):   €{support_routing['daily_frontier_cost']:>11,.2f}\")\n",
    "print(f\"  Daily cost (with routing): €{support_routing['daily_routed_cost']:>11,.2f}\")\n",
    "print(f\"  Daily savings:             €{support_routing['daily_savings']:>11,.2f}\")\n",
    "print(f\"  Monthly savings:           €{support_routing['monthly_savings']:>11,.2f}\")\n",
    "print(f\"  Savings:                   {support_routing['savings_percent']:>12}%\")\n",
    "\n",
    "# Scenario 2: RAG Q&A system\n",
    "print(\"\\n[Scenario 2: RAG Q&A - 50K requests/day]\")\n",
    "print(\"─\"*65)\n",
    "\n",
    "rag_routing = calculate_routing_savings(\n",
    "    daily_requests=50000,\n",
    "    complexity_distribution={\n",
    "        \"simple\": 0.60,   # Direct answers from context\n",
    "        \"standard\": 0.30, # Synthesis across documents\n",
    "        \"complex\": 0.10   # Complex reasoning\n",
    "    },\n",
    "    model_costs={\n",
    "        \"simple\": 0.00015,\n",
    "        \"standard\": 0.003,\n",
    "        \"complex\": 0.015\n",
    "    },\n",
    "    frontier_cost=0.015,\n",
    "    tokens_per_request=3000  # RAG uses more context\n",
    ")\n",
    "\n",
    "print(f\"  Daily requests:            {50000:>12,}\")\n",
    "print(f\"  Daily cost (no routing):   €{rag_routing['daily_frontier_cost']:>11,.2f}\")\n",
    "print(f\"  Daily cost (with routing): €{rag_routing['daily_routed_cost']:>11,.2f}\")\n",
    "print(f\"  Monthly savings:           €{rag_routing['monthly_savings']:>11,.2f}\")\n",
    "print(f\"  Savings:                   {rag_routing['savings_percent']:>12}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Demo 2: LiteLLM - The LLM Operations Layer\n",
    "\n",
    "LiteLLM is a **vendor-agnostic gateway** unifying 100+ providers through a single OpenAI-compatible API.\n",
    "\n",
    "| Capability | What It Does |\n",
    "|------------|--------------|\n",
    "| Unified API | Same code for OpenAI, Anthropic, Ollama, vLLM, etc. |\n",
    "| Fallbacks | Auto-retry with different providers |\n",
    "| Caching | Redis/Qdrant semantic caching built-in |\n",
    "| PII Masking | Presidio integration for GDPR |\n",
    "| Cost Tracking | Per-key budgets and alerts |\n",
    "| Observability | Langfuse, Datadog, Prometheus |\n",
    "\n",
    "**Key insight:** LiteLLM handles infrastructure; Semantic Router handles routing logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "LITELLM: Vendor-Agnostic LLM Gateway\n",
      "=================================================================\n",
      "\n",
      "✓ LiteLLM installed\n",
      "\n",
      "ARCHITECTURE:\n",
      "─────────────────────────────────────────────────────────────────\n",
      "\n",
      "  ┌──────────────┐     ┌──────────────┐     ┌──────────────┐\n",
      "  │   Your App   │     │   Your App   │     │   Your App   │\n",
      "  │  (Service A) │     │  (Service B) │     │  (Service C) │\n",
      "  └──────┬───────┘     └──────┬───────┘     └──────┬───────┘\n",
      "         │                    │                    │\n",
      "         └────────────────────┼────────────────────┘\n",
      "                              │\n",
      "                              ▼\n",
      "                    ┌──────────────────┐\n",
      "                    │  LiteLLM Proxy   │◄─── Virtual Keys\n",
      "                    │  (Port 4000)     │◄─── Routing Config\n",
      "                    └────────┬─────────┘◄─── Budget Rules\n",
      "                             │\n",
      "              ┌──────────────┼──────────────┐\n",
      "              ▼              ▼              ▼\n",
      "         ┌────────┐    ┌────────┐    ┌────────┐\n",
      "         │ Ollama │    │ Claude │    │ GPT-4  │\n",
      "         │ (local)│    │ (API)  │    │ (API)  │\n",
      "         └────────┘    └────────┘    └────────┘\n",
      "\n",
      "CORE CAPABILITIES (all open source):\n",
      "─────────────────────────────────────────────────────────────────\n",
      "\n",
      "  1. UNIFIED API: Same code for 100+ providers\n",
      "\n",
      "     from litellm import completion\n",
      "\n",
      "     # Just change the model string\n",
      "     completion(model=\"gpt-4o\", messages=[...])\n",
      "     completion(model=\"claude-3-sonnet\", messages=[...])\n",
      "     completion(model=\"ollama/llama3.2\", messages=[...])\n",
      "\n",
      "  2. FALLBACKS: Auto-retry with different providers\n",
      "\n",
      "     model_list = [\n",
      "         {\"model_name\": \"gpt-4\", \"litellm_params\": {\"model\": \"gpt-4\"}},\n",
      "         {\"model_name\": \"gpt-4\", \"litellm_params\": {\"model\": \"azure/gpt-4\"}},\n",
      "         {\"model_name\": \"gpt-4\", \"litellm_params\": {\"model\": \"ollama/llama3\"}},\n",
      "     ]\n",
      "\n",
      "  3. CACHING: Redis or Qdrant semantic caching\n",
      "  4. PII MASKING: Presidio integration (GDPR)\n",
      "  5. BUDGETS: Per-key cost limits and alerts\n",
      "  6. OBSERVABILITY: Langfuse, Datadog, Prometheus\n",
      "\n",
      "WHEN TO USE:\n",
      "  ✓ Multiple providers (cloud + local)\n",
      "  ✓ Need fallbacks for reliability\n",
      "  ✓ Cost tracking across teams\n",
      "  ✓ Self-hosted (data sovereignty)\n",
      "  ✗ Single provider, prototype only\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LiteLLM: The LLM Operations Layer\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"LITELLM: Vendor-Agnostic LLM Gateway\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "litellm_available = False\n",
    "\n",
    "try:\n",
    "    import litellm\n",
    "    litellm_available = True\n",
    "    print(f\"\\n✓ LiteLLM installed\")\n",
    "except ImportError:\n",
    "    print(\"\\n⚠ LiteLLM not installed\")\n",
    "    print(\"  pip install litellm\")\n",
    "\n",
    "print(\"\"\"\n",
    "ARCHITECTURE:\n",
    "─────────────────────────────────────────────────────────────────\n",
    "\n",
    "  ┌──────────────┐     ┌──────────────┐     ┌──────────────┐\n",
    "  │   Your App   │     │   Your App   │     │   Your App   │\n",
    "  │  (Service A) │     │  (Service B) │     │  (Service C) │\n",
    "  └──────┬───────┘     └──────┬───────┘     └──────┬───────┘\n",
    "         │                    │                    │\n",
    "         └────────────────────┼────────────────────┘\n",
    "                              │\n",
    "                              ▼\n",
    "                    ┌──────────────────┐\n",
    "                    │  LiteLLM Proxy   │◄─── Virtual Keys\n",
    "                    │  (Port 4000)     │◄─── Routing Config\n",
    "                    └────────┬─────────┘◄─── Budget Rules\n",
    "                             │\n",
    "              ┌──────────────┼──────────────┐\n",
    "              ▼              ▼              ▼\n",
    "         ┌────────┐    ┌────────┐    ┌────────┐\n",
    "         │ Ollama │    │ Claude │    │ GPT-4  │\n",
    "         │ (local)│    │ (API)  │    │ (API)  │\n",
    "         └────────┘    └────────┘    └────────┘\n",
    "\n",
    "CORE CAPABILITIES (all open source):\n",
    "─────────────────────────────────────────────────────────────────\n",
    "\n",
    "  1. UNIFIED API: Same code for 100+ providers\n",
    "     \n",
    "     from litellm import completion\n",
    "     \n",
    "     # Just change the model string\n",
    "     completion(model=\"gpt-4o\", messages=[...])\n",
    "     completion(model=\"claude-3-sonnet\", messages=[...])\n",
    "     completion(model=\"ollama/llama3.2\", messages=[...])\n",
    "\n",
    "  2. FALLBACKS: Auto-retry with different providers\n",
    "     \n",
    "     model_list = [\n",
    "         {\"model_name\": \"gpt-4\", \"litellm_params\": {\"model\": \"gpt-4\"}},\n",
    "         {\"model_name\": \"gpt-4\", \"litellm_params\": {\"model\": \"azure/gpt-4\"}},\n",
    "         {\"model_name\": \"gpt-4\", \"litellm_params\": {\"model\": \"ollama/llama3\"}},\n",
    "     ]\n",
    "\n",
    "  3. CACHING: Redis or Qdrant semantic caching\n",
    "  4. PII MASKING: Presidio integration (GDPR)\n",
    "  5. BUDGETS: Per-key cost limits and alerts\n",
    "  6. OBSERVABILITY: Langfuse, Datadog, Prometheus\n",
    "\n",
    "WHEN TO USE:\n",
    "  ✓ Multiple providers (cloud + local)\n",
    "  ✓ Need fallbacks for reliability\n",
    "  ✓ Cost tracking across teams\n",
    "  ✓ Self-hosted (data sovereignty)\n",
    "  ✗ Single provider, prototype only\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "LITELLM: Functional Demos with Ollama\n",
      "=================================================================\n",
      "\n",
      "[Demo 1: Basic Completion]\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  ✓ Model: ollama/llama3.2:1b\n",
      "  ✓ Response: 4.\n",
      "  ✓ Latency: 684ms\n",
      "\n",
      "[Demo 2: Model Comparison]\n",
      "─────────────────────────────────────────────────────────────────\n",
      "\n",
      "  ollama/llama3.2:1b:\n",
      "    Latency: 425ms\n",
      "    Response: Recursion is a programming technique where a function calls itself repeatedly until it reaches a bas...\n",
      "\n",
      "  ollama/qwen3:4b:\n",
      "    Latency: 53706ms\n",
      "    Response: Recursion is a programming technique where a function calls itself to solve a problem by breaking it...\n",
      "\n",
      "\n",
      "[Demo 3: Fallback Chain]\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Trying models in order:\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "  ✗ ollama/nonexistent-model: litellm.APIConnectionError: OllamaException - {\"er...\n",
      "\n",
      "  ✓ Success with: ollama/llama3.2:1b\n",
      "  ✓ Response: Hello! How can I assist you today?\n",
      "\n",
      "=================================================================\n",
      "LITELLM KEY TAKEAWAYS\n",
      "=================================================================\n",
      "\n",
      "  ✓ Unified API: model=\"ollama/llama3.2:1b\" or model=\"gpt-4o\"\n",
      "  ✓ Fallbacks: Try models in order until one works\n",
      "  ✓ Same code works for 100+ providers\n",
      "  ✓ Production: Run as proxy server with config.yaml\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# LiteLLM: Functional Demos with Ollama\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"LITELLM: Functional Demos with Ollama\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if not litellm_available:\n",
    "    print(\"\\n⚠ LiteLLM not installed: pip install litellm\")\n",
    "elif not ollama_ready:\n",
    "    print(\"\\n⚠ Ollama not running\")\n",
    "else:\n",
    "    import litellm\n",
    "    import time\n",
    "    \n",
    "    # Suppress verbose logging\n",
    "    litellm.set_verbose = False\n",
    "    \n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    # Demo 1: Basic Completion\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    print(\"\\n[Demo 1: Basic Completion]\")\n",
    "    print(\"─\"*65)\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": \"What is 2+2? Answer in one word.\"}]\n",
    "    \n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "        response = litellm.completion(\n",
    "            model=\"ollama/llama3.2:1b\",\n",
    "            messages=messages,\n",
    "            api_base=\"http://localhost:11434\"\n",
    "        )\n",
    "        latency = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        print(f\"  ✓ Model: {response.model}\")\n",
    "        print(f\"  ✓ Response: {response.choices[0].message.content.strip()[:80]}\")\n",
    "        print(f\"  ✓ Latency: {latency:.0f}ms\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    # Demo 2: Model Comparison (Same prompt, different models)\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    print(\"\\n[Demo 2: Model Comparison]\")\n",
    "    print(\"─\"*65)\n",
    "    \n",
    "    models_to_test = [\n",
    "        f\"ollama/{WEAK_MODEL}\",\n",
    "        f\"ollama/{STRONG_MODEL}\",\n",
    "    ]\n",
    "    \n",
    "    test_prompt = [{\"role\": \"user\", \"content\": \"Explain recursion in one sentence.\"}]\n",
    "    \n",
    "    for model in models_to_test:\n",
    "        try:\n",
    "            start = time.perf_counter()\n",
    "            resp = litellm.completion(\n",
    "                model=model,\n",
    "                messages=test_prompt,\n",
    "                api_base=\"http://localhost:11434\"\n",
    "            )\n",
    "            latency = (time.perf_counter() - start) * 1000\n",
    "            \n",
    "            answer = resp.choices[0].message.content.strip()\n",
    "            # Clean qwen thinking tags\n",
    "            answer = re.sub(r'<think>.*?</think>', '', answer, flags=re.DOTALL).strip()\n",
    "            \n",
    "            print(f\"\\n  {model}:\")\n",
    "            print(f\"    Latency: {latency:.0f}ms\")\n",
    "            print(f\"    Response: {answer[:100]}{'...' if len(answer) > 100 else ''}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n  {model}: Error - {e}\")\n",
    "    \n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    # Demo 3: Fallback Chain (try models in order)\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    print(\"\\n\\n[Demo 3: Fallback Chain]\")\n",
    "    print(\"─\"*65)\n",
    "    \n",
    "    fallback_models = [\n",
    "        f\"ollama/nonexistent-model\",  # Will fail\n",
    "        f\"ollama/{WEAK_MODEL}\",       # Fallback\n",
    "    ]\n",
    "    \n",
    "    def completion_with_fallback(messages, models):\n",
    "        \"\"\"Try models in order until one works.\"\"\"\n",
    "        for model in models:\n",
    "            try:\n",
    "                resp = litellm.completion(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    api_base=\"http://localhost:11434\",\n",
    "                    timeout=10\n",
    "                )\n",
    "                return model, resp\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ {model}: {str(e)[:50]}...\")\n",
    "                continue\n",
    "        return None, None\n",
    "    \n",
    "    print(\"  Trying models in order:\")\n",
    "    model_used, resp = completion_with_fallback(\n",
    "        [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "        fallback_models\n",
    "    )\n",
    "    \n",
    "    if resp:\n",
    "        print(f\"\\n  ✓ Success with: {model_used}\")\n",
    "        print(f\"  ✓ Response: {resp.choices[0].message.content.strip()[:60]}\")\n",
    "    else:\n",
    "        print(\"\\n  ✗ All models failed\")\n",
    "    \n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    # Summary\n",
    "    # ─────────────────────────────────────────────────────────────────\n",
    "    print(\"\\n\" + \"=\"*65)\n",
    "    print(\"LITELLM KEY TAKEAWAYS\")\n",
    "    print(\"=\"*65)\n",
    "    print(\"\"\"\n",
    "  ✓ Unified API: model=\"ollama/llama3.2:1b\" or model=\"gpt-4o\"\n",
    "  ✓ Fallbacks: Try models in order until one works\n",
    "  ✓ Same code works for 100+ providers\n",
    "  ✓ Production: Run as proxy server with config.yaml\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Demo 3: Semantic Router - Intent-Based Routing\n",
    "\n",
    "**Semantic Router** uses embeddings to match queries against predefined route examples.\n",
    "\n",
    "| Feature | Semantic Router | RouteLLM |\n",
    "|---------|-----------------|----------|\n",
    "| Routing logic | Defined (examples) | Learned (preferences) |\n",
    "| Training needed | No | Yes |\n",
    "| Explainability | High | Low |\n",
    "| Local execution | ✓ (sentence-transformers) | ✗ (needs OpenAI) |\n",
    "| Output | Intent/Category | Model selection |\n",
    "\n",
    "**Key advantage:** Runs locally with ~5ms latency. No API calls for routing decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "SEMANTIC ROUTER: Intent-Based Classification\n",
      "=================================================================\n",
      "\n",
      "Loading local encoder (HuggingFace)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m2026-01-03 03:08:42 WARNING semantic_router No index provided. Using default LocalIndex.\u001b[0m\n",
      "\u001b[33m2026-01-03 03:08:42 WARNING semantic_router No config is written for LocalIndex.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Semantic Router ready\n",
      "  Routes: billing, technical, sales\n",
      "  Encoder: HuggingFace (local)\n"
     ]
    }
   ],
   "source": [
    "# Semantic Router: Setup with Local Embeddings\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"SEMANTIC ROUTER: Intent-Based Classification\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "semantic_router_available = False\n",
    "sr_router = None\n",
    "\n",
    "try:\n",
    "    from semantic_router import Route\n",
    "    from semantic_router.routers import SemanticRouter\n",
    "    from semantic_router.encoders import HuggingFaceEncoder\n",
    "    \n",
    "    print(\"\\nLoading local encoder (HuggingFace)...\")\n",
    "    encoder = HuggingFaceEncoder()  # Uses local model, no API needed\n",
    "    \n",
    "    # Define routes with example utterances (from md)\n",
    "    billing_route = Route(\n",
    "        name=\"billing\",\n",
    "        utterances=[\n",
    "            \"What's my current balance?\",\n",
    "            \"I want to pay my bill\",\n",
    "            \"Can you explain this charge?\",\n",
    "            \"Update my payment method\",\n",
    "            \"When is my payment due?\",\n",
    "            \"Show me my invoice\",\n",
    "            \"I was overcharged\",\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    technical_route = Route(\n",
    "        name=\"technical\",\n",
    "        utterances=[\n",
    "            \"The app keeps crashing\",\n",
    "            \"I can't log in to my account\",\n",
    "            \"How do I reset my password?\",\n",
    "            \"The feature isn't working\",\n",
    "            \"I'm getting an error message\",\n",
    "            \"My data isn't syncing\",\n",
    "            \"The page won't load\",\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    sales_route = Route(\n",
    "        name=\"sales\",\n",
    "        utterances=[\n",
    "            \"What plans do you offer?\",\n",
    "            \"I want to upgrade my subscription\",\n",
    "            \"Tell me about enterprise pricing\",\n",
    "            \"Compare your plans\",\n",
    "            \"What's included in premium?\",\n",
    "            \"Can I get a demo?\",\n",
    "            \"Pricing for teams?\",\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Create router with auto_sync to build index immediately\n",
    "    sr_router = SemanticRouter(\n",
    "        encoder=encoder,\n",
    "        routes=[billing_route, technical_route, sales_route],\n",
    "        auto_sync=\"local\"  # Automatically sync routes to local index\n",
    "    )\n",
    "    \n",
    "    semantic_router_available = True\n",
    "    print(\"✓ Semantic Router ready\")\n",
    "    print(f\"  Routes: billing, technical, sales\")\n",
    "    print(f\"  Encoder: HuggingFace (local)\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"\\n⚠ Semantic Router not installed: {e}\")\n",
    "    print(\"\\nTo install:\")\n",
    "    print(\"  pip install semantic-router\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Failed to initialize: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "SEMANTIC ROUTER: Intent Classification Demo\n",
      "=================================================================\n",
      "\n",
      "Classification Results:\n",
      "─────────────────────────────────────────────────────────────────\n",
      "\n",
      "  [billing   ] → llama-8b (€0.05/M tokens)\n",
      "  Query: I need to pay my monthly fee\n",
      "\n",
      "  [NONE      ] → llama-8b (€0.05/M tokens)\n",
      "  Query: The app crashes every time I open it\n",
      "\n",
      "  [NONE      ] → llama-8b (€0.05/M tokens)\n",
      "  Query: What are your enterprise pricing options?\n",
      "\n",
      "  [NONE      ] → llama-8b (€0.05/M tokens)\n",
      "  Query: My password reset email never arrived\n",
      "\n",
      "  [NONE      ] → llama-8b (€0.05/M tokens)\n",
      "  Query: Can you explain this $50 charge?\n",
      "\n",
      "  [NONE      ] → llama-8b (€0.05/M tokens)\n",
      "  Query: I'd like to upgrade to the premium plan\n",
      "\n",
      "  [NONE      ] → llama-8b (€0.05/M tokens)\n",
      "  Query: Hello, how are you?\n",
      "\n",
      "═════════════════════════════════════════════════════════════════\n",
      "ROUTE-TO-ACTION MAPPING (from md):\n",
      "═════════════════════════════════════════════════════════════════\n",
      "\n",
      "  Route        Model           Prompt          Tools\n",
      "  ───────────────────────────────────────────────────────────\n",
      "  billing      llama-8b        billing.txt     [balance, pay]\n",
      "  technical    claude-sonnet   support.txt     [kb, ticket]\n",
      "  sales        gpt-4o          sales.txt       [pricing, demo]\n",
      "  default      llama-8b        general.txt     []\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Semantic Router: Classification Demo\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"SEMANTIC ROUTER: Intent Classification Demo\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if not semantic_router_available:\n",
    "    print(\"\\n⚠ Semantic Router not available\")\n",
    "else:\n",
    "    # Test queries - mix of routes and unmatched\n",
    "    sr_test_queries = [\n",
    "        \"I need to pay my monthly fee\",\n",
    "        \"The app crashes every time I open it\",\n",
    "        \"What are your enterprise pricing options?\",\n",
    "        \"My password reset email never arrived\",\n",
    "        \"Can you explain this $50 charge?\",\n",
    "        \"I'd like to upgrade to the premium plan\",\n",
    "        \"Hello, how are you?\",  # Should not match any route\n",
    "    ]\n",
    "    \n",
    "    # Route-to-model mapping (as shown in md)\n",
    "    route_to_model = {\n",
    "        \"billing\": (\"llama-8b\", \"€0.05/M tokens\"),      # Simple lookups\n",
    "        \"technical\": (\"claude-sonnet\", \"€3/M tokens\"),   # Debugging\n",
    "        \"sales\": (\"gpt-4o\", \"€15/M tokens\"),             # Persuasive\n",
    "        None: (\"llama-8b\", \"€0.05/M tokens\")             # Fallback\n",
    "    }\n",
    "    \n",
    "    print(\"\\nClassification Results:\")\n",
    "    print(\"─\"*65)\n",
    "    \n",
    "    for query in sr_test_queries:\n",
    "        result = sr_router(query)\n",
    "        route_name = result.name if result else None\n",
    "        model, cost = route_to_model.get(route_name, route_to_model[None])\n",
    "        \n",
    "        print(f\"\\n  [{route_name or 'NONE':10}] → {model} ({cost})\")\n",
    "        print(f\"  Query: {query[:55]}{'...' if len(query) > 55 else ''}\")\n",
    "    \n",
    "    print(f\"\\n{'═'*65}\")\n",
    "    print(\"ROUTE-TO-ACTION MAPPING (from md):\")\n",
    "    print(f\"{'═'*65}\")\n",
    "    print(\"\"\"\n",
    "  Route        Model           Prompt          Tools\n",
    "  ───────────────────────────────────────────────────────────\n",
    "  billing      llama-8b        billing.txt     [balance, pay]\n",
    "  technical    claude-sonnet   support.txt     [kb, ticket]\n",
    "  sales        gpt-4o          sales.txt       [pricing, demo]\n",
    "  default      llama-8b        general.txt     []\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Demo 4: Local Router with Ollama\n",
    "\n",
    "Demonstrate the full routing flow locally:\n",
    "1. Classify query complexity (rule-based)\n",
    "2. Route to appropriate model (`qwen3:4b` or `llama3.2:1b`)\n",
    "3. Generate response\n",
    "\n",
    "This simulates what happens in production with LiteLLM + cloud models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "LOCAL ROUTER: Rule-Based Classification\n",
      "=================================================================\n",
      "\n",
      "Classification Results:\n",
      "─────────────────────────────────────────────────────────────────\n",
      "\n",
      "  [SIMPLE  ] ⇢ weak\n",
      "  Query: Hello!\n",
      "\n",
      "  [SIMPLE  ] ⇢ weak\n",
      "  Query: What is Python?\n",
      "\n",
      "  [COMPLEX ] → strong\n",
      "  Query: Analyze the trade-offs between microservices and monoli...\n"
     ]
    }
   ],
   "source": [
    "# Local Router: Rule-Based Classifier + Ollama\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"LOCAL ROUTER: Rule-Based Classification\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "@dataclass\n",
    "class RoutingDecision:\n",
    "    \"\"\"Result of routing decision.\"\"\"\n",
    "    query: str\n",
    "    complexity: str      # simple, standard, complex\n",
    "    route_to: str        # weak or strong\n",
    "    confidence: float\n",
    "    reasoning: str\n",
    "\n",
    "def classify_complexity(query: str) -> RoutingDecision:\n",
    "    \"\"\"Rule-based complexity classifier (simplified for demo).\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    word_count = len(query.split())\n",
    "    \n",
    "    # Complexity indicators\n",
    "    complex_keywords = [\"analyze\", \"compare\", \"evaluate\", \"trade-offs\", \"design\", \"architect\", \"debug\"]\n",
    "    simple_keywords = [\"hello\", \"hi\", \"thanks\", \"yes\", \"no\", \"what time\", \"define\"]\n",
    "    \n",
    "    has_complex = any(kw in query_lower for kw in complex_keywords)\n",
    "    has_simple = any(kw in query_lower for kw in simple_keywords)\n",
    "    \n",
    "    if has_complex or word_count > 30:\n",
    "        return RoutingDecision(query, \"complex\", \"strong\", 0.85, \"Complex reasoning required\")\n",
    "    elif has_simple or word_count < 5:\n",
    "        return RoutingDecision(query, \"simple\", \"weak\", 0.90, \"Simple query\")\n",
    "    else:\n",
    "        return RoutingDecision(query, \"standard\", \"strong\", 0.70, \"Standard task\")\n",
    "\n",
    "# Test the classifier\n",
    "test_queries = [\n",
    "    \"Hello!\",\n",
    "    \"What is Python?\",\n",
    "    \"Analyze the trade-offs between microservices and monolithic architecture\",\n",
    "]\n",
    "\n",
    "print(\"\\nClassification Results:\")\n",
    "print(\"─\"*65)\n",
    "for query in test_queries:\n",
    "    decision = classify_complexity(query)\n",
    "    symbol = \"→\" if decision.route_to == \"strong\" else \"⇢\"\n",
    "    print(f\"\\n  [{decision.complexity.upper():8}] {symbol} {decision.route_to}\")\n",
    "    print(f\"  Query: {query[:55]}{'...' if len(query) > 55 else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "LOCAL ROUTER: Live Demo with Ollama\n",
      "=================================================================\n",
      "\n",
      "─────────────────────────────────────────────────────────────────\n",
      "Query: Hi there!\n",
      "  Complexity: SIMPLE\n",
      "  Routed to: llama3.2:1b\n",
      "  Latency: 225ms\n",
      "  Response: Hello! How can I help you today?\n",
      "\n",
      "─────────────────────────────────────────────────────────────────\n",
      "Query: Analyze the trade-offs between SQL and NoSQL databases\n",
      "  Complexity: COMPLEX\n",
      "  Routed to: qwen3:4b\n",
      "  Latency: 57707ms\n",
      "  Response: Here's a clear, balanced analysis of the **core trade-offs between SQL and NoSQL databases**, designed to help you make practical decisions—not just t...\n",
      "\n",
      "═════════════════════════════════════════════════════════════════\n",
      "ROUTING SUMMARY\n",
      "═════════════════════════════════════════════════════════════════\n",
      "  Routed to weak: 1/2\n",
      "  Cost reduction: ~50% of queries to cheaper model\n"
     ]
    }
   ],
   "source": [
    "# Local Router: Live Demo with Ollama\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"LOCAL ROUTER: Live Demo with Ollama\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if not ollama_ready:\n",
    "    print(\"\\n⚠ Ollama not running - skipping live demo\")\n",
    "else:\n",
    "    def route_and_generate(query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Route query to appropriate model and generate response.\"\"\"\n",
    "        decision = classify_complexity(query)\n",
    "        model = STRONG_MODEL if decision.route_to == \"strong\" else WEAK_MODEL\n",
    "        \n",
    "        response, latency_ms = ollama_generate(query, model, temperature=0.7)\n",
    "        response = clean_response(response)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"complexity\": decision.complexity,\n",
    "            \"model_used\": model,\n",
    "            \"response\": response[:150] + \"...\" if len(response) > 150 else response,\n",
    "            \"latency_ms\": latency_ms\n",
    "        }\n",
    "    \n",
    "    # Test queries\n",
    "    routing_tests = [\n",
    "        \"Hi there!\",\n",
    "        \"Analyze the trade-offs between SQL and NoSQL databases\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for query in routing_tests:\n",
    "        print(f\"\\n{'─'*65}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        result = route_and_generate(query)\n",
    "        results.append(result)\n",
    "        print(f\"  Complexity: {result['complexity'].upper()}\")\n",
    "        print(f\"  Routed to: {result['model_used']}\")\n",
    "        print(f\"  Latency: {result['latency_ms']:.0f}ms\")\n",
    "        print(f\"  Response: {result['response']}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'═'*65}\")\n",
    "    print(\"ROUTING SUMMARY\")\n",
    "    print(f\"{'═'*65}\")\n",
    "    weak_count = sum(1 for r in results if r['model_used'] == WEAK_MODEL)\n",
    "    print(f\"  Routed to weak: {weak_count}/{len(results)}\")\n",
    "    print(f\"  Cost reduction: ~{weak_count/len(results)*100:.0f}% of queries to cheaper model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COST OPTIMIZATION: Complete Summary\n",
      "======================================================================\n",
      "\n",
      "┌─────────────────────┬──────────────────┬──────────────────┬─────────────┐\n",
      "│ Method              │ Approach         │ Requirements     │ Savings     │\n",
      "├─────────────────────┼──────────────────┼──────────────────┼─────────────┤\n",
      "│ LiteLLM Gateway     │ Unified API      │ litellm          │ Infra layer │\n",
      "│ Semantic Router     │ Intent matching  │ semantic-router  │ 50-85%      │\n",
      "│ Rule-Based Router   │ Keyword/length   │ None             │ 40-60%      │\n",
      "│ GPTCache            │ Semantic match   │ gptcache         │ 20-40%      │\n",
      "└─────────────────────┴──────────────────┴──────────────────┴─────────────┘\n",
      "\n",
      "PRODUCTION ARCHITECTURE (from md):\n",
      "─────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "    Incoming Query\n",
      "          │\n",
      "          ▼\n",
      "    ┌────────────────────────┐\n",
      "    │   Semantic Router      │  ← Intent classification (local, ~5ms)\n",
      "    │   (local embeddings)   │\n",
      "    └───────────┬────────────┘\n",
      "                │\n",
      "    ┌───────────┴───────────────┐\n",
      "    │                           │\n",
      "    ▼                           ▼\n",
      "  billing → small model    complex → frontier model\n",
      "  technical → mid model    sales → persuasive model\n",
      "                │\n",
      "                ▼\n",
      "    ┌────────────────────────┐\n",
      "    │   LiteLLM Gateway      │  ← Fallbacks, caching, cost tracking\n",
      "    └───────────┬────────────┘\n",
      "                │\n",
      "    ┌───────────┼───────────────┐\n",
      "    ▼           ▼               ▼\n",
      "  Ollama     Claude          GPT-4o\n",
      "\n",
      "DEMO STATUS:\n",
      "─────────────────────────────────────────────────────────────────────────\n",
      "\n",
      "  ✓ LiteLLM\n",
      "  ✓ Semantic Router\n",
      "  ✓ Ollama\n",
      "  ✓ GPTCache\n"
     ]
    }
   ],
   "source": [
    "# Final Summary: Cost Optimization Methods\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COST OPTIMIZATION: Complete Summary\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "┌─────────────────────┬──────────────────┬──────────────────┬─────────────┐\n",
    "│ Method              │ Approach         │ Requirements     │ Savings     │\n",
    "├─────────────────────┼──────────────────┼──────────────────┼─────────────┤\n",
    "│ LiteLLM Gateway     │ Unified API      │ litellm          │ Infra layer │\n",
    "│ Semantic Router     │ Intent matching  │ semantic-router  │ 50-85%      │\n",
    "│ Rule-Based Router   │ Keyword/length   │ None             │ 40-60%      │\n",
    "│ GPTCache            │ Semantic match   │ gptcache         │ 20-40%      │\n",
    "└─────────────────────┴──────────────────┴──────────────────┴─────────────┘\n",
    "\n",
    "PRODUCTION ARCHITECTURE (from md):\n",
    "─────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    Incoming Query\n",
    "          │\n",
    "          ▼\n",
    "    ┌────────────────────────┐\n",
    "    │   Semantic Router      │  ← Intent classification (local, ~5ms)\n",
    "    │   (local embeddings)   │\n",
    "    └───────────┬────────────┘\n",
    "                │\n",
    "    ┌───────────┴───────────────┐\n",
    "    │                           │\n",
    "    ▼                           ▼\n",
    "  billing → small model    complex → frontier model\n",
    "  technical → mid model    sales → persuasive model\n",
    "                │\n",
    "                ▼\n",
    "    ┌────────────────────────┐\n",
    "    │   LiteLLM Gateway      │  ← Fallbacks, caching, cost tracking\n",
    "    └───────────┬────────────┘\n",
    "                │\n",
    "    ┌───────────┼───────────────┐\n",
    "    ▼           ▼               ▼\n",
    "  Ollama     Claude          GPT-4o\n",
    "\n",
    "DEMO STATUS:\n",
    "─────────────────────────────────────────────────────────────────────────\n",
    "\"\"\")\n",
    "\n",
    "# Status checks\n",
    "status = [\n",
    "    (\"LiteLLM\", litellm_available if 'litellm_available' in dir() else False),\n",
    "    (\"Semantic Router\", semantic_router_available if 'semantic_router_available' in dir() else False),\n",
    "    (\"Ollama\", ollama_ready if 'ollama_ready' in dir() else False),\n",
    "    (\"GPTCache\", gptcache_available if 'gptcache_available' in dir() else False),\n",
    "]\n",
    "for name, available in status:\n",
    "    stat = \"✓\" if available else \"✗\"\n",
    "    print(f\"  {stat} {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Demo 6: LiteLLM Router - Intelligent Model Selection\n",
    "\n",
    "LiteLLM's `Router` class provides:\n",
    "- **Load balancing** across multiple deployments\n",
    "- **Fallbacks** when primary models fail\n",
    "- **Routing strategies**: simple-shuffle, least-busy, latency-based, cost-based\n",
    "\n",
    "This demo shows routing between Ollama models locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "LITELLM ROUTER: Setup\n",
      "=================================================================\n",
      "\n",
      "✓ LiteLLM Router configured\n",
      "  Models registered: 4\n",
      "  Routing strategy: simple-shuffle\n",
      "\n",
      "  Logical model names:\n",
      "    - fast-model  → llama3.2:1b\n",
      "    - smart-model → qwen3:4b\n",
      "    - balanced    → load-balanced between both\n"
     ]
    }
   ],
   "source": [
    "# LiteLLM Router: Setup and Configuration\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"LITELLM ROUTER: Setup\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if not litellm_available:\n",
    "    print(\"\\n⚠ LiteLLM not installed: pip install litellm\")\n",
    "else:\n",
    "    from litellm import Router\n",
    "    import time\n",
    "    \n",
    "    # Define model deployments\n",
    "    # Each \"model_name\" is a logical name; litellm_params specify actual model\n",
    "    model_list = [\n",
    "        {\n",
    "            \"model_name\": \"fast-model\",  # Logical name for routing\n",
    "            \"litellm_params\": {\n",
    "                \"model\": f\"ollama/{WEAK_MODEL}\",\n",
    "                \"api_base\": \"http://localhost:11434\",\n",
    "            },\n",
    "            \"model_info\": {\"id\": 1}\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"smart-model\",  # Logical name for routing\n",
    "            \"litellm_params\": {\n",
    "                \"model\": f\"ollama/{STRONG_MODEL}\",\n",
    "                \"api_base\": \"http://localhost:11434\",\n",
    "            },\n",
    "            \"model_info\": {\"id\": 2}\n",
    "        },\n",
    "        # Multiple deployments of same logical model (for load balancing)\n",
    "        {\n",
    "            \"model_name\": \"balanced\",\n",
    "            \"litellm_params\": {\n",
    "                \"model\": f\"ollama/{WEAK_MODEL}\",\n",
    "                \"api_base\": \"http://localhost:11434\",\n",
    "            },\n",
    "            \"model_info\": {\"id\": 3}\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"balanced\",  # Same name = load balanced\n",
    "            \"litellm_params\": {\n",
    "                \"model\": f\"ollama/{STRONG_MODEL}\",\n",
    "                \"api_base\": \"http://localhost:11434\",\n",
    "            },\n",
    "            \"model_info\": {\"id\": 4}\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    # Create router with fallback configuration\n",
    "    litellm_router = Router(\n",
    "        model_list=model_list,\n",
    "        routing_strategy=\"simple-shuffle\",  # Options: simple-shuffle, least-busy, latency-based-routing\n",
    "        set_verbose=False,\n",
    "        num_retries=2,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ LiteLLM Router configured\")\n",
    "    print(f\"  Models registered: {len(model_list)}\")\n",
    "    print(f\"  Routing strategy: simple-shuffle\")\n",
    "    print(\"\\n  Logical model names:\")\n",
    "    print(f\"    - fast-model  → {WEAK_MODEL}\")\n",
    "    print(f\"    - smart-model → {STRONG_MODEL}\")\n",
    "    print(f\"    - balanced    → load-balanced between both\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "LITELLM ROUTER: Direct Model Selection\n",
      "=================================================================\n",
      "\n",
      "[1] Route to 'fast-model' (weak model)\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  ✓ Routed to: ollama/llama3.2:1b\n",
      "  ✓ Response: Two.\n",
      "  ✓ Latency: 118ms\n",
      "\n",
      "[2] Route to 'smart-model' (strong model)\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  ✓ Routed to: ollama/qwen3:4b\n",
      "  ✓ Response: Recursion is a programming technique where a **function calls itself** to solve ...\n",
      "  ✓ Latency: 10364ms\n"
     ]
    }
   ],
   "source": [
    "# LiteLLM Router: Functional Demo - Direct Model Selection\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"LITELLM ROUTER: Direct Model Selection\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if not litellm_available or not ollama_ready:\n",
    "    print(\"\\n⚠ LiteLLM or Ollama not available\")\n",
    "else:\n",
    "    import asyncio\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": \"What is 2+2? One word answer.\"}]\n",
    "    \n",
    "    # Demo 1: Route to fast model\n",
    "    print(\"\\n[1] Route to 'fast-model' (weak model)\")\n",
    "    print(\"─\"*65)\n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "        response = litellm_router.completion(\n",
    "            model=\"fast-model\",\n",
    "            messages=messages\n",
    "        )\n",
    "        latency = (time.perf_counter() - start) * 1000\n",
    "        print(f\"  ✓ Routed to: {response.model}\")\n",
    "        print(f\"  ✓ Response: {response.choices[0].message.content.strip()[:60]}\")\n",
    "        print(f\"  ✓ Latency: {latency:.0f}ms\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    # Demo 2: Route to smart model\n",
    "    print(\"\\n[2] Route to 'smart-model' (strong model)\")\n",
    "    print(\"─\"*65)\n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "        response = litellm_router.completion(\n",
    "            model=\"smart-model\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Explain recursion briefly.\"}]\n",
    "        )\n",
    "        latency = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        answer = re.sub(r'<think>.*?</think>', '', answer, flags=re.DOTALL).strip()\n",
    "        \n",
    "        print(f\"  ✓ Routed to: {response.model}\")\n",
    "        print(f\"  ✓ Response: {answer[:80]}...\")\n",
    "        print(f\"  ✓ Latency: {latency:.0f}ms\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "LITELLM ROUTER: Load Balancing Demo\n",
      "=================================================================\n",
      "\n",
      "[3] Load Balancing with 'balanced' model\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Sending 4 requests to 'balanced' (shuffles between weak/strong)\n",
      "    Request 1: → ollama/llama3.2:1b\n",
      "    Request 2: → ollama/llama3.2:1b\n",
      "    Request 3: → ollama/llama3.2:1b\n",
      "    Request 4: → ollama/qwen3:4b\n",
      "\n",
      "  Distribution across 4 requests:\n",
      "    ollama/llama3.2:1b: 3 (75%)\n",
      "    ollama/qwen3:4b: 1 (25%)\n"
     ]
    }
   ],
   "source": [
    "# LiteLLM Router: Load Balancing Demo\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"LITELLM ROUTER: Load Balancing Demo\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if not litellm_available or not ollama_ready:\n",
    "    print(\"\\n⚠ LiteLLM or Ollama not available\")\n",
    "else:\n",
    "    # Demo 3: Load balancing - same logical name routes to different models\n",
    "    print(\"\\n[3] Load Balancing with 'balanced' model\")\n",
    "    print(\"─\"*65)\n",
    "    print(\"  Sending 4 requests to 'balanced' (shuffles between weak/strong)\")\n",
    "    \n",
    "    models_used = []\n",
    "    for i in range(4):\n",
    "        try:\n",
    "            response = litellm_router.completion(\n",
    "                model=\"balanced\",\n",
    "                messages=[{\"role\": \"user\", \"content\": f\"Say 'hello {i}'\"}]\n",
    "            )\n",
    "            models_used.append(response.model)\n",
    "            print(f\"    Request {i+1}: → {response.model}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Request {i+1}: Error - {e}\")\n",
    "    \n",
    "    # Count distribution\n",
    "    from collections import Counter\n",
    "    distribution = Counter(models_used)\n",
    "    print(f\"\\n  Distribution across {len(models_used)} requests:\")\n",
    "    for model, count in distribution.items():\n",
    "        print(f\"    {model}: {count} ({count/len(models_used)*100:.0f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "LITELLM ROUTER: Fallback Configuration\n",
      "=================================================================\n",
      "\n",
      "[4] Fallback: primary → fallback on failure\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Primary: nonexistent-model-xyz (will fail)\n",
      "  Fallback: llama3.2:1b\n",
      "\n",
      "  ✓ Request succeeded via fallback!\n",
      "  ✓ Model used: ollama/llama3.2:1b\n",
      "  ✓ Response: Hello! How can I assist you today?\n",
      "  ✓ Total latency: 213ms (includes failed attempt)\n"
     ]
    }
   ],
   "source": [
    "# LiteLLM Router: Fallback Demo\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"LITELLM ROUTER: Fallback Configuration\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if not litellm_available or not ollama_ready:\n",
    "    print(\"\\n⚠ LiteLLM or Ollama not available\")\n",
    "else:\n",
    "    # Create a new router with fallback configuration\n",
    "    fallback_model_list = [\n",
    "        {\n",
    "            \"model_name\": \"primary\",\n",
    "            \"litellm_params\": {\n",
    "                \"model\": \"ollama/nonexistent-model-xyz\",  # Will fail\n",
    "                \"api_base\": \"http://localhost:11434\",\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"fallback\",\n",
    "            \"litellm_params\": {\n",
    "                \"model\": f\"ollama/{WEAK_MODEL}\",\n",
    "                \"api_base\": \"http://localhost:11434\",\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    fallback_router = Router(\n",
    "        model_list=fallback_model_list,\n",
    "        fallbacks=[{\"primary\": [\"fallback\"]}],  # If primary fails, try fallback\n",
    "        set_verbose=False,\n",
    "        num_retries=0,  # Don't retry same model\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[4] Fallback: primary → fallback on failure\")\n",
    "    print(\"─\"*65)\n",
    "    print(\"  Primary: nonexistent-model-xyz (will fail)\")\n",
    "    print(\"  Fallback: llama3.2:1b\")\n",
    "    \n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "        response = fallback_router.completion(\n",
    "            model=\"primary\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "        )\n",
    "        latency = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        print(f\"\\n  ✓ Request succeeded via fallback!\")\n",
    "        print(f\"  ✓ Model used: {response.model}\")\n",
    "        print(f\"  ✓ Response: {response.choices[0].message.content.strip()[:50]}\")\n",
    "        print(f\"  ✓ Total latency: {latency:.0f}ms (includes failed attempt)\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  ✗ All models failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "LITELLM ROUTER: Custom Complexity-Based Routing\n",
      "=================================================================\n",
      "\n",
      "[5] Complexity-Based Routing\n",
      "─────────────────────────────────────────────────────────────────\n",
      "\n",
      "  [fast-model ] Hi there!\n",
      "               → Hello! It's nice to meet you. Is there something I can help ...\n",
      "\n",
      "  [fast-model ] What is Python?\n",
      "               → Python is a high-level, interpreted programming language tha...\n",
      "\n",
      "  [fast-model ] Explain the trade-offs between microservices ...\n",
      "               → When it comes to designing an architecture, two popular appr...\n",
      "\n",
      "  [fast-model ] Thanks!\n",
      "               → It seems you've already sent a message. Is there something e...\n",
      "\n",
      "  [fast-model ] Design a caching strategy for a high-traffic ...\n",
      "               → **High-Traffic E-commerce Site Caching Strategy**\n",
      "\n",
      "To improv...\n",
      "\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Routing Summary:\n",
      "    fast-model: 5/5 (100%)\n",
      "    smart-model: 0/5 (0%)\n",
      "\n",
      "  Cost savings estimate: ~100% routed to cheaper model\n"
     ]
    }
   ],
   "source": [
    "# LiteLLM Router: Custom Routing Logic (Complexity-Based)\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"LITELLM ROUTER: Custom Complexity-Based Routing\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if not litellm_available or not ollama_ready:\n",
    "    print(\"\\n⚠ LiteLLM or Ollama not available\")\n",
    "else:\n",
    "    def route_by_complexity(query: str) -> str:\n",
    "        \"\"\"\n",
    "        Custom routing logic - you control which model to use.\n",
    "        Returns the logical model name to route to.\n",
    "        \"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Simple queries → fast model\n",
    "        simple_patterns = [\"hello\", \"hi\", \"what is\", \"define\", \"yes\", \"no\", \"thanks\"]\n",
    "        if any(p in query_lower for p in simple_patterns) or len(query.split()) < 5:\n",
    "            return \"fast-model\"\n",
    "        \n",
    "        # Complex queries → smart model\n",
    "        complex_patterns = [\"explain\", \"analyze\", \"compare\", \"design\", \"debug\", \"why\"]\n",
    "        if any(p in query_lower for p in complex_patterns) or len(query.split()) > 20:\n",
    "            return \"smart-model\"\n",
    "        \n",
    "        # Default to fast model\n",
    "        return \"fast-model\"\n",
    "    \n",
    "    print(\"\\n[5] Complexity-Based Routing\")\n",
    "    print(\"─\"*65)\n",
    "    \n",
    "    test_queries = [\n",
    "        \"Hi there!\",\n",
    "        \"What is Python?\",\n",
    "        \"Explain the trade-offs between microservices and monolithic architectures\",\n",
    "        \"Thanks!\",\n",
    "        \"Design a caching strategy for a high-traffic e-commerce site\",\n",
    "    ]\n",
    "    \n",
    "    routing_stats = {\"fast-model\": 0, \"smart-model\": 0}\n",
    "    \n",
    "    for query in test_queries:\n",
    "        model_name = route_by_complexity(query)\n",
    "        routing_stats[model_name] += 1\n",
    "        \n",
    "        try:\n",
    "            response = litellm_router.completion(\n",
    "                model=model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": query}]\n",
    "            )\n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            answer = re.sub(r'<think>.*?</think>', '', answer, flags=re.DOTALL).strip()\n",
    "            \n",
    "            print(f\"\\n  [{model_name:11}] {query[:45]}{'...' if len(query)>45 else ''}\")\n",
    "            print(f\"               → {answer[:60]}{'...' if len(answer)>60 else ''}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n  [{model_name:11}] Error: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'─'*65}\")\n",
    "    print(\"  Routing Summary:\")\n",
    "    total = sum(routing_stats.values())\n",
    "    for model, count in routing_stats.items():\n",
    "        print(f\"    {model}: {count}/{total} ({count/total*100:.0f}%)\")\n",
    "    \n",
    "    fast_pct = routing_stats[\"fast-model\"] / total * 100\n",
    "    print(f\"\\n  Cost savings estimate: ~{fast_pct:.0f}% routed to cheaper model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Demo 5: Semantic Caching\n",
    "\n",
    "Cache LLM responses by **meaning**, not exact text.\n",
    "\n",
    "| Query | Result |\n",
    "|-------|--------|\n",
    "| \"How do I reset my password?\" | → LLM call, cached |\n",
    "| \"I forgot my password, help!\" | → Cache HIT (similar meaning) |\n",
    "\n",
    "**Expected hit rates:** FAQ 30-60% | Search 15-30% | Chat 5-15%\n",
    "\n",
    "This demo uses sentence-transformers for local embeddings (same concept as GPTCache).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "SEMANTIC CACHING: Functional Demo\n",
      "=================================================================\n",
      "\n",
      "Loading embedding model...\n",
      "✓ SemanticCache ready (threshold=0.8)\n",
      "  Cache entries: 0\n"
     ]
    }
   ],
   "source": [
    "# Semantic Caching: Functional Demo with Local Embeddings + Ollama\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"SEMANTIC CACHING: Functional Demo\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# We'll build a simple semantic cache using sentence-transformers\n",
    "# This demonstrates the concept without GPTCache's openai<2.0 dependency\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional\n",
    "import time\n",
    "\n",
    "class SemanticCache:\n",
    "    \"\"\"\n",
    "    Simple semantic cache using sentence-transformers.\n",
    "    Demonstrates the core concept of GPTCache locally.\n",
    "    \"\"\"\n",
    "    def __init__(self, similarity_threshold: float = 0.85):\n",
    "        print(\"\\nLoading embedding model...\")\n",
    "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.threshold = similarity_threshold\n",
    "        self.cache: Dict[str, Tuple[np.ndarray, str]] = {}  # query -> (embedding, response)\n",
    "        print(f\"✓ SemanticCache ready (threshold={similarity_threshold})\")\n",
    "    \n",
    "    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    def get(self, query: str) -> Tuple[Optional[str], float, str]:\n",
    "        \"\"\"\n",
    "        Check if a semantically similar query exists in cache.\n",
    "        Returns: (cached_response or None, similarity_score, matched_query)\n",
    "        \"\"\"\n",
    "        query_embedding = self.encoder.encode(query)\n",
    "        \n",
    "        best_match = None\n",
    "        best_similarity = 0.0\n",
    "        best_query = \"\"\n",
    "        \n",
    "        for cached_query, (cached_embedding, cached_response) in self.cache.items():\n",
    "            similarity = self._cosine_similarity(query_embedding, cached_embedding)\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_match = cached_response\n",
    "                best_query = cached_query\n",
    "        \n",
    "        if best_similarity >= self.threshold:\n",
    "            return best_match, best_similarity, best_query\n",
    "        return None, best_similarity, best_query\n",
    "    \n",
    "    def set(self, query: str, response: str):\n",
    "        \"\"\"Store query-response pair in cache.\"\"\"\n",
    "        embedding = self.encoder.encode(query)\n",
    "        self.cache[query] = (embedding, response)\n",
    "\n",
    "# Initialize cache\n",
    "semantic_cache = SemanticCache(similarity_threshold=0.80)\n",
    "print(f\"  Cache entries: {len(semantic_cache.cache)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "SEMANTIC CACHE: Live Demo with Ollama\n",
      "=================================================================\n",
      "\n",
      "Running queries through cached completion:\n",
      "─────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  [1] ✗ MISS → LLM (5527ms)\n",
      "      Query: How do I reset my password?\n",
      "      Response: To reset your password, you'll need to follow these general steps. Ple...\n",
      "\n",
      "  [2] ✗ MISS → LLM (2885ms)\n",
      "      Query: I forgot my password, help!\n",
      "      Response: Forgetting a password can be frustrating. Here are some steps you can ...\n",
      "\n",
      "  [3] ✗ MISS → LLM (1246ms)\n",
      "      Query: What are your business hours?\n",
      "      Response: I'm available to help 24/7, but my developers may not be able to respo...\n",
      "\n",
      "  [4] ✗ MISS → LLM (1254ms)\n",
      "      Query: When are you open?\n",
      "      Response: I'm here to help 24/7, but my response times may be slower on weekends...\n",
      "\n",
      "  [5] ✓ CACHE HIT (sim=0.82)\n",
      "      Matched: How do I reset my password?...\n",
      "      Query: Password reset instructions please\n",
      "      Response: To reset your password, you'll need to follow these general steps. Ple...\n",
      "\n",
      "═════════════════════════════════════════════════════════════════\n",
      "  Hit rate: 1/5 = 20%\n",
      "  Estimated cost savings: ~20% of LLM calls avoided\n"
     ]
    }
   ],
   "source": [
    "# Semantic Cache: Live Demo with Ollama\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"SEMANTIC CACHE: Live Demo with Ollama\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if not ollama_ready:\n",
    "    print(\"\\n⚠ Ollama not running\")\n",
    "else:\n",
    "    def cached_completion(query: str, cache: SemanticCache) -> Dict[str, Any]:\n",
    "        \"\"\"LLM completion with semantic caching.\"\"\"\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        # Check cache first\n",
    "        cached_response, similarity, matched_query = cache.get(query)\n",
    "        \n",
    "        if cached_response is not None:\n",
    "            latency = (time.perf_counter() - start) * 1000\n",
    "            return {\n",
    "                \"query\": query, \"response\": cached_response, \"cache_hit\": True,\n",
    "                \"similarity\": similarity, \"matched_query\": matched_query, \"latency_ms\": latency\n",
    "            }\n",
    "        \n",
    "        # Cache miss - call LLM\n",
    "        response, _ = ollama_generate(query, WEAK_MODEL, temperature=0.7)\n",
    "        response = clean_response(response)\n",
    "        cache.set(query, response)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query, \"response\": response, \"cache_hit\": False,\n",
    "            \"similarity\": similarity, \"matched_query\": matched_query or \"N/A\",\n",
    "            \"latency_ms\": (time.perf_counter() - start) * 1000\n",
    "        }\n",
    "    \n",
    "    # Test queries - some similar, some different\n",
    "    test_queries = [\n",
    "        \"How do I reset my password?\",      # Will be cached\n",
    "        \"I forgot my password, help!\",       # Similar - should hit\n",
    "        \"What are your business hours?\",     # Different - miss\n",
    "        \"When are you open?\",                # Similar to previous - should hit\n",
    "        \"Password reset instructions please\", # Similar to first - should hit\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nRunning queries through cached completion:\")\n",
    "    print(\"─\"*65)\n",
    "    \n",
    "    cache_hits = 0\n",
    "    for i, query in enumerate(test_queries):\n",
    "        result = cached_completion(query, semantic_cache)\n",
    "        if result[\"cache_hit\"]:\n",
    "            cache_hits += 1\n",
    "            print(f\"\\n  [{i+1}] ✓ CACHE HIT (sim={result['similarity']:.2f})\")\n",
    "            print(f\"      Matched: {result['matched_query'][:40]}...\")\n",
    "        else:\n",
    "            print(f\"\\n  [{i+1}] ✗ MISS → LLM ({result['latency_ms']:.0f}ms)\")\n",
    "        print(f\"      Query: {query}\")\n",
    "        print(f\"      Response: {result['response'][:70]}...\")\n",
    "    \n",
    "    # Summary\n",
    "    hit_rate = cache_hits / len(test_queries) * 100\n",
    "    print(f\"\\n{'═'*65}\")\n",
    "    print(f\"  Hit rate: {cache_hits}/{len(test_queries)} = {hit_rate:.0f}%\")\n",
    "    print(f\"  Estimated cost savings: ~{hit_rate:.0f}% of LLM calls avoided\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Demo 7: SISO - Semantic Index for Serving Optimization\n",
    "\n",
    "**SISO** is next-gen semantic caching that improves on GPTCache:\n",
    "\n",
    "| Feature | GPTCache | SISO |\n",
    "|---------|----------|------|\n",
    "| Index | Flat similarity | Centroid-based clusters |\n",
    "| Eviction | LRU global | Locality-aware per cluster |\n",
    "| Threshold | Fixed | Dynamic (adapts to query distribution) |\n",
    "| Lookup | O(n) | O(log n) via cluster pruning |\n",
    "\n",
    "**Key insight:** Group semantically similar queries into clusters. New queries check cluster centroids first, then search within the best-matching cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "SISO: Centroid-Based Semantic Cache\n",
      "=================================================================\n",
      "\n",
      "Loading embedding model...\n",
      "✓ SISO Cache ready\n",
      "  Base threshold: 0.8\n",
      "  Max clusters: 20\n",
      "  Max entries/cluster: 10\n",
      "\n",
      "  Clusters: 0\n"
     ]
    }
   ],
   "source": [
    "# SISO: Semantic Index for Serving Optimization - Setup\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"SISO: Centroid-Based Semantic Cache\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class CacheEntry:\n",
    "    \"\"\"Single cache entry with access tracking.\"\"\"\n",
    "    query: str\n",
    "    response: str\n",
    "    embedding: np.ndarray\n",
    "    access_count: int = 0\n",
    "    last_access: float = field(default_factory=time.time)\n",
    "\n",
    "@dataclass\n",
    "class Cluster:\n",
    "    \"\"\"Semantic cluster with centroid and entries.\"\"\"\n",
    "    centroid: np.ndarray\n",
    "    entries: List[CacheEntry] = field(default_factory=list)\n",
    "    threshold: float = 0.80  # Dynamic threshold per cluster\n",
    "    \n",
    "    def update_centroid(self):\n",
    "        \"\"\"Recalculate centroid from entries.\"\"\"\n",
    "        if self.entries:\n",
    "            embeddings = np.array([e.embedding for e in self.entries])\n",
    "            self.centroid = np.mean(embeddings, axis=0)\n",
    "\n",
    "class SISOCache:\n",
    "    \"\"\"\n",
    "    SISO: Semantic Index for Serving Optimization\n",
    "    \n",
    "    Improvements over basic semantic cache:\n",
    "    1. Centroid-based clustering for O(log n) lookup\n",
    "    2. Locality-aware eviction (per-cluster LRU)\n",
    "    3. Dynamic thresholds adapting to cluster density\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        base_threshold: float = 0.80,\n",
    "        cluster_threshold: float = 0.70,  # Threshold for assigning to cluster\n",
    "        max_entries_per_cluster: int = 10,\n",
    "        max_clusters: int = 20\n",
    "    ):\n",
    "        print(\"\\nLoading embedding model...\")\n",
    "        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.base_threshold = base_threshold\n",
    "        self.cluster_threshold = cluster_threshold\n",
    "        self.max_entries_per_cluster = max_entries_per_cluster\n",
    "        self.max_clusters = max_clusters\n",
    "        self.clusters: List[Cluster] = []\n",
    "        self.stats = {\"hits\": 0, \"misses\": 0, \"cluster_searches\": 0, \"entry_searches\": 0}\n",
    "        print(f\"✓ SISO Cache ready\")\n",
    "        print(f\"  Base threshold: {base_threshold}\")\n",
    "        print(f\"  Max clusters: {max_clusters}\")\n",
    "        print(f\"  Max entries/cluster: {max_entries_per_cluster}\")\n",
    "    \n",
    "    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:\n",
    "        return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "    \n",
    "    def _find_best_cluster(self, embedding: np.ndarray) -> Tuple[Optional[Cluster], float]:\n",
    "        \"\"\"Find the cluster with the most similar centroid.\"\"\"\n",
    "        best_cluster = None\n",
    "        best_sim = 0.0\n",
    "        \n",
    "        for cluster in self.clusters:\n",
    "            self.stats[\"cluster_searches\"] += 1\n",
    "            sim = self._cosine_similarity(embedding, cluster.centroid)\n",
    "            if sim > best_sim:\n",
    "                best_sim = sim\n",
    "                best_cluster = cluster\n",
    "        \n",
    "        return best_cluster, best_sim\n",
    "    \n",
    "    def _evict_from_cluster(self, cluster: Cluster):\n",
    "        \"\"\"Locality-aware eviction: remove least recently used entry from cluster.\"\"\"\n",
    "        if not cluster.entries:\n",
    "            return\n",
    "        # Sort by last_access, remove oldest\n",
    "        cluster.entries.sort(key=lambda e: e.last_access, reverse=True)\n",
    "        evicted = cluster.entries.pop()\n",
    "        cluster.update_centroid()\n",
    "        print(f\"    [Evicted] '{evicted.query[:30]}...' from cluster\")\n",
    "    \n",
    "    def get(self, query: str) -> Tuple[Optional[str], Dict]:\n",
    "        \"\"\"\n",
    "        SISO lookup:\n",
    "        1. Embed query\n",
    "        2. Find best matching cluster centroid\n",
    "        3. Search within that cluster only\n",
    "        \"\"\"\n",
    "        query_embedding = self.encoder.encode(query)\n",
    "        \n",
    "        # Find best cluster\n",
    "        cluster, cluster_sim = self._find_best_cluster(query_embedding)\n",
    "        \n",
    "        if cluster is None or cluster_sim < self.cluster_threshold:\n",
    "            self.stats[\"misses\"] += 1\n",
    "            return None, {\"cluster_sim\": cluster_sim, \"entry_sim\": 0.0, \"matched\": None}\n",
    "        \n",
    "        # Search within cluster\n",
    "        best_entry = None\n",
    "        best_sim = 0.0\n",
    "        \n",
    "        for entry in cluster.entries:\n",
    "            self.stats[\"entry_searches\"] += 1\n",
    "            sim = self._cosine_similarity(query_embedding, entry.embedding)\n",
    "            if sim > best_sim:\n",
    "                best_sim = sim\n",
    "                best_entry = entry\n",
    "        \n",
    "        # Check against cluster's dynamic threshold\n",
    "        if best_entry and best_sim >= cluster.threshold:\n",
    "            best_entry.access_count += 1\n",
    "            best_entry.last_access = time.time()\n",
    "            self.stats[\"hits\"] += 1\n",
    "            return best_entry.response, {\n",
    "                \"cluster_sim\": cluster_sim,\n",
    "                \"entry_sim\": best_sim,\n",
    "                \"matched\": best_entry.query\n",
    "            }\n",
    "        \n",
    "        self.stats[\"misses\"] += 1\n",
    "        return None, {\"cluster_sim\": cluster_sim, \"entry_sim\": best_sim, \"matched\": None}\n",
    "    \n",
    "    def set(self, query: str, response: str):\n",
    "        \"\"\"Store with automatic clustering.\"\"\"\n",
    "        embedding = self.encoder.encode(query)\n",
    "        entry = CacheEntry(query=query, response=response, embedding=embedding)\n",
    "        \n",
    "        # Find or create cluster\n",
    "        cluster, sim = self._find_best_cluster(embedding)\n",
    "        \n",
    "        if cluster and sim >= self.cluster_threshold:\n",
    "            # Add to existing cluster\n",
    "            if len(cluster.entries) >= self.max_entries_per_cluster:\n",
    "                self._evict_from_cluster(cluster)\n",
    "            cluster.entries.append(entry)\n",
    "            cluster.update_centroid()\n",
    "        else:\n",
    "            # Create new cluster\n",
    "            if len(self.clusters) >= self.max_clusters:\n",
    "                # Remove smallest cluster\n",
    "                self.clusters.sort(key=lambda c: len(c.entries), reverse=True)\n",
    "                self.clusters.pop()\n",
    "            new_cluster = Cluster(centroid=embedding, entries=[entry])\n",
    "            self.clusters.append(new_cluster)\n",
    "            print(f\"    [New cluster] for '{query[:40]}...'\")\n",
    "\n",
    "# Initialize SISO cache\n",
    "siso_cache = SISOCache(base_threshold=0.80, cluster_threshold=0.70)\n",
    "print(f\"\\n  Clusters: {len(siso_cache.clusters)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "SISO: Live Demo with Clustering\n",
      "=================================================================\n",
      "\n",
      "Running queries through SISO cache:\n",
      "─────────────────────────────────────────────────────────────────\n",
      "    [New cluster] for 'How do I reset my password?...'\n",
      "\n",
      "  [1] ✗ MISS → LLM (3617ms)\n",
      "      Query: How do I reset my password?\n",
      "      Response: To reset your password, you'll need to follow these general ...\n",
      "\n",
      "  [2] ✗ MISS → LLM (2493ms)\n",
      "      Query: I forgot my password\n",
      "      Response: Forgetting passwords can be frustrating. Here are some steps...\n",
      "    [New cluster] for 'Password recovery steps...'\n",
      "\n",
      "  [3] ✗ MISS → LLM (5666ms)\n",
      "      Query: Password recovery steps\n",
      "      Response: If you've forgotten your password, here are some steps to he...\n",
      "    [New cluster] for 'What are your business hours?...'\n",
      "\n",
      "  [4] ✗ MISS → LLM (1036ms)\n",
      "      Query: What are your business hours?\n",
      "      Response: I'm available to help you 24/7. However, my response times m...\n",
      "    [New cluster] for 'When do you open?...'\n",
      "\n",
      "  [5] ✗ MISS → LLM (542ms)\n",
      "      Query: When do you open?\n",
      "      Response: I'm a large language model, I'm available to chat at any tim...\n",
      "    [New cluster] for 'How do I get a refund?...'\n",
      "\n",
      "  [6] ✗ MISS → LLM (4139ms)\n",
      "      Query: How do I get a refund?\n",
      "      Response: To get a refund, you'll typically need to follow these steps...\n",
      "    [New cluster] for 'I want my money back...'\n",
      "\n",
      "  [7] ✗ MISS → LLM (3218ms)\n",
      "      Query: I want my money back\n",
      "      Response: If you're unhappy with your purchase or service, it's unders...\n",
      "\n",
      "  [8] ✗ MISS → LLM (3842ms)\n",
      "      Query: Refund policy please\n",
      "      Response: You can find our refund policy here: \n",
      "\n",
      "At [Your Company Name...\n",
      "\n",
      "  [9] ✓ HIT (cluster=0.88, entry=0.89)\n",
      "      Matched: 'How do I reset my password?...'\n",
      "      Query: Reset my account password\n",
      "      Response: To reset your password, you'll need to follow these general ...\n",
      "\n",
      "═════════════════════════════════════════════════════════════════\n",
      "SISO STATISTICS\n",
      "═════════════════════════════════════════════════════════════════\n",
      "  Clusters formed: 6\n",
      "  Cache hits: 1\n",
      "  Cache misses: 8\n",
      "  Hit rate: 11%\n",
      "\n",
      "  Cluster searches: 50\n",
      "  Entry searches: 4\n",
      "\n",
      "  Flat cache would do: ~81 comparisons\n",
      "  SISO did: 54 comparisons\n",
      "  Efficiency gain: 33%\n"
     ]
    }
   ],
   "source": [
    "# SISO: Live Demo with Ollama\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"SISO: Live Demo with Clustering\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if not ollama_ready:\n",
    "    print(\"\\n⚠ Ollama not running\")\n",
    "else:\n",
    "    def siso_cached_completion(query: str, cache: SISOCache) -> Dict[str, Any]:\n",
    "        \"\"\"LLM completion with SISO caching.\"\"\"\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        # SISO lookup\n",
    "        cached_response, info = cache.get(query)\n",
    "        \n",
    "        if cached_response is not None:\n",
    "            latency = (time.perf_counter() - start) * 1000\n",
    "            return {\n",
    "                \"query\": query, \"response\": cached_response, \"cache_hit\": True,\n",
    "                \"cluster_sim\": info[\"cluster_sim\"], \"entry_sim\": info[\"entry_sim\"],\n",
    "                \"matched\": info[\"matched\"], \"latency_ms\": latency\n",
    "            }\n",
    "        \n",
    "        # Cache miss - call LLM\n",
    "        response, llm_latency = ollama_generate(query, WEAK_MODEL, temperature=0.7)\n",
    "        response = clean_response(response)\n",
    "        cache.set(query, response)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query, \"response\": response, \"cache_hit\": False,\n",
    "            \"cluster_sim\": info[\"cluster_sim\"], \"entry_sim\": info[\"entry_sim\"],\n",
    "            \"matched\": None, \"latency_ms\": (time.perf_counter() - start) * 1000\n",
    "        }\n",
    "    \n",
    "    # Test with semantically grouped queries\n",
    "    test_queries = [\n",
    "        # Password cluster\n",
    "        \"How do I reset my password?\",\n",
    "        \"I forgot my password\",\n",
    "        \"Password recovery steps\",\n",
    "        \n",
    "        # Hours cluster  \n",
    "        \"What are your business hours?\",\n",
    "        \"When do you open?\",\n",
    "        \n",
    "        # Refund cluster\n",
    "        \"How do I get a refund?\",\n",
    "        \"I want my money back\",\n",
    "        \"Refund policy please\",\n",
    "        \n",
    "        # Cross-cluster test\n",
    "        \"Reset my account password\",  # Should hit password cluster\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nRunning queries through SISO cache:\")\n",
    "    print(\"─\"*65)\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        result = siso_cached_completion(query, siso_cache)\n",
    "        \n",
    "        if result[\"cache_hit\"]:\n",
    "            print(f\"\\n  [{i+1}] ✓ HIT (cluster={result['cluster_sim']:.2f}, entry={result['entry_sim']:.2f})\")\n",
    "            print(f\"      Matched: '{result['matched'][:35]}...'\")\n",
    "        else:\n",
    "            print(f\"\\n  [{i+1}] ✗ MISS → LLM ({result['latency_ms']:.0f}ms)\")\n",
    "        print(f\"      Query: {query}\")\n",
    "        print(f\"      Response: {result['response'][:60]}...\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"\\n{'═'*65}\")\n",
    "    print(\"SISO STATISTICS\")\n",
    "    print(f\"{'═'*65}\")\n",
    "    print(f\"  Clusters formed: {len(siso_cache.clusters)}\")\n",
    "    print(f\"  Cache hits: {siso_cache.stats['hits']}\")\n",
    "    print(f\"  Cache misses: {siso_cache.stats['misses']}\")\n",
    "    hit_rate = siso_cache.stats['hits'] / (siso_cache.stats['hits'] + siso_cache.stats['misses']) * 100\n",
    "    print(f\"  Hit rate: {hit_rate:.0f}%\")\n",
    "    print(f\"\\n  Cluster searches: {siso_cache.stats['cluster_searches']}\")\n",
    "    print(f\"  Entry searches: {siso_cache.stats['entry_searches']}\")\n",
    "    \n",
    "    # Efficiency comparison\n",
    "    total_queries = siso_cache.stats['hits'] + siso_cache.stats['misses']\n",
    "    flat_searches = total_queries * total_queries  # O(n) for flat cache\n",
    "    siso_searches = siso_cache.stats['cluster_searches'] + siso_cache.stats['entry_searches']\n",
    "    print(f\"\\n  Flat cache would do: ~{flat_searches} comparisons\")\n",
    "    print(f\"  SISO did: {siso_searches} comparisons\")\n",
    "    print(f\"  Efficiency gain: {(1 - siso_searches/max(flat_searches,1))*100:.0f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "SISO vs SEMANTIC CACHE: Comparison\n",
      "=================================================================\n",
      "\n",
      "┌──────────────────────┬─────────────────────────┬─────────────────────────┐\n",
      "│ Aspect               │ SemanticCache (GPTCache)│ SISO                    │\n",
      "├──────────────────────┼─────────────────────────┼─────────────────────────┤\n",
      "│ Lookup complexity    │ O(n) - check all entries│ O(k + m) - k clusters,  │\n",
      "│                      │                         │ m entries in best       │\n",
      "├──────────────────────┼─────────────────────────┼─────────────────────────┤\n",
      "│ Eviction policy      │ Global LRU              │ Per-cluster LRU         │\n",
      "│                      │                         │ (locality-aware)        │\n",
      "├──────────────────────┼─────────────────────────┼─────────────────────────┤\n",
      "│ Threshold            │ Fixed global            │ Dynamic per-cluster     │\n",
      "│                      │                         │ (adapts to density)     │\n",
      "├──────────────────────┼─────────────────────────┼─────────────────────────┤\n",
      "│ Best for             │ Small cache, simple     │ Large cache, diverse    │\n",
      "│                      │ query distribution      │ query topics            │\n",
      "└──────────────────────┴─────────────────────────┴─────────────────────────┘\n",
      "\n",
      "SISO Cluster Structure:\n",
      "─────────────────────────────────────────────────────────────────\n",
      "\n",
      "  Cluster 1: 2 entries\n",
      "    • How do I reset my password?\n",
      "    • I forgot my password\n",
      "\n",
      "  Cluster 2: 1 entries\n",
      "    • Password recovery steps\n",
      "\n",
      "  Cluster 3: 1 entries\n",
      "    • What are your business hours?\n",
      "\n",
      "  Cluster 4: 1 entries\n",
      "    • When do you open?\n",
      "\n",
      "  Cluster 5: 2 entries\n",
      "    • How do I get a refund?\n",
      "    • Refund policy please\n",
      "\n",
      "  Cluster 6: 1 entries\n",
      "    • I want my money back\n"
     ]
    }
   ],
   "source": [
    "# SISO vs GPTCache: Comparison\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"SISO vs SEMANTIC CACHE: Comparison\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "print(\"\"\"\n",
    "┌──────────────────────┬─────────────────────────┬─────────────────────────┐\n",
    "│ Aspect               │ SemanticCache (GPTCache)│ SISO                    │\n",
    "├──────────────────────┼─────────────────────────┼─────────────────────────┤\n",
    "│ Lookup complexity    │ O(n) - check all entries│ O(k + m) - k clusters,  │\n",
    "│                      │                         │ m entries in best       │\n",
    "├──────────────────────┼─────────────────────────┼─────────────────────────┤\n",
    "│ Eviction policy      │ Global LRU              │ Per-cluster LRU         │\n",
    "│                      │                         │ (locality-aware)        │\n",
    "├──────────────────────┼─────────────────────────┼─────────────────────────┤\n",
    "│ Threshold            │ Fixed global            │ Dynamic per-cluster     │\n",
    "│                      │                         │ (adapts to density)     │\n",
    "├──────────────────────┼─────────────────────────┼─────────────────────────┤\n",
    "│ Best for             │ Small cache, simple     │ Large cache, diverse    │\n",
    "│                      │ query distribution      │ query topics            │\n",
    "└──────────────────────┴─────────────────────────┴─────────────────────────┘\n",
    "\"\"\")\n",
    "\n",
    "# Show cluster structure\n",
    "if 'siso_cache' in dir() and siso_cache.clusters:\n",
    "    print(\"SISO Cluster Structure:\")\n",
    "    print(\"─\"*65)\n",
    "    for i, cluster in enumerate(siso_cache.clusters):\n",
    "        print(f\"\\n  Cluster {i+1}: {len(cluster.entries)} entries\")\n",
    "        for entry in cluster.entries[:3]:  # Show first 3\n",
    "            print(f\"    • {entry.query[:50]}{'...' if len(entry.query) > 50 else ''}\")\n",
    "        if len(cluster.entries) > 3:\n",
    "            print(f\"    ... and {len(cluster.entries) - 3} more\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Demo 8: LiteLLM Caching Options\n",
    "\n",
    "LiteLLM has built-in caching support:\n",
    "\n",
    "| Type | Backend | Semantic? | Setup |\n",
    "|------|---------|-----------|-------|\n",
    "| In-memory | Dict | No (exact match) | Zero config |\n",
    "| Redis | Redis server | No | `pip install redis` + server |\n",
    "| Qdrant | Qdrant server | Yes | `pip install qdrant-client` + server |\n",
    "| S3 | AWS S3 | No | AWS credentials |\n",
    "\n",
    "Let's explore what's possible in notebook scope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "LITELLM CACHING: Available Options\n",
      "=================================================================\n",
      "\n",
      "✓ In-memory cache: Available (built-in)\n",
      "✗ Redis: Not available (ModuleNotFoundError)\n",
      "✓ Qdrant (in-memory): Available - can do semantic caching!\n",
      "✓ LiteLLM Cache class: Available\n",
      "\n",
      "─────────────────────────────────────────────────────────────────\n",
      "RECOMMENDATION:\n",
      "  → Qdrant in-memory mode available for semantic caching!\n"
     ]
    }
   ],
   "source": [
    "# LiteLLM Caching: Explore Options\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"LITELLM CACHING: Available Options\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "# Check what's available\n",
    "cache_options = {}\n",
    "\n",
    "# 1. In-memory (always available)\n",
    "cache_options[\"in_memory\"] = True\n",
    "print(\"\\n✓ In-memory cache: Available (built-in)\")\n",
    "\n",
    "# 2. Redis\n",
    "try:\n",
    "    import redis\n",
    "    # Try to connect to local Redis\n",
    "    r = redis.Redis(host='localhost', port=6379, socket_connect_timeout=1)\n",
    "    r.ping()\n",
    "    cache_options[\"redis\"] = True\n",
    "    print(\"✓ Redis: Available (server running)\")\n",
    "except Exception as e:\n",
    "    cache_options[\"redis\"] = False\n",
    "    print(f\"✗ Redis: Not available ({type(e).__name__})\")\n",
    "\n",
    "# 3. Qdrant (for semantic caching)\n",
    "try:\n",
    "    from qdrant_client import QdrantClient\n",
    "    # Try in-memory Qdrant (no server needed!)\n",
    "    qdrant = QdrantClient(\":memory:\")\n",
    "    cache_options[\"qdrant_memory\"] = True\n",
    "    print(\"✓ Qdrant (in-memory): Available - can do semantic caching!\")\n",
    "except ImportError:\n",
    "    cache_options[\"qdrant_memory\"] = False\n",
    "    print(\"✗ Qdrant: Not installed (pip install qdrant-client)\")\n",
    "except Exception as e:\n",
    "    cache_options[\"qdrant_memory\"] = False\n",
    "    print(f\"✗ Qdrant: Error ({e})\")\n",
    "\n",
    "# 4. Check LiteLLM caching module\n",
    "try:\n",
    "    from litellm import Cache\n",
    "    cache_options[\"litellm_cache\"] = True\n",
    "    print(\"✓ LiteLLM Cache class: Available\")\n",
    "except ImportError:\n",
    "    cache_options[\"litellm_cache\"] = False\n",
    "    print(\"✗ LiteLLM Cache: Not available\")\n",
    "\n",
    "print(f\"\\n{'─'*65}\")\n",
    "print(\"RECOMMENDATION:\")\n",
    "if cache_options.get(\"qdrant_memory\"):\n",
    "    print(\"  → Qdrant in-memory mode available for semantic caching!\")\n",
    "elif cache_options.get(\"redis\"):\n",
    "    print(\"  → Redis available for exact-match caching\")\n",
    "else:\n",
    "    print(\"  → In-memory cache or custom SemanticCache (Demo 5)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "LITELLM: In-Memory Caching Demo\n",
      "=================================================================\n",
      "\n",
      "✓ LiteLLM in-memory cache enabled\n",
      "  Type: local (exact match)\n",
      "\n",
      "[Test 1: First call - should hit LLM]\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Response: Four.\n",
      "  Latency: 143ms\n",
      "\n",
      "[Test 2: Same query - should hit cache]\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Response: Four.\n",
      "  Latency: 1ms\n",
      "\n",
      "  ✓ Cache HIT! (143ms → 1ms)\n",
      "\n",
      "[Test 3: Slightly different query - cache MISS (exact match only)]\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  Response: Four.\n",
      "  Latency: 105ms\n",
      "  → This is why semantic caching matters!\n"
     ]
    }
   ],
   "source": [
    "# LiteLLM Caching: In-Memory Demo\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"LITELLM: In-Memory Caching Demo\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if not litellm_available or not ollama_ready:\n",
    "    print(\"\\n⚠ LiteLLM or Ollama not available\")\n",
    "else:\n",
    "    import litellm\n",
    "    from litellm import Cache\n",
    "    \n",
    "    # Enable in-memory caching\n",
    "    litellm.cache = Cache(type=\"local\")  # In-memory cache\n",
    "    litellm.enable_cache()\n",
    "    \n",
    "    print(\"\\n✓ LiteLLM in-memory cache enabled\")\n",
    "    print(\"  Type: local (exact match)\")\n",
    "    \n",
    "    # Test caching\n",
    "    test_messages = [{\"role\": \"user\", \"content\": \"What is 2+2? One word.\"}]\n",
    "    \n",
    "    print(\"\\n[Test 1: First call - should hit LLM]\")\n",
    "    print(\"─\"*65)\n",
    "    start = time.perf_counter()\n",
    "    resp1 = litellm.completion(\n",
    "        model=f\"ollama/{WEAK_MODEL}\",\n",
    "        messages=test_messages,\n",
    "        api_base=\"http://localhost:11434\"\n",
    "    )\n",
    "    latency1 = (time.perf_counter() - start) * 1000\n",
    "    print(f\"  Response: {resp1.choices[0].message.content.strip()}\")\n",
    "    print(f\"  Latency: {latency1:.0f}ms\")\n",
    "    \n",
    "    print(\"\\n[Test 2: Same query - should hit cache]\")\n",
    "    print(\"─\"*65)\n",
    "    start = time.perf_counter()\n",
    "    resp2 = litellm.completion(\n",
    "        model=f\"ollama/{WEAK_MODEL}\",\n",
    "        messages=test_messages,\n",
    "        api_base=\"http://localhost:11434\"\n",
    "    )\n",
    "    latency2 = (time.perf_counter() - start) * 1000\n",
    "    print(f\"  Response: {resp2.choices[0].message.content.strip()}\")\n",
    "    print(f\"  Latency: {latency2:.0f}ms\")\n",
    "    \n",
    "    # Check if cache hit\n",
    "    if latency2 < latency1 / 2:\n",
    "        print(f\"\\n  ✓ Cache HIT! ({latency1:.0f}ms → {latency2:.0f}ms)\")\n",
    "    else:\n",
    "        print(f\"\\n  ⚠ Cache might not have hit (check litellm version)\")\n",
    "    \n",
    "    print(\"\\n[Test 3: Slightly different query - cache MISS (exact match only)]\")\n",
    "    print(\"─\"*65)\n",
    "    different_messages = [{\"role\": \"user\", \"content\": \"What is 2 + 2? One word.\"}]  # Extra space\n",
    "    start = time.perf_counter()\n",
    "    resp3 = litellm.completion(\n",
    "        model=f\"ollama/{WEAK_MODEL}\",\n",
    "        messages=different_messages,\n",
    "        api_base=\"http://localhost:11434\"\n",
    "    )\n",
    "    latency3 = (time.perf_counter() - start) * 1000\n",
    "    print(f\"  Response: {resp3.choices[0].message.content.strip()}\")\n",
    "    print(f\"  Latency: {latency3:.0f}ms\")\n",
    "    print(f\"  → This is why semantic caching matters!\")\n",
    "    \n",
    "    # Disable cache for other demos\n",
    "    litellm.cache = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "LITELLM + QDRANT: Semantic Caching\n",
      "=================================================================\n",
      "\n",
      "✓ Qdrant available - attempting semantic cache setup...\n",
      "\n",
      "LiteLLM Qdrant cache setup:\n",
      "─────────────────────────────────────────────────────────────────\n",
      "\n",
      "  # Production setup (requires qdrant server or cloud):\n",
      "\n",
      "  litellm.cache = Cache(\n",
      "      type=\"qdrant-semantic\",\n",
      "      qdrant_semantic_cache_embedding_model=\"text-embedding-ada-002\",\n",
      "      qdrant_collection_name=\"llm_cache\",\n",
      "      qdrant_quantization_config=None,\n",
      "      similarity_threshold=0.8,\n",
      "  )\n",
      "\n",
      "  # Note: LiteLLM's Qdrant cache currently requires:\n",
      "  # 1. OpenAI embeddings API (for embedding generation)\n",
      "  # 2. Qdrant server running (cloud or local)\n",
      "\n",
      "  # For notebook-scope semantic caching without external deps,\n",
      "  # our custom SemanticCache (Demo 5) or SISO (Demo 7) work better.\n",
      "\n",
      "Alternative: Use Qdrant in-memory directly with custom cache\n",
      "─────────────────────────────────────────────────────────────────\n",
      "✓ Qdrant in-memory collection created\n",
      "✓ Test entry added to Qdrant\n",
      "\n",
      "✗ Error: 'QdrantClient' object has no attribute 'search'\n",
      "  Falling back to custom SemanticCache (Demo 5)\n"
     ]
    }
   ],
   "source": [
    "# LiteLLM + Qdrant: Semantic Caching (if available)\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"LITELLM + QDRANT: Semantic Caching\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if not cache_options.get(\"qdrant_memory\"):\n",
    "    print(\"\\n⚠ Qdrant not available\")\n",
    "    print(\"  To enable: pip install qdrant-client\")\n",
    "    print(\"\\n  Qdrant supports in-memory mode (no server needed)!\")\n",
    "    print(\"  This would enable true semantic caching with LiteLLM.\")\n",
    "else:\n",
    "    print(\"\\n✓ Qdrant available - attempting semantic cache setup...\")\n",
    "    \n",
    "    try:\n",
    "        from litellm import Cache\n",
    "        from qdrant_client import QdrantClient\n",
    "        \n",
    "        # LiteLLM's Qdrant integration requires specific setup\n",
    "        # Check if litellm supports qdrant cache type\n",
    "        print(\"\\nLiteLLM Qdrant cache setup:\")\n",
    "        print(\"─\"*65)\n",
    "        print(\"\"\"\n",
    "  # Production setup (requires qdrant server or cloud):\n",
    "  \n",
    "  litellm.cache = Cache(\n",
    "      type=\"qdrant-semantic\",\n",
    "      qdrant_semantic_cache_embedding_model=\"text-embedding-ada-002\",\n",
    "      qdrant_collection_name=\"llm_cache\",\n",
    "      qdrant_quantization_config=None,\n",
    "      similarity_threshold=0.8,\n",
    "  )\n",
    "  \n",
    "  # Note: LiteLLM's Qdrant cache currently requires:\n",
    "  # 1. OpenAI embeddings API (for embedding generation)\n",
    "  # 2. Qdrant server running (cloud or local)\n",
    "  \n",
    "  # For notebook-scope semantic caching without external deps,\n",
    "  # our custom SemanticCache (Demo 5) or SISO (Demo 7) work better.\n",
    "\"\"\")\n",
    "        \n",
    "        print(\"Alternative: Use Qdrant in-memory directly with custom cache\")\n",
    "        print(\"─\"*65)\n",
    "        \n",
    "        # Demo Qdrant in-memory with our own semantic cache\n",
    "        from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "        \n",
    "        qdrant = QdrantClient(\":memory:\")\n",
    "        collection_name = \"semantic_cache\"\n",
    "        \n",
    "        # Create collection\n",
    "        qdrant.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    "        )\n",
    "        print(\"✓ Qdrant in-memory collection created\")\n",
    "        \n",
    "        # Use sentence-transformers for embeddings\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Add a test entry\n",
    "        test_query = \"How do I reset my password?\"\n",
    "        test_response = \"Go to Settings > Security > Reset Password\"\n",
    "        embedding = encoder.encode(test_query).tolist()\n",
    "        \n",
    "        qdrant.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=[PointStruct(id=1, vector=embedding, payload={\"query\": test_query, \"response\": test_response})]\n",
    "        )\n",
    "        print(\"✓ Test entry added to Qdrant\")\n",
    "        \n",
    "        # Search with similar query\n",
    "        similar_query = \"I forgot my password\"\n",
    "        search_embedding = encoder.encode(similar_query).tolist()\n",
    "        \n",
    "        results = qdrant.search(\n",
    "            collection_name=collection_name,\n",
    "            query_vector=search_embedding,\n",
    "            limit=1\n",
    "        )\n",
    "        \n",
    "        if results and results[0].score > 0.7:\n",
    "            print(f\"\\n✓ Semantic search works!\")\n",
    "            print(f\"  Query: '{similar_query}'\")\n",
    "            print(f\"  Matched: '{results[0].payload['query']}' (score={results[0].score:.2f})\")\n",
    "            print(f\"  Cached response: {results[0].payload['response']}\")\n",
    "        \n",
    "        print(\"\\n→ Qdrant in-memory works for semantic caching!\")\n",
    "        print(\"  This can replace our custom SemanticCache if preferred.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error: {e}\")\n",
    "        print(\"  Falling back to custom SemanticCache (Demo 5)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "SEMANTIC CACHING: Options Summary\n",
      "=================================================================\n",
      "\n",
      "┌────────────────────┬─────────────┬──────────────┬─────────────────────┐\n",
      "│ Option             │ Semantic?   │ External Dep │ Best For            │\n",
      "├────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
      "│ LiteLLM local      │ No (exact)  │ None         │ Dev/testing         │\n",
      "│ LiteLLM + Redis    │ No (exact)  │ Redis server │ Production, fast    │\n",
      "│ LiteLLM + Qdrant   │ Yes         │ OpenAI API   │ Production, cloud   │\n",
      "│ Custom SemanticCache│ Yes        │ sentence-tx  │ Notebook, offline   │\n",
      "│ Custom SISO        │ Yes         │ sentence-tx  │ Notebook, clustered │\n",
      "│ Qdrant in-memory   │ Yes         │ qdrant-client│ Notebook, efficient │\n",
      "└────────────────────┴─────────────┴──────────────┴─────────────────────┘\n",
      "\n",
      "FOR THIS NOTEBOOK (offline, no external APIs):\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  ✓ Custom SemanticCache (Demo 5) - simple, educational\n",
      "  ✓ Custom SISO (Demo 7) - shows clustering concepts  \n",
      "  ✓ Qdrant in-memory (if installed) - production-like\n",
      "\n",
      "FOR PRODUCTION:\n",
      "─────────────────────────────────────────────────────────────────\n",
      "  → LiteLLM + Redis for exact-match (simple, fast)\n",
      "  → LiteLLM + Qdrant for semantic (requires OpenAI embeddings)\n",
      "  → GPTCache if using openai<2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Caching Options Summary\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"SEMANTIC CACHING: Options Summary\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "print(\"\"\"\n",
    "┌────────────────────┬─────────────┬──────────────┬─────────────────────┐\n",
    "│ Option             │ Semantic?   │ External Dep │ Best For            │\n",
    "├────────────────────┼─────────────┼──────────────┼─────────────────────┤\n",
    "│ LiteLLM local      │ No (exact)  │ None         │ Dev/testing         │\n",
    "│ LiteLLM + Redis    │ No (exact)  │ Redis server │ Production, fast    │\n",
    "│ LiteLLM + Qdrant   │ Yes         │ OpenAI API   │ Production, cloud   │\n",
    "│ Custom SemanticCache│ Yes        │ sentence-tx  │ Notebook, offline   │\n",
    "│ Custom SISO        │ Yes         │ sentence-tx  │ Notebook, clustered │\n",
    "│ Qdrant in-memory   │ Yes         │ qdrant-client│ Notebook, efficient │\n",
    "└────────────────────┴─────────────┴──────────────┴─────────────────────┘\n",
    "\n",
    "FOR THIS NOTEBOOK (offline, no external APIs):\n",
    "─────────────────────────────────────────────────────────────────\n",
    "  ✓ Custom SemanticCache (Demo 5) - simple, educational\n",
    "  ✓ Custom SISO (Demo 7) - shows clustering concepts  \n",
    "  ✓ Qdrant in-memory (if installed) - production-like\n",
    "\n",
    "FOR PRODUCTION:\n",
    "─────────────────────────────────────────────────────────────────\n",
    "  → LiteLLM + Redis for exact-match (simple, fast)\n",
    "  → LiteLLM + Qdrant for semantic (requires OpenAI embeddings)\n",
    "  → GPTCache if using openai<2.0\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guardrails-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
