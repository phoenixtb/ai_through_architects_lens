{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a03d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Selection Advisor\n",
      "============================================================\n",
      "\n",
      "MODEL RECOMMENDATION: Support Ticket Classifier\n",
      "============================================================\n",
      "\n",
      "Task Profile:\n",
      "  Complexity:   simple\n",
      "  Sensitivity:  sensitive\n",
      "  Latency:      realtime\n",
      "  Daily Volume: 50,000\n",
      "\n",
      "Recommended: SMALL_OPEN\n",
      "  Examples: Llama 3.1 8B (self-hosted), Mistral 7B\n",
      "\n",
      "Alternatives: small_closed\n",
      "\n",
      "Reasoning:\n",
      "  • SENSITIVE data → prefer self-hosted or regional provider\n",
      "  • Simple task → small open model ideal\n",
      "\n",
      "Warnings:\n",
      "  ⚠ If using cloud API, ensure GDPR-compliant DPA in place\n",
      "\n",
      "Estimated Monthly Cost: €150\n",
      "  (Based on €0.10 per 1K requests)\n",
      "\n",
      "MODEL RECOMMENDATION: Contract Risk Analyzer\n",
      "============================================================\n",
      "\n",
      "Task Profile:\n",
      "  Complexity:   complex\n",
      "  Sensitivity:  restricted\n",
      "  Latency:      batch\n",
      "  Daily Volume: 500\n",
      "\n",
      "Recommended: SELF_HOSTED\n",
      "  Examples: Llama 3.1 70B, Mixtral 8x22B, Qwen 72B\n",
      "\n",
      "Reasoning:\n",
      "  • RESTRICTED data → must self-host (no external APIs)\n",
      "  • Complex task → larger self-hosted model needed\n",
      "\n",
      "Warnings:\n",
      "  ⚠ 70B+ models require significant GPU infrastructure\n",
      "\n",
      "Estimated Monthly Cost: €8\n",
      "  (Based on €0.50 per 1K requests)\n",
      "\n",
      "MODEL RECOMMENDATION: Product Q&A Chatbot\n",
      "============================================================\n",
      "\n",
      "Task Profile:\n",
      "  Complexity:   standard\n",
      "  Sensitivity:  public\n",
      "  Latency:      interactive\n",
      "  Daily Volume: 100,000\n",
      "\n",
      "Recommended: MID_CLOSED\n",
      "  Examples: GPT-4o, Claude Sonnet\n",
      "\n",
      "Alternatives: small_closed\n",
      "\n",
      "Reasoning:\n",
      "  • Standard task → mid-tier model recommended\n",
      "  • High volume (100,000/day) → consider routing\n",
      "\n",
      "Warnings:\n",
      "  ⚠ Route simple queries to smaller model for 50-70% cost reduction\n",
      "\n",
      "Estimated Monthly Cost: €18,000\n",
      "  (Based on €6.00 per 1K requests)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "\n",
    "class TaskComplexity(Enum):\n",
    "    SIMPLE = \"simple\"      # Classification, extraction, formatting\n",
    "    STANDARD = \"standard\"  # Summarization, Q&A, basic generation\n",
    "    COMPLEX = \"complex\"    # Multi-step reasoning, analysis, debugging\n",
    "    AGENTIC = \"agentic\"    # Tool use, planning, self-correction\n",
    "\n",
    "class DataSensitivity(Enum):\n",
    "    PUBLIC = \"public\"           # No restrictions\n",
    "    INTERNAL = \"internal\"       # Business data, contractual API use OK\n",
    "    SENSITIVE = \"sensitive\"     # PII, regulated—regional restrictions apply\n",
    "    RESTRICTED = \"restricted\"   # Cannot leave your infrastructure\n",
    "\n",
    "class LatencyTier(Enum):\n",
    "    REALTIME = \"realtime\"       # < 500ms end-to-end\n",
    "    INTERACTIVE = \"interactive\" # < 2s end-to-end\n",
    "    BATCH = \"batch\"             # Minutes acceptable\n",
    "\n",
    "class ModelClass(Enum):\n",
    "    \"\"\"Model classes representing capability/deployment combinations.\"\"\"\n",
    "    SMALL_OPEN = \"small_open\"           # Llama 8B, Mistral 7B, Phi-3\n",
    "    SMALL_CLOSED = \"small_closed\"       # GPT-4o-mini, Claude Haiku\n",
    "    MID_OPEN = \"mid_open\"               # Llama 70B, Mixtral 8x22B\n",
    "    MID_CLOSED = \"mid_closed\"           # GPT-4o, Claude Sonnet\n",
    "    FRONTIER = \"frontier\"               # Claude Opus, GPT-4.5\n",
    "    SELF_HOSTED = \"self_hosted\"         # Any model, your infrastructure\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TaskProfile:\n",
    "    \"\"\"\n",
    "    Encodes the four dimensions that drive model selection.\n",
    "    \n",
    "    Use this to characterize any LLM task before choosing a model.\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    complexity: TaskComplexity\n",
    "    sensitivity: DataSensitivity\n",
    "    latency: LatencyTier\n",
    "    daily_volume: int\n",
    "    \n",
    "    def requires_self_hosting(self) -> bool:\n",
    "        \"\"\"Restricted data mandates self-hosting.\"\"\"\n",
    "        return self.sensitivity == DataSensitivity.RESTRICTED\n",
    "    \n",
    "    def prefers_self_hosting(self) -> bool:\n",
    "        \"\"\"Sensitive data strongly prefers self-hosting.\"\"\"\n",
    "        return self.sensitivity in (DataSensitivity.SENSITIVE, \n",
    "                                     DataSensitivity.RESTRICTED)\n",
    "    \n",
    "    def is_cost_sensitive(self, threshold: int = 10000) -> bool:\n",
    "        \"\"\"High volume makes per-request cost significant.\"\"\"\n",
    "        return self.daily_volume >= threshold\n",
    "    \n",
    "    def is_latency_constrained(self) -> bool:\n",
    "        \"\"\"Real-time requirements limit model size.\"\"\"\n",
    "        return self.latency == LatencyTier.REALTIME\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelRecommendation:\n",
    "    \"\"\"A model recommendation with reasoning and trade-offs.\"\"\"\n",
    "    primary: ModelClass\n",
    "    primary_examples: List[str]\n",
    "    alternatives: List[ModelClass] = field(default_factory=list)\n",
    "    reasoning: List[str] = field(default_factory=list)\n",
    "    warnings: List[str] = field(default_factory=list)\n",
    "    estimated_cost_per_1k: float = 0.0  # € per 1000 requests (2K tokens avg)\n",
    "\n",
    "\n",
    "def recommend_model(profile: TaskProfile) -> ModelRecommendation:\n",
    "    \"\"\"\n",
    "    Recommend a model class based on task profile.\n",
    "    \n",
    "    Implements the decision logic as executable code.\n",
    "    The reasoning list explains each constraint applied.\n",
    "    \"\"\"\n",
    "    reasoning = []\n",
    "    warnings = []\n",
    "    alternatives = []\n",
    "    \n",
    "    # Hard constraint: restricted data must self-host\n",
    "    if profile.requires_self_hosting():\n",
    "        reasoning.append(\"RESTRICTED data → must self-host (no external APIs)\")\n",
    "        \n",
    "        if profile.complexity in (TaskComplexity.SIMPLE, TaskComplexity.STANDARD):\n",
    "            examples = [\"Llama 3.1 8B\", \"Mistral 7B\", \"Phi-3\"]\n",
    "            reasoning.append(\"Simple/standard task → small model sufficient\")\n",
    "            cost = 0.10  # Rough compute estimate\n",
    "        else:\n",
    "            examples = [\"Llama 3.1 70B\", \"Mixtral 8x22B\", \"Qwen 72B\"]\n",
    "            reasoning.append(\"Complex task → larger self-hosted model needed\")\n",
    "            warnings.append(\"70B+ models require significant GPU infrastructure\")\n",
    "            cost = 0.50\n",
    "        \n",
    "        return ModelRecommendation(\n",
    "            primary=ModelClass.SELF_HOSTED,\n",
    "            primary_examples=examples,\n",
    "            reasoning=reasoning,\n",
    "            warnings=warnings,\n",
    "            estimated_cost_per_1k=cost\n",
    "        )\n",
    "    \n",
    "    # Soft constraint: sensitive data prefers self-hosting\n",
    "    if profile.prefers_self_hosting():\n",
    "        reasoning.append(\"SENSITIVE data → prefer self-hosted or regional provider\")\n",
    "        \n",
    "        if profile.complexity == TaskComplexity.SIMPLE:\n",
    "            return ModelRecommendation(\n",
    "                primary=ModelClass.SMALL_OPEN,\n",
    "                primary_examples=[\"Llama 3.1 8B (self-hosted)\", \"Mistral 7B\"],\n",
    "                alternatives=[ModelClass.SMALL_CLOSED],\n",
    "                reasoning=reasoning + [\"Simple task → small open model ideal\"],\n",
    "                warnings=[\"If using cloud API, ensure GDPR-compliant DPA in place\"],\n",
    "                estimated_cost_per_1k=0.10\n",
    "            )\n",
    "        elif profile.complexity == TaskComplexity.STANDARD:\n",
    "            return ModelRecommendation(\n",
    "                primary=ModelClass.MID_OPEN,\n",
    "                primary_examples=[\"Llama 3.1 70B\", \"Mixtral 8x22B\"],\n",
    "                alternatives=[ModelClass.MID_CLOSED],\n",
    "                reasoning=reasoning + [\"Standard task → mid-tier open model\"],\n",
    "                warnings=[\"Cloud APIs (GPT-4o, Sonnet) viable with proper DPA\"],\n",
    "                estimated_cost_per_1k=0.50\n",
    "            )\n",
    "        else:  # COMPLEX or AGENTIC\n",
    "            reasoning.append(\"Complex task with sensitive data → trade-off required\")\n",
    "            warnings.append(\"Best open models lag frontier by ~6 months on reasoning\")\n",
    "            warnings.append(\"Consider: Can you decompose into sensitive + non-sensitive parts?\")\n",
    "            return ModelRecommendation(\n",
    "                primary=ModelClass.MID_OPEN,\n",
    "                primary_examples=[\"Llama 3.1 70B\", \"Mixtral 8x22B\"],\n",
    "                alternatives=[ModelClass.MID_CLOSED, ModelClass.FRONTIER],\n",
    "                reasoning=reasoning,\n",
    "                warnings=warnings,\n",
    "                estimated_cost_per_1k=0.50\n",
    "            )\n",
    "    \n",
    "    # No sovereignty constraints—optimize for capability and cost\n",
    "    \n",
    "    # Simple tasks: small models suffice\n",
    "    if profile.complexity == TaskComplexity.SIMPLE:\n",
    "        reasoning.append(\"Simple task → small model sufficient\")\n",
    "        \n",
    "        if profile.is_cost_sensitive():\n",
    "            reasoning.append(f\"High volume ({profile.daily_volume:,}/day) → optimize cost\")\n",
    "            return ModelRecommendation(\n",
    "                primary=ModelClass.SMALL_CLOSED,\n",
    "                primary_examples=[\"GPT-4o-mini\", \"Claude Haiku\"],\n",
    "                alternatives=[ModelClass.SMALL_OPEN],\n",
    "                reasoning=reasoning,\n",
    "                estimated_cost_per_1k=0.30\n",
    "            )\n",
    "        else:\n",
    "            return ModelRecommendation(\n",
    "                primary=ModelClass.SMALL_CLOSED,\n",
    "                primary_examples=[\"GPT-4o-mini\", \"Claude Haiku\"],\n",
    "                reasoning=reasoning,\n",
    "                estimated_cost_per_1k=0.30\n",
    "            )\n",
    "    \n",
    "    # Standard tasks: mid-tier models\n",
    "    if profile.complexity == TaskComplexity.STANDARD:\n",
    "        reasoning.append(\"Standard task → mid-tier model recommended\")\n",
    "        \n",
    "        if profile.is_latency_constrained():\n",
    "            reasoning.append(\"Real-time latency → prefer optimized inference\")\n",
    "            warnings.append(\"GPT-4o and Sonnet typically 200-500ms; may need caching\")\n",
    "        \n",
    "        if profile.is_cost_sensitive():\n",
    "            reasoning.append(f\"High volume ({profile.daily_volume:,}/day) → consider routing\")\n",
    "            alternatives.append(ModelClass.SMALL_CLOSED)\n",
    "            warnings.append(\"Route simple queries to smaller model for 50-70% cost reduction\")\n",
    "        \n",
    "        return ModelRecommendation(\n",
    "            primary=ModelClass.MID_CLOSED,\n",
    "            primary_examples=[\"GPT-4o\", \"Claude Sonnet\"],\n",
    "            alternatives=alternatives,\n",
    "            reasoning=reasoning,\n",
    "            warnings=warnings,\n",
    "            estimated_cost_per_1k=6.00\n",
    "        )\n",
    "    \n",
    "    # Complex reasoning: frontier models\n",
    "    if profile.complexity == TaskComplexity.COMPLEX:\n",
    "        reasoning.append(\"Complex reasoning → frontier model recommended\")\n",
    "        \n",
    "        if profile.is_latency_constrained():\n",
    "            warnings.append(\"Frontier models may exceed 500ms on complex prompts\")\n",
    "            warnings.append(\"Consider mid-tier for latency-critical paths\")\n",
    "            alternatives.append(ModelClass.MID_CLOSED)\n",
    "        \n",
    "        if profile.is_cost_sensitive():\n",
    "            warnings.append(f\"At {profile.daily_volume:,}/day, frontier costs add up fast\")\n",
    "            warnings.append(\"Implement routing: frontier for hard queries, mid-tier for rest\")\n",
    "            alternatives.append(ModelClass.MID_CLOSED)\n",
    "        \n",
    "        return ModelRecommendation(\n",
    "            primary=ModelClass.FRONTIER,\n",
    "            primary_examples=[\"Claude Opus\", \"GPT-4.5\", \"Gemini Ultra\"],\n",
    "            alternatives=alternatives,\n",
    "            reasoning=reasoning,\n",
    "            warnings=warnings,\n",
    "            estimated_cost_per_1k=30.00\n",
    "        )\n",
    "    \n",
    "    # Agentic tasks: tool-use optimized models\n",
    "    reasoning.append(\"Agentic task → models optimized for tool use\")\n",
    "    reasoning.append(\"Claude Sonnet and GPT-4o excel at structured tool calling\")\n",
    "    \n",
    "    if profile.is_cost_sensitive():\n",
    "        warnings.append(\"Agentic loops multiply token usage—monitor closely\")\n",
    "    \n",
    "    return ModelRecommendation(\n",
    "        primary=ModelClass.MID_CLOSED,\n",
    "        primary_examples=[\"Claude Sonnet\", \"GPT-4o\"],\n",
    "        alternatives=[ModelClass.FRONTIER],\n",
    "        reasoning=reasoning + [\"Mid-tier often matches frontier on tool use\"],\n",
    "        warnings=warnings,\n",
    "        estimated_cost_per_1k=6.00\n",
    "    )\n",
    "\n",
    "\n",
    "def format_recommendation(profile: TaskProfile, rec: ModelRecommendation) -> str:\n",
    "    \"\"\"Format recommendation as readable output.\"\"\"\n",
    "    lines = [\n",
    "        f\"MODEL RECOMMENDATION: {profile.name}\",\n",
    "        \"=\" * 60,\n",
    "        \"\",\n",
    "        f\"Task Profile:\",\n",
    "        f\"  Complexity:   {profile.complexity.value}\",\n",
    "        f\"  Sensitivity:  {profile.sensitivity.value}\",\n",
    "        f\"  Latency:      {profile.latency.value}\",\n",
    "        f\"  Daily Volume: {profile.daily_volume:,}\",\n",
    "        \"\",\n",
    "        f\"Recommended: {rec.primary.value.upper()}\",\n",
    "        f\"  Examples: {', '.join(rec.primary_examples)}\",\n",
    "        \"\",\n",
    "    ]\n",
    "    \n",
    "    if rec.alternatives:\n",
    "        alt_names = [a.value for a in rec.alternatives]\n",
    "        lines.append(f\"Alternatives: {', '.join(alt_names)}\")\n",
    "        lines.append(\"\")\n",
    "    \n",
    "    lines.append(\"Reasoning:\")\n",
    "    for r in rec.reasoning:\n",
    "        lines.append(f\"  • {r}\")\n",
    "    \n",
    "    if rec.warnings:\n",
    "        lines.append(\"\")\n",
    "        lines.append(\"Warnings:\")\n",
    "        for w in rec.warnings:\n",
    "            lines.append(f\"  ⚠ {w}\")\n",
    "    \n",
    "    lines.append(\"\")\n",
    "    monthly_cost = rec.estimated_cost_per_1k * (profile.daily_volume * 30 / 1000)\n",
    "    lines.append(f\"Estimated Monthly Cost: €{monthly_cost:,.0f}\")\n",
    "    lines.append(f\"  (Based on €{rec.estimated_cost_per_1k:.2f} per 1K requests)\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: Model selection for real scenarios\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Model Selection Advisor\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Scenario 1: Support ticket classifier with PII\n",
    "ticket_classifier = TaskProfile(\n",
    "    name=\"Support Ticket Classifier\",\n",
    "    complexity=TaskComplexity.SIMPLE,\n",
    "    sensitivity=DataSensitivity.SENSITIVE,\n",
    "    latency=LatencyTier.REALTIME,\n",
    "    daily_volume=50000\n",
    ")\n",
    "rec1 = recommend_model(ticket_classifier)\n",
    "print(format_recommendation(ticket_classifier, rec1))\n",
    "print()\n",
    "\n",
    "# Scenario 2: Contract analysis for legal team\n",
    "contract_analyzer = TaskProfile(\n",
    "    name=\"Contract Risk Analyzer\", \n",
    "    complexity=TaskComplexity.COMPLEX,\n",
    "    sensitivity=DataSensitivity.RESTRICTED,\n",
    "    latency=LatencyTier.BATCH,\n",
    "    daily_volume=500\n",
    ")\n",
    "rec2 = recommend_model(contract_analyzer)\n",
    "print(format_recommendation(contract_analyzer, rec2))\n",
    "print()\n",
    "\n",
    "# Scenario 3: Customer-facing chatbot\n",
    "chatbot = TaskProfile(\n",
    "    name=\"Product Q&A Chatbot\",\n",
    "    complexity=TaskComplexity.STANDARD,\n",
    "    sensitivity=DataSensitivity.PUBLIC,\n",
    "    latency=LatencyTier.INTERACTIVE,\n",
    "    daily_volume=100000\n",
    ")\n",
    "rec3 = recommend_model(chatbot)\n",
    "print(format_recommendation(chatbot, rec3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7ec84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Validation Framework\n",
      "============================================================\n",
      "\n",
      "To validate models on YOUR task:\n",
      "\n",
      "1. BUILD YOUR TEST SET (50-200 examples from production):\n",
      "\n",
      "   test_cases = [\n",
      "       {\"input\": \"Where is my order #12345?\", \"expected\": \"order_status\"},\n",
      "       {\"input\": \"I want a refund\", \"expected\": \"refund_request\"},\n",
      "       {\"input\": \"Your product broke my dishwasher\", \"expected\": \"complaint\"},\n",
      "       # Include edge cases that have caused problems\n",
      "   ]\n",
      "\n",
      "2. DEFINE YOUR EVALUATOR:\n",
      "\n",
      "   # For classification:\n",
      "   def evaluator(actual: str, expected: str) -> float:\n",
      "       return 1.0 if expected.lower() in actual.lower() else 0.0\n",
      "\n",
      "   # For generation (using embedding similarity):\n",
      "   def evaluator(actual: str, expected: str) -> float:\n",
      "       return cosine_similarity(embed(actual), embed(expected))\n",
      "\n",
      "3. DEFINE YOUR REQUIREMENTS:\n",
      "\n",
      "   requirements = {\n",
      "       \"min_accuracy\": 0.92,        # 92% accuracy minimum\n",
      "       \"max_latency_p95_ms\": 500,   # 500ms P95 latency\n",
      "       \"max_cost_per_1k\": 10.0      # €10 per 1000 requests\n",
      "   }\n",
      "\n",
      "4. SET UP MODEL CANDIDATES:\n",
      "\n",
      "   models = {\n",
      "       \"gpt-4o-mini\": (\n",
      "           lambda x: call_openai(x, model=\"gpt-4o-mini\"),\n",
      "           0.00015  # cost per 1K tokens\n",
      "       ),\n",
      "       \"claude-haiku\": (\n",
      "           lambda x: call_anthropic(x, model=\"claude-3-haiku\"),\n",
      "           0.00025\n",
      "       ),\n",
      "       \"llama-8b-local\": (\n",
      "           lambda x: call_local(x, model=\"llama-8b\"),\n",
      "           0.00005  # compute cost estimate\n",
      "       ),\n",
      "   }\n",
      "\n",
      "5. RUN COMPARISON:\n",
      "\n",
      "   results = compare_models(models, test_cases, evaluator, requirements)\n",
      "\n",
      "   for r in results:\n",
      "       status = \"✓\" if r.meets_requirements(**requirements) else \"✗\"\n",
      "       print(f\"{status} {r.model_name}: {r.accuracy:.1%} accuracy, \"\n",
      "             f\"{r.latency_p95_ms:.0f}ms P95, €{r.cost_per_1k_requests:.2f}/1K\")\n",
      "\n",
      "The model that meets all requirements at lowest cost wins.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict, Callable, Any\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class BenchmarkResult:\n",
    "    \"\"\"Results from benchmarking a model on your task.\"\"\"\n",
    "    model_name: str\n",
    "    accuracy: float\n",
    "    latency_p50_ms: float\n",
    "    latency_p95_ms: float\n",
    "    cost_per_1k_requests: float\n",
    "    \n",
    "    def meets_requirements(\n",
    "        self, \n",
    "        min_accuracy: float, \n",
    "        max_latency_p95_ms: float,\n",
    "        max_cost_per_1k: float\n",
    "    ) -> bool:\n",
    "        \"\"\"Check if this model meets all requirements.\"\"\"\n",
    "        return (\n",
    "            self.accuracy >= min_accuracy and\n",
    "            self.latency_p95_ms <= max_latency_p95_ms and\n",
    "            self.cost_per_1k_requests <= max_cost_per_1k\n",
    "        )\n",
    "\n",
    "\n",
    "def benchmark_model(\n",
    "    model_fn: Callable[[str], str],\n",
    "    test_cases: List[Dict[str, str]],\n",
    "    evaluator: Callable[[str, str], float],\n",
    "    cost_per_1k_tokens: float,\n",
    "    avg_tokens_per_request: int = 2000\n",
    ") -> BenchmarkResult:\n",
    "    \"\"\"\n",
    "    Benchmark a single model on your test cases.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_fn : Callable\n",
    "        Function that takes input string, returns output string\n",
    "    test_cases : List[Dict]\n",
    "        Each dict has 'input' and 'expected' keys\n",
    "    evaluator : Callable\n",
    "        Function(actual, expected) -> score (0.0 to 1.0)\n",
    "    cost_per_1k_tokens : float\n",
    "        Model's price per 1000 tokens\n",
    "    avg_tokens_per_request : int\n",
    "        Expected tokens per request for cost calculation\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    latencies = []\n",
    "    \n",
    "    for case in test_cases:\n",
    "        start = time.perf_counter()\n",
    "        actual = model_fn(case['input'])\n",
    "        latency_ms = (time.perf_counter() - start) * 1000\n",
    "        \n",
    "        score = evaluator(actual, case['expected'])\n",
    "        scores.append(score)\n",
    "        latencies.append(latency_ms)\n",
    "    \n",
    "    latencies.sort()\n",
    "    n = len(latencies)\n",
    "    \n",
    "    return BenchmarkResult(\n",
    "        model_name=\"\",  # Set by caller\n",
    "        accuracy=sum(scores) / len(scores),\n",
    "        latency_p50_ms=latencies[n // 2],\n",
    "        latency_p95_ms=latencies[int(n * 0.95)],\n",
    "        cost_per_1k_requests=(avg_tokens_per_request / 1000) * cost_per_1k_tokens * 1000\n",
    "    )\n",
    "\n",
    "\n",
    "def compare_models(\n",
    "    models: Dict[str, tuple],  # name -> (model_fn, cost_per_1k_tokens)\n",
    "    test_cases: List[Dict[str, str]],\n",
    "    evaluator: Callable[[str, str], float],\n",
    "    requirements: Dict[str, float]  # min_accuracy, max_latency_p95_ms, max_cost_per_1k\n",
    ") -> List[BenchmarkResult]:\n",
    "    \"\"\"\n",
    "    Benchmark multiple models and filter by requirements.\n",
    "    \n",
    "    Returns results sorted by accuracy (highest first),\n",
    "    with models not meeting requirements flagged.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, (model_fn, cost) in models.items():\n",
    "        result = benchmark_model(model_fn, test_cases, evaluator, cost)\n",
    "        result.model_name = name\n",
    "        results.append(result)\n",
    "    \n",
    "    # Sort by accuracy descending\n",
    "    results.sort(key=lambda r: r.accuracy, reverse=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: How to set up your benchmark\n",
    "# =============================================================================\n",
    "\n",
    "print()\n",
    "print(\"Model Validation Framework\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "To validate models on YOUR task:\n",
    "\n",
    "1. BUILD YOUR TEST SET (50-200 examples from production):\n",
    "\n",
    "   test_cases = [\n",
    "       {\"input\": \"Where is my order #12345?\", \"expected\": \"order_status\"},\n",
    "       {\"input\": \"I want a refund\", \"expected\": \"refund_request\"},\n",
    "       {\"input\": \"Your product broke my dishwasher\", \"expected\": \"complaint\"},\n",
    "       # Include edge cases that have caused problems\n",
    "   ]\n",
    "\n",
    "2. DEFINE YOUR EVALUATOR:\n",
    "\n",
    "   # For classification:\n",
    "   def evaluator(actual: str, expected: str) -> float:\n",
    "       return 1.0 if expected.lower() in actual.lower() else 0.0\n",
    "   \n",
    "   # For generation (using embedding similarity):\n",
    "   def evaluator(actual: str, expected: str) -> float:\n",
    "       return cosine_similarity(embed(actual), embed(expected))\n",
    "\n",
    "3. DEFINE YOUR REQUIREMENTS:\n",
    "\n",
    "   requirements = {\n",
    "       \"min_accuracy\": 0.92,        # 92% accuracy minimum\n",
    "       \"max_latency_p95_ms\": 500,   # 500ms P95 latency\n",
    "       \"max_cost_per_1k\": 10.0      # €10 per 1000 requests\n",
    "   }\n",
    "\n",
    "4. SET UP MODEL CANDIDATES:\n",
    "\n",
    "   models = {\n",
    "       \"gpt-4o-mini\": (\n",
    "           lambda x: call_openai(x, model=\"gpt-4o-mini\"),\n",
    "           0.00015  # cost per 1K tokens\n",
    "       ),\n",
    "       \"claude-haiku\": (\n",
    "           lambda x: call_anthropic(x, model=\"claude-3-haiku\"),\n",
    "           0.00025\n",
    "       ),\n",
    "       \"llama-8b-local\": (\n",
    "           lambda x: call_local(x, model=\"llama-8b\"),\n",
    "           0.00005  # compute cost estimate\n",
    "       ),\n",
    "   }\n",
    "\n",
    "5. RUN COMPARISON:\n",
    "\n",
    "   results = compare_models(models, test_cases, evaluator, requirements)\n",
    "   \n",
    "   for r in results:\n",
    "       status = \"✓\" if r.meets_requirements(**requirements) else \"✗\"\n",
    "       print(f\"{status} {r.model_name}: {r.accuracy:.1%} accuracy, \"\n",
    "             f\"{r.latency_p95_ms:.0f}ms P95, €{r.cost_per_1k_requests:.2f}/1K\")\n",
    "\n",
    "The model that meets all requirements at lowest cost wins.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8209b06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision Pipeline Cost Analysis: Invoice Processing\n",
      "=======================================================\n",
      "Daily token consumption:    18,000,000\n",
      "Daily cost:               €      45.00\n",
      "Monthly cost:             €   1,350.00\n",
      "Cost vs text-only:                 6.0×\n",
      "\n",
      "Decision guidance:\n",
      "  • If OCR + text extraction achieves 95%+ accuracy → use text-only\n",
      "  • If documents have complex layouts, tables → vision may be worth 4×\n",
      "  • Consider hybrid: OCR first, vision fallback for low-confidence cases\n"
     ]
    }
   ],
   "source": [
    "def estimate_vision_costs(\n",
    "    images_per_day: int,\n",
    "    tokens_per_image: int = 1500,  # Typical for 1024x1024\n",
    "    text_tokens_per_request: int = 500,\n",
    "    price_per_1k_input: float = 0.0025  # GPT-4o pricing\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Estimate costs for a vision-enabled pipeline.\n",
    "    \n",
    "    Vision tokens typically cost the same as text tokens,\n",
    "    but images consume many more tokens than equivalent text.\n",
    "    \"\"\"\n",
    "    daily_vision_tokens = images_per_day * tokens_per_image\n",
    "    daily_text_tokens = images_per_day * text_tokens_per_request\n",
    "    daily_total_tokens = daily_vision_tokens + daily_text_tokens\n",
    "    \n",
    "    daily_cost = (daily_total_tokens / 1000) * price_per_1k_input\n",
    "    monthly_cost = daily_cost * 30\n",
    "    \n",
    "    # Compare to text-only alternative\n",
    "    text_only_daily = (images_per_day * text_tokens_per_request / 1000) * price_per_1k_input\n",
    "    vision_premium = daily_cost / text_only_daily if text_only_daily > 0 else float('inf')\n",
    "    \n",
    "    return {\n",
    "        'daily_tokens': daily_total_tokens,\n",
    "        'daily_cost': round(daily_cost, 2),\n",
    "        'monthly_cost': round(monthly_cost, 2),\n",
    "        'vision_cost_multiplier': round(vision_premium, 1)\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Driver: Vision cost analysis for document processing\n",
    "# =============================================================================\n",
    "\n",
    "# Scenario: Invoice processing system\n",
    "invoice_processing = estimate_vision_costs(\n",
    "    images_per_day=10000,\n",
    "    tokens_per_image=1500,\n",
    "    text_tokens_per_request=300,\n",
    "    price_per_1k_input=0.0025\n",
    ")\n",
    "\n",
    "print(\"Vision Pipeline Cost Analysis: Invoice Processing\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Daily token consumption:  {invoice_processing['daily_tokens']:>12,}\")\n",
    "print(f\"Daily cost:               €{invoice_processing['daily_cost']:>11,.2f}\")\n",
    "print(f\"Monthly cost:             €{invoice_processing['monthly_cost']:>11,.2f}\")\n",
    "print(f\"Cost vs text-only:        {invoice_processing['vision_cost_multiplier']:>12}×\")\n",
    "print()\n",
    "print(\"Decision guidance:\")\n",
    "print(\"  • If OCR + text extraction achieves 95%+ accuracy → use text-only\")\n",
    "print(\"  • If documents have complex layouts, tables → vision may be worth 4×\")\n",
    "print(\"  • Consider hybrid: OCR first, vision fallback for low-confidence cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917d317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Output with Instructor\n",
      "=======================================================\n",
      "\n",
      "Setup (one-time):\n",
      "    from openai import OpenAI\n",
      "    import instructor\n",
      "\n",
      "    client = instructor.from_openai(OpenAI())\n",
      "    # Or: instructor.from_anthropic(Anthropic())\n",
      "    # Or: instructor.from_provider(\"openai/gpt-4o-mini\")\n",
      "\n",
      "Usage:\n",
      "    raw_message = '''\n",
      "    I've been trying to reset my password for 3 days now!\n",
      "    The mobile app keeps crashing when I tap \"Forgot Password\".\n",
      "    This is ridiculous - I need access to my account for work.\n",
      "    Order #12345 is stuck and I can't track it.\n",
      "    '''\n",
      "\n",
      "    ticket = extract_ticket_info(\n",
      "        raw_text=raw_message,\n",
      "        customer_id=\"CUST-98765\",\n",
      "        client=client\n",
      "    )\n",
      "\n",
      "    print(ticket.model_dump_json(indent=2))\n",
      "\n",
      "Expected output:\n",
      "    {\n",
      "      \"customer_id\": \"CUST-98765\",\n",
      "      \"category\": \"authentication\",\n",
      "      \"priority\": \"high\",\n",
      "      \"summary\": \"Password reset failing on mobile app, blocking order tracking\",\n",
      "      \"entities_mentioned\": [\"mobile app\", \"Forgot Password\", \"Order #12345\"],\n",
      "      \"sentiment\": -0.8\n",
      "    }\n",
      "\n",
      "Key benefits:\n",
      "    • Type safety: IDE autocompletion, static analysis\n",
      "    • Validation: Pydantic validators catch bad data\n",
      "    • Retries: Failed validation → re-prompt with error context\n",
      "    • Portability: Same code works across 15+ providers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Structured Output with Instructor\n",
    "# pip install instructor pydantic\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from enum import Enum\n",
    "\n",
    "class Priority(str, Enum):\n",
    "    HIGH = \"high\"\n",
    "    MEDIUM = \"medium\"\n",
    "    LOW = \"low\"\n",
    "\n",
    "class SupportTicket(BaseModel):\n",
    "    \"\"\"Schema for structured extraction - Pydantic does the heavy lifting.\"\"\"\n",
    "    category: str = Field(description=\"Issue category\")\n",
    "    priority: Priority = Field(description=\"Urgency level\")\n",
    "    summary: str = Field(description=\"One-sentence summary\", max_length=200)\n",
    "    entities: List[str] = Field(default_factory=list, description=\"Products/orders mentioned\")\n",
    "    sentiment: float = Field(ge=-1.0, le=1.0, description=\"Sentiment score\")\n",
    "\n",
    "\n",
    "print(\"Structured Output with Instructor\")\n",
    "print(\"=\" * 55)\n",
    "print(\"\"\"\n",
    "WHAT INSTRUCTOR DOES:\n",
    "  1. Injects your Pydantic schema into the prompt\n",
    "  2. Parses LLM response into typed object\n",
    "  3. On validation failure → re-prompts with error context\n",
    "  4. Returns validated Pydantic object, not raw text\n",
    "\n",
    "RELIABILITY SPECTRUM:\n",
    "  Prompt-only parsing:     ~85% (model adds explanations, breaks JSON)\n",
    "  Instructor:              ~95-99% (auto-retry with validation feedback)\n",
    "  Constrained generation:  ~99.9% (grammar-enforced, for self-hosted)\n",
    "\n",
    "SETUP:\n",
    "  # Cloud APIs\n",
    "  client = instructor.from_openai(OpenAI())\n",
    "  client = instructor.from_anthropic(Anthropic())\n",
    "  \n",
    "  # Local (Ollama)\n",
    "  client = instructor.from_openai(\n",
    "      OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),\n",
    "      mode=instructor.Mode.JSON\n",
    "  )\n",
    "\n",
    "USAGE:\n",
    "  ticket = client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      response_model=SupportTicket,\n",
    "      max_retries=2,\n",
    "      messages=[{\"role\": \"user\", \"content\": raw_message}]\n",
    "  )\n",
    "  # ticket is a SupportTicket object, not a string\n",
    "\n",
    "→ See 1B/demos.ipynb for runnable demo with Ollama\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e08b3898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constrained Generation Decision\n",
      "=======================================================\n",
      "\n",
      "Choose your approach:\n",
      "\n",
      "┌─────────────────────┬────────────────┬──────────────────┐\n",
      "│ Approach            │ Reliability    │ Best For         │\n",
      "├─────────────────────┼────────────────┼──────────────────┤\n",
      "│ Prompt + parsing    │ ~85%           │ Prototyping      │\n",
      "│ Instructor          │ ~95-99%        │ Cloud APIs       │\n",
      "│ Outlines/guidance   │ ~99.9%         │ Self-hosted      │\n",
      "│ Native JSON mode    │ ~95%           │ Simple schemas   │\n",
      "└─────────────────────┴────────────────┴──────────────────┘\n",
      "\n",
      "For most production systems, Instructor is the sweet spot:\n",
      "high reliability, great DX, works everywhere.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conceptual example - Outlines library for constrained generation\n",
    "# pip install outlines\n",
    "\n",
    "\"\"\"\n",
    "Outlines constrains token generation to match a grammar.\n",
    "The model literally cannot produce invalid JSON.\n",
    "\n",
    "from outlines import models, generate\n",
    "import outlines\n",
    "\n",
    "model = models.transformers(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "# Define schema\n",
    "schema = '''\n",
    "{\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"category\": {\"type\": \"string\", \"enum\": [\"billing\", \"technical\", \"general\"]},\n",
    "        \"priority\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 5}\n",
    "    },\n",
    "    \"required\": [\"category\", \"priority\"]\n",
    "}\n",
    "'''\n",
    "\n",
    "# Generator is constrained to this schema\n",
    "generator = generate.json(model, schema)\n",
    "result = generator(\"Classify this ticket: My payment failed twice\")\n",
    "# result is GUARANTEED to be valid JSON matching schema\n",
    "\n",
    "Use Outlines when:\n",
    "    • Self-hosting models (full control over generation)\n",
    "    • Zero tolerance for malformed output\n",
    "    • High-volume pipelines where retry costs add up\n",
    "    \n",
    "Use Instructor when:\n",
    "    • Using cloud APIs (OpenAI, Anthropic, etc.)\n",
    "    • Need cross-provider portability\n",
    "    • Validation logic is complex (custom Pydantic validators)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Constrained Generation Decision\")\n",
    "print(\"=\" * 55)\n",
    "print(\"\"\"\n",
    "Choose your approach:\n",
    "\n",
    "┌─────────────────────┬────────────────┬──────────────────┐\n",
    "│ Approach            │ Reliability    │ Best For         │\n",
    "├─────────────────────┼────────────────┼──────────────────┤\n",
    "│ Prompt + parsing    │ ~85%           │ Prototyping      │\n",
    "│ Instructor          │ ~95-99%        │ Cloud APIs       │\n",
    "│ Outlines/guidance   │ ~99.9%         │ Self-hosted      │\n",
    "│ Native JSON mode    │ ~95%           │ Simple schemas   │\n",
    "└─────────────────────┴────────────────┴──────────────────┘\n",
    "\n",
    "For most production systems, Instructor is the sweet spot:\n",
    "high reliability, great DX, works everywhere.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbd001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
