{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "38a03d85",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Selection Advisor\n",
            "============================================================\n",
            "\n",
            "MODEL RECOMMENDATION: Support Ticket Classifier\n",
            "============================================================\n",
            "\n",
            "Task Profile:\n",
            "  Complexity:   simple\n",
            "  Sensitivity:  sensitive\n",
            "  Latency:      realtime\n",
            "  Daily Volume: 50,000\n",
            "\n",
            "Recommended: SMALL_OPEN\n",
            "  Examples: Llama 3.1 8B (self-hosted), Mistral 7B\n",
            "\n",
            "Alternatives: small_closed\n",
            "\n",
            "Reasoning:\n",
            "  \u2022 SENSITIVE data \u2192 prefer self-hosted or regional provider\n",
            "  \u2022 Simple task \u2192 small open model ideal\n",
            "\n",
            "Warnings:\n",
            "  \u26a0 If using cloud API, ensure GDPR-compliant DPA in place\n",
            "\n",
            "Estimated Monthly Cost: \u20ac150\n",
            "  (Based on \u20ac0.10 per 1K requests)\n",
            "\n",
            "MODEL RECOMMENDATION: Contract Risk Analyzer\n",
            "============================================================\n",
            "\n",
            "Task Profile:\n",
            "  Complexity:   complex\n",
            "  Sensitivity:  restricted\n",
            "  Latency:      batch\n",
            "  Daily Volume: 500\n",
            "\n",
            "Recommended: SELF_HOSTED\n",
            "  Examples: Llama 3.1 70B, Mixtral 8x22B, Qwen 72B\n",
            "\n",
            "Reasoning:\n",
            "  \u2022 RESTRICTED data \u2192 must self-host (no external APIs)\n",
            "  \u2022 Complex task \u2192 larger self-hosted model needed\n",
            "\n",
            "Warnings:\n",
            "  \u26a0 70B+ models require significant GPU infrastructure\n",
            "\n",
            "Estimated Monthly Cost: \u20ac8\n",
            "  (Based on \u20ac0.50 per 1K requests)\n",
            "\n",
            "MODEL RECOMMENDATION: Product Q&A Chatbot\n",
            "============================================================\n",
            "\n",
            "Task Profile:\n",
            "  Complexity:   standard\n",
            "  Sensitivity:  public\n",
            "  Latency:      interactive\n",
            "  Daily Volume: 100,000\n",
            "\n",
            "Recommended: MID_CLOSED\n",
            "  Examples: GPT-4o, Claude Sonnet\n",
            "\n",
            "Alternatives: small_closed\n",
            "\n",
            "Reasoning:\n",
            "  \u2022 Standard task \u2192 mid-tier model recommended\n",
            "  \u2022 High volume (100,000/day) \u2192 consider routing\n",
            "\n",
            "Warnings:\n",
            "  \u26a0 Route simple queries to smaller model for 50-70% cost reduction\n",
            "\n",
            "Estimated Monthly Cost: \u20ac18,000\n",
            "  (Based on \u20ac6.00 per 1K requests)\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "from typing import List\n",
        "\n",
        "class TaskComplexity(Enum):\n",
        "    SIMPLE = \"simple\"      # Classification, extraction, formatting\n",
        "    STANDARD = \"standard\"  # Summarization, Q&A, basic generation\n",
        "    COMPLEX = \"complex\"    # Multi-step reasoning, analysis, debugging\n",
        "    AGENTIC = \"agentic\"    # Tool use, planning, self-correction\n",
        "\n",
        "class DataSensitivity(Enum):\n",
        "    PUBLIC = \"public\"           # No restrictions\n",
        "    INTERNAL = \"internal\"       # Business data, contractual API use OK\n",
        "    SENSITIVE = \"sensitive\"     # PII, regulated\u2014regional restrictions apply\n",
        "    RESTRICTED = \"restricted\"   # Cannot leave your infrastructure\n",
        "\n",
        "class LatencyTier(Enum):\n",
        "    REALTIME = \"realtime\"       # < 500ms end-to-end\n",
        "    INTERACTIVE = \"interactive\" # < 2s end-to-end\n",
        "    BATCH = \"batch\"             # Minutes acceptable\n",
        "\n",
        "class ModelClass(Enum):\n",
        "    \"\"\"Model classes representing capability/deployment combinations.\"\"\"\n",
        "    SMALL_OPEN = \"small_open\"           # Llama 8B, Mistral 7B, Phi-3\n",
        "    SMALL_CLOSED = \"small_closed\"       # GPT-4o-mini, Claude Haiku\n",
        "    MID_OPEN = \"mid_open\"               # Llama 70B, Mixtral 8x22B\n",
        "    MID_CLOSED = \"mid_closed\"           # GPT-4o, Claude Sonnet\n",
        "    FRONTIER = \"frontier\"               # Claude Opus, GPT-4.5\n",
        "    SELF_HOSTED = \"self_hosted\"         # Any model, your infrastructure\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TaskProfile:\n",
        "    \"\"\"\n",
        "    Encodes the four dimensions that drive model selection.\n",
        "    \n",
        "    Use this to characterize any LLM task before choosing a model.\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    complexity: TaskComplexity\n",
        "    sensitivity: DataSensitivity\n",
        "    latency: LatencyTier\n",
        "    daily_volume: int\n",
        "    \n",
        "    def requires_self_hosting(self) -> bool:\n",
        "        \"\"\"Restricted data mandates self-hosting.\"\"\"\n",
        "        return self.sensitivity == DataSensitivity.RESTRICTED\n",
        "    \n",
        "    def prefers_self_hosting(self) -> bool:\n",
        "        \"\"\"Sensitive data strongly prefers self-hosting.\"\"\"\n",
        "        return self.sensitivity in (DataSensitivity.SENSITIVE, \n",
        "                                     DataSensitivity.RESTRICTED)\n",
        "    \n",
        "    def is_cost_sensitive(self, threshold: int = 10000) -> bool:\n",
        "        \"\"\"High volume makes per-request cost significant.\"\"\"\n",
        "        return self.daily_volume >= threshold\n",
        "    \n",
        "    def is_latency_constrained(self) -> bool:\n",
        "        \"\"\"Real-time requirements limit model size.\"\"\"\n",
        "        return self.latency == LatencyTier.REALTIME\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelRecommendation:\n",
        "    \"\"\"A model recommendation with reasoning and trade-offs.\"\"\"\n",
        "    primary: ModelClass\n",
        "    primary_examples: List[str]\n",
        "    alternatives: List[ModelClass] = field(default_factory=list)\n",
        "    reasoning: List[str] = field(default_factory=list)\n",
        "    warnings: List[str] = field(default_factory=list)\n",
        "    estimated_cost_per_1k: float = 0.0  # \u20ac per 1000 requests (2K tokens avg)\n",
        "\n",
        "\n",
        "def recommend_model(profile: TaskProfile) -> ModelRecommendation:\n",
        "    \"\"\"\n",
        "    Recommend a model class based on task profile.\n",
        "    \n",
        "    Implements the decision logic as executable code.\n",
        "    The reasoning list explains each constraint applied.\n",
        "    \"\"\"\n",
        "    reasoning = []\n",
        "    warnings = []\n",
        "    alternatives = []\n",
        "    \n",
        "    # Hard constraint: restricted data must self-host\n",
        "    if profile.requires_self_hosting():\n",
        "        reasoning.append(\"RESTRICTED data \u2192 must self-host (no external APIs)\")\n",
        "        \n",
        "        if profile.complexity in (TaskComplexity.SIMPLE, TaskComplexity.STANDARD):\n",
        "            examples = [\"Llama 3.1 8B\", \"Mistral 7B\", \"Phi-3\"]\n",
        "            reasoning.append(\"Simple/standard task \u2192 small model sufficient\")\n",
        "            cost = 0.10  # Rough compute estimate\n",
        "        else:\n",
        "            examples = [\"Llama 3.1 70B\", \"Mixtral 8x22B\", \"Qwen 72B\"]\n",
        "            reasoning.append(\"Complex task \u2192 larger self-hosted model needed\")\n",
        "            warnings.append(\"70B+ models require significant GPU infrastructure\")\n",
        "            cost = 0.50\n",
        "        \n",
        "        return ModelRecommendation(\n",
        "            primary=ModelClass.SELF_HOSTED,\n",
        "            primary_examples=examples,\n",
        "            reasoning=reasoning,\n",
        "            warnings=warnings,\n",
        "            estimated_cost_per_1k=cost\n",
        "        )\n",
        "    \n",
        "    # Soft constraint: sensitive data prefers self-hosting\n",
        "    if profile.prefers_self_hosting():\n",
        "        reasoning.append(\"SENSITIVE data \u2192 prefer self-hosted or regional provider\")\n",
        "        \n",
        "        if profile.complexity == TaskComplexity.SIMPLE:\n",
        "            return ModelRecommendation(\n",
        "                primary=ModelClass.SMALL_OPEN,\n",
        "                primary_examples=[\"Llama 3.1 8B (self-hosted)\", \"Mistral 7B\"],\n",
        "                alternatives=[ModelClass.SMALL_CLOSED],\n",
        "                reasoning=reasoning + [\"Simple task \u2192 small open model ideal\"],\n",
        "                warnings=[\"If using cloud API, ensure GDPR-compliant DPA in place\"],\n",
        "                estimated_cost_per_1k=0.10\n",
        "            )\n",
        "        elif profile.complexity == TaskComplexity.STANDARD:\n",
        "            return ModelRecommendation(\n",
        "                primary=ModelClass.MID_OPEN,\n",
        "                primary_examples=[\"Llama 3.1 70B\", \"Mixtral 8x22B\"],\n",
        "                alternatives=[ModelClass.MID_CLOSED],\n",
        "                reasoning=reasoning + [\"Standard task \u2192 mid-tier open model\"],\n",
        "                warnings=[\"Cloud APIs (GPT-4o, Sonnet) viable with proper DPA\"],\n",
        "                estimated_cost_per_1k=0.50\n",
        "            )\n",
        "        else:  # COMPLEX or AGENTIC\n",
        "            reasoning.append(\"Complex task with sensitive data \u2192 trade-off required\")\n",
        "            warnings.append(\"Best open models lag frontier by ~6 months on reasoning\")\n",
        "            warnings.append(\"Consider: Can you decompose into sensitive + non-sensitive parts?\")\n",
        "            return ModelRecommendation(\n",
        "                primary=ModelClass.MID_OPEN,\n",
        "                primary_examples=[\"Llama 3.1 70B\", \"Mixtral 8x22B\"],\n",
        "                alternatives=[ModelClass.MID_CLOSED, ModelClass.FRONTIER],\n",
        "                reasoning=reasoning,\n",
        "                warnings=warnings,\n",
        "                estimated_cost_per_1k=0.50\n",
        "            )\n",
        "    \n",
        "    # No sovereignty constraints\u2014optimize for capability and cost\n",
        "    \n",
        "    # Simple tasks: small models suffice\n",
        "    if profile.complexity == TaskComplexity.SIMPLE:\n",
        "        reasoning.append(\"Simple task \u2192 small model sufficient\")\n",
        "        \n",
        "        if profile.is_cost_sensitive():\n",
        "            reasoning.append(f\"High volume ({profile.daily_volume:,}/day) \u2192 optimize cost\")\n",
        "            return ModelRecommendation(\n",
        "                primary=ModelClass.SMALL_CLOSED,\n",
        "                primary_examples=[\"GPT-4o-mini\", \"Claude Haiku\"],\n",
        "                alternatives=[ModelClass.SMALL_OPEN],\n",
        "                reasoning=reasoning,\n",
        "                estimated_cost_per_1k=0.30\n",
        "            )\n",
        "        else:\n",
        "            return ModelRecommendation(\n",
        "                primary=ModelClass.SMALL_CLOSED,\n",
        "                primary_examples=[\"GPT-4o-mini\", \"Claude Haiku\"],\n",
        "                reasoning=reasoning,\n",
        "                estimated_cost_per_1k=0.30\n",
        "            )\n",
        "    \n",
        "    # Standard tasks: mid-tier models\n",
        "    if profile.complexity == TaskComplexity.STANDARD:\n",
        "        reasoning.append(\"Standard task \u2192 mid-tier model recommended\")\n",
        "        \n",
        "        if profile.is_latency_constrained():\n",
        "            reasoning.append(\"Real-time latency \u2192 prefer optimized inference\")\n",
        "            warnings.append(\"GPT-4o and Sonnet typically 200-500ms; may need caching\")\n",
        "        \n",
        "        if profile.is_cost_sensitive():\n",
        "            reasoning.append(f\"High volume ({profile.daily_volume:,}/day) \u2192 consider routing\")\n",
        "            alternatives.append(ModelClass.SMALL_CLOSED)\n",
        "            warnings.append(\"Route simple queries to smaller model for 50-70% cost reduction\")\n",
        "        \n",
        "        return ModelRecommendation(\n",
        "            primary=ModelClass.MID_CLOSED,\n",
        "            primary_examples=[\"GPT-4o\", \"Claude Sonnet\"],\n",
        "            alternatives=alternatives,\n",
        "            reasoning=reasoning,\n",
        "            warnings=warnings,\n",
        "            estimated_cost_per_1k=6.00\n",
        "        )\n",
        "    \n",
        "    # Complex reasoning: frontier models\n",
        "    if profile.complexity == TaskComplexity.COMPLEX:\n",
        "        reasoning.append(\"Complex reasoning \u2192 frontier model recommended\")\n",
        "        \n",
        "        if profile.is_latency_constrained():\n",
        "            warnings.append(\"Frontier models may exceed 500ms on complex prompts\")\n",
        "            warnings.append(\"Consider mid-tier for latency-critical paths\")\n",
        "            alternatives.append(ModelClass.MID_CLOSED)\n",
        "        \n",
        "        if profile.is_cost_sensitive():\n",
        "            warnings.append(f\"At {profile.daily_volume:,}/day, frontier costs add up fast\")\n",
        "            warnings.append(\"Implement routing: frontier for hard queries, mid-tier for rest\")\n",
        "            alternatives.append(ModelClass.MID_CLOSED)\n",
        "        \n",
        "        return ModelRecommendation(\n",
        "            primary=ModelClass.FRONTIER,\n",
        "            primary_examples=[\"Claude Opus\", \"GPT-4.5\", \"Gemini Ultra\"],\n",
        "            alternatives=alternatives,\n",
        "            reasoning=reasoning,\n",
        "            warnings=warnings,\n",
        "            estimated_cost_per_1k=30.00\n",
        "        )\n",
        "    \n",
        "    # Agentic tasks: tool-use optimized models\n",
        "    reasoning.append(\"Agentic task \u2192 models optimized for tool use\")\n",
        "    reasoning.append(\"Claude Sonnet and GPT-4o excel at structured tool calling\")\n",
        "    \n",
        "    if profile.is_cost_sensitive():\n",
        "        warnings.append(\"Agentic loops multiply token usage\u2014monitor closely\")\n",
        "    \n",
        "    return ModelRecommendation(\n",
        "        primary=ModelClass.MID_CLOSED,\n",
        "        primary_examples=[\"Claude Sonnet\", \"GPT-4o\"],\n",
        "        alternatives=[ModelClass.FRONTIER],\n",
        "        reasoning=reasoning + [\"Mid-tier often matches frontier on tool use\"],\n",
        "        warnings=warnings,\n",
        "        estimated_cost_per_1k=6.00\n",
        "    )\n",
        "\n",
        "\n",
        "def format_recommendation(profile: TaskProfile, rec: ModelRecommendation) -> str:\n",
        "    \"\"\"Format recommendation as readable output.\"\"\"\n",
        "    lines = [\n",
        "        f\"MODEL RECOMMENDATION: {profile.name}\",\n",
        "        \"=\" * 60,\n",
        "        \"\",\n",
        "        f\"Task Profile:\",\n",
        "        f\"  Complexity:   {profile.complexity.value}\",\n",
        "        f\"  Sensitivity:  {profile.sensitivity.value}\",\n",
        "        f\"  Latency:      {profile.latency.value}\",\n",
        "        f\"  Daily Volume: {profile.daily_volume:,}\",\n",
        "        \"\",\n",
        "        f\"Recommended: {rec.primary.value.upper()}\",\n",
        "        f\"  Examples: {', '.join(rec.primary_examples)}\",\n",
        "        \"\",\n",
        "    ]\n",
        "    \n",
        "    if rec.alternatives:\n",
        "        alt_names = [a.value for a in rec.alternatives]\n",
        "        lines.append(f\"Alternatives: {', '.join(alt_names)}\")\n",
        "        lines.append(\"\")\n",
        "    \n",
        "    lines.append(\"Reasoning:\")\n",
        "    for r in rec.reasoning:\n",
        "        lines.append(f\"  \u2022 {r}\")\n",
        "    \n",
        "    if rec.warnings:\n",
        "        lines.append(\"\")\n",
        "        lines.append(\"Warnings:\")\n",
        "        for w in rec.warnings:\n",
        "            lines.append(f\"  \u26a0 {w}\")\n",
        "    \n",
        "    lines.append(\"\")\n",
        "    monthly_cost = rec.estimated_cost_per_1k * (profile.daily_volume * 30 / 1000)\n",
        "    lines.append(f\"Estimated Monthly Cost: \u20ac{monthly_cost:,.0f}\")\n",
        "    lines.append(f\"  (Based on \u20ac{rec.estimated_cost_per_1k:.2f} per 1K requests)\")\n",
        "    \n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Model selection for real scenarios\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Model Selection Advisor\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Scenario 1: Support ticket classifier with PII\n",
        "ticket_classifier = TaskProfile(\n",
        "    name=\"Support Ticket Classifier\",\n",
        "    complexity=TaskComplexity.SIMPLE,\n",
        "    sensitivity=DataSensitivity.SENSITIVE,\n",
        "    latency=LatencyTier.REALTIME,\n",
        "    daily_volume=50000\n",
        ")\n",
        "rec1 = recommend_model(ticket_classifier)\n",
        "print(format_recommendation(ticket_classifier, rec1))\n",
        "print()\n",
        "\n",
        "# Scenario 2: Contract analysis for legal team\n",
        "contract_analyzer = TaskProfile(\n",
        "    name=\"Contract Risk Analyzer\", \n",
        "    complexity=TaskComplexity.COMPLEX,\n",
        "    sensitivity=DataSensitivity.RESTRICTED,\n",
        "    latency=LatencyTier.BATCH,\n",
        "    daily_volume=500\n",
        ")\n",
        "rec2 = recommend_model(contract_analyzer)\n",
        "print(format_recommendation(contract_analyzer, rec2))\n",
        "print()\n",
        "\n",
        "# Scenario 3: Customer-facing chatbot\n",
        "chatbot = TaskProfile(\n",
        "    name=\"Product Q&A Chatbot\",\n",
        "    complexity=TaskComplexity.STANDARD,\n",
        "    sensitivity=DataSensitivity.PUBLIC,\n",
        "    latency=LatencyTier.INTERACTIVE,\n",
        "    daily_volume=100000\n",
        ")\n",
        "rec3 = recommend_model(chatbot)\n",
        "print(format_recommendation(chatbot, rec3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0c7ec84a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Validation Framework\n",
            "============================================================\n",
            "\n",
            "To validate models on YOUR task:\n",
            "\n",
            "1. BUILD YOUR TEST SET (50-200 examples from production):\n",
            "\n",
            "   test_cases = [\n",
            "       {\"input\": \"Where is my order #12345?\", \"expected\": \"order_status\"},\n",
            "       {\"input\": \"I want a refund\", \"expected\": \"refund_request\"},\n",
            "       {\"input\": \"Your product broke my dishwasher\", \"expected\": \"complaint\"},\n",
            "       # Include edge cases that have caused problems\n",
            "   ]\n",
            "\n",
            "2. DEFINE YOUR EVALUATOR:\n",
            "\n",
            "   # For classification:\n",
            "   def evaluator(actual: str, expected: str) -> float:\n",
            "       return 1.0 if expected.lower() in actual.lower() else 0.0\n",
            "\n",
            "   # For generation (using embedding similarity):\n",
            "   def evaluator(actual: str, expected: str) -> float:\n",
            "       return cosine_similarity(embed(actual), embed(expected))\n",
            "\n",
            "3. DEFINE YOUR REQUIREMENTS:\n",
            "\n",
            "   requirements = {\n",
            "       \"min_accuracy\": 0.92,        # 92% accuracy minimum\n",
            "       \"max_latency_p95_ms\": 500,   # 500ms P95 latency\n",
            "       \"max_cost_per_1k\": 10.0      # \u20ac10 per 1000 requests\n",
            "   }\n",
            "\n",
            "4. SET UP MODEL CANDIDATES:\n",
            "\n",
            "   models = {\n",
            "       \"gpt-4o-mini\": (\n",
            "           lambda x: call_openai(x, model=\"gpt-4o-mini\"),\n",
            "           0.00015  # cost per 1K tokens\n",
            "       ),\n",
            "       \"claude-haiku\": (\n",
            "           lambda x: call_anthropic(x, model=\"claude-3-haiku\"),\n",
            "           0.00025\n",
            "       ),\n",
            "       \"llama-8b-local\": (\n",
            "           lambda x: call_local(x, model=\"llama-8b\"),\n",
            "           0.00005  # compute cost estimate\n",
            "       ),\n",
            "   }\n",
            "\n",
            "5. RUN COMPARISON:\n",
            "\n",
            "   results = compare_models(models, test_cases, evaluator, requirements)\n",
            "\n",
            "   for r in results:\n",
            "       status = \"\u2713\" if r.meets_requirements(**requirements) else \"\u2717\"\n",
            "       print(f\"{status} {r.model_name}: {r.accuracy:.1%} accuracy, \"\n",
            "             f\"{r.latency_p95_ms:.0f}ms P95, \u20ac{r.cost_per_1k_requests:.2f}/1K\")\n",
            "\n",
            "The model that meets all requirements at lowest cost wins.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Dict, Callable, Any\n",
        "from dataclasses import dataclass\n",
        "import time\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkResult:\n",
        "    \"\"\"Results from benchmarking a model on your task.\"\"\"\n",
        "    model_name: str\n",
        "    accuracy: float\n",
        "    latency_p50_ms: float\n",
        "    latency_p95_ms: float\n",
        "    cost_per_1k_requests: float\n",
        "    \n",
        "    def meets_requirements(\n",
        "        self, \n",
        "        min_accuracy: float, \n",
        "        max_latency_p95_ms: float,\n",
        "        max_cost_per_1k: float\n",
        "    ) -> bool:\n",
        "        \"\"\"Check if this model meets all requirements.\"\"\"\n",
        "        return (\n",
        "            self.accuracy >= min_accuracy and\n",
        "            self.latency_p95_ms <= max_latency_p95_ms and\n",
        "            self.cost_per_1k_requests <= max_cost_per_1k\n",
        "        )\n",
        "\n",
        "\n",
        "def benchmark_model(\n",
        "    model_fn: Callable[[str], str],\n",
        "    test_cases: List[Dict[str, str]],\n",
        "    evaluator: Callable[[str, str], float],\n",
        "    cost_per_1k_tokens: float,\n",
        "    avg_tokens_per_request: int = 2000\n",
        ") -> BenchmarkResult:\n",
        "    \"\"\"\n",
        "    Benchmark a single model on your test cases.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model_fn : Callable\n",
        "        Function that takes input string, returns output string\n",
        "    test_cases : List[Dict]\n",
        "        Each dict has 'input' and 'expected' keys\n",
        "    evaluator : Callable\n",
        "        Function(actual, expected) -> score (0.0 to 1.0)\n",
        "    cost_per_1k_tokens : float\n",
        "        Model's price per 1000 tokens\n",
        "    avg_tokens_per_request : int\n",
        "        Expected tokens per request for cost calculation\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    latencies = []\n",
        "    \n",
        "    for case in test_cases:\n",
        "        start = time.perf_counter()\n",
        "        actual = model_fn(case['input'])\n",
        "        latency_ms = (time.perf_counter() - start) * 1000\n",
        "        \n",
        "        score = evaluator(actual, case['expected'])\n",
        "        scores.append(score)\n",
        "        latencies.append(latency_ms)\n",
        "    \n",
        "    latencies.sort()\n",
        "    n = len(latencies)\n",
        "    \n",
        "    return BenchmarkResult(\n",
        "        model_name=\"\",  # Set by caller\n",
        "        accuracy=sum(scores) / len(scores),\n",
        "        latency_p50_ms=latencies[n // 2],\n",
        "        latency_p95_ms=latencies[int(n * 0.95)],\n",
        "        cost_per_1k_requests=(avg_tokens_per_request / 1000) * cost_per_1k_tokens * 1000\n",
        "    )\n",
        "\n",
        "\n",
        "def compare_models(\n",
        "    models: Dict[str, tuple],  # name -> (model_fn, cost_per_1k_tokens)\n",
        "    test_cases: List[Dict[str, str]],\n",
        "    evaluator: Callable[[str, str], float],\n",
        "    requirements: Dict[str, float]  # min_accuracy, max_latency_p95_ms, max_cost_per_1k\n",
        ") -> List[BenchmarkResult]:\n",
        "    \"\"\"\n",
        "    Benchmark multiple models and filter by requirements.\n",
        "    \n",
        "    Returns results sorted by accuracy (highest first),\n",
        "    with models not meeting requirements flagged.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for name, (model_fn, cost) in models.items():\n",
        "        result = benchmark_model(model_fn, test_cases, evaluator, cost)\n",
        "        result.model_name = name\n",
        "        results.append(result)\n",
        "    \n",
        "    # Sort by accuracy descending\n",
        "    results.sort(key=lambda r: r.accuracy, reverse=True)\n",
        "    return results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: How to set up your benchmark\n",
        "# =============================================================================\n",
        "\n",
        "print()\n",
        "print(\"Model Validation Framework\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "To validate models on YOUR task:\n",
        "\n",
        "1. BUILD YOUR TEST SET (50-200 examples from production):\n",
        "\n",
        "   test_cases = [\n",
        "       {\"input\": \"Where is my order #12345?\", \"expected\": \"order_status\"},\n",
        "       {\"input\": \"I want a refund\", \"expected\": \"refund_request\"},\n",
        "       {\"input\": \"Your product broke my dishwasher\", \"expected\": \"complaint\"},\n",
        "       # Include edge cases that have caused problems\n",
        "   ]\n",
        "\n",
        "2. DEFINE YOUR EVALUATOR:\n",
        "\n",
        "   # For classification:\n",
        "   def evaluator(actual: str, expected: str) -> float:\n",
        "       return 1.0 if expected.lower() in actual.lower() else 0.0\n",
        "   \n",
        "   # For generation (using embedding similarity):\n",
        "   def evaluator(actual: str, expected: str) -> float:\n",
        "       return cosine_similarity(embed(actual), embed(expected))\n",
        "\n",
        "3. DEFINE YOUR REQUIREMENTS:\n",
        "\n",
        "   requirements = {\n",
        "       \"min_accuracy\": 0.92,        # 92% accuracy minimum\n",
        "       \"max_latency_p95_ms\": 500,   # 500ms P95 latency\n",
        "       \"max_cost_per_1k\": 10.0      # \u20ac10 per 1000 requests\n",
        "   }\n",
        "\n",
        "4. SET UP MODEL CANDIDATES:\n",
        "\n",
        "   models = {\n",
        "       \"gpt-4o-mini\": (\n",
        "           lambda x: call_openai(x, model=\"gpt-4o-mini\"),\n",
        "           0.00015  # cost per 1K tokens\n",
        "       ),\n",
        "       \"claude-haiku\": (\n",
        "           lambda x: call_anthropic(x, model=\"claude-3-haiku\"),\n",
        "           0.00025\n",
        "       ),\n",
        "       \"llama-8b-local\": (\n",
        "           lambda x: call_local(x, model=\"llama-8b\"),\n",
        "           0.00005  # compute cost estimate\n",
        "       ),\n",
        "   }\n",
        "\n",
        "5. RUN COMPARISON:\n",
        "\n",
        "   results = compare_models(models, test_cases, evaluator, requirements)\n",
        "   \n",
        "   for r in results:\n",
        "       status = \"\u2713\" if r.meets_requirements(**requirements) else \"\u2717\"\n",
        "       print(f\"{status} {r.model_name}: {r.accuracy:.1%} accuracy, \"\n",
        "             f\"{r.latency_p95_ms:.0f}ms P95, \u20ac{r.cost_per_1k_requests:.2f}/1K\")\n",
        "\n",
        "The model that meets all requirements at lowest cost wins.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8209b06d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vision Pipeline Cost Analysis: Invoice Processing\n",
            "=======================================================\n",
            "Daily token consumption:    18,000,000\n",
            "Daily cost:               \u20ac      45.00\n",
            "Monthly cost:             \u20ac   1,350.00\n",
            "Cost vs text-only:                 6.0\u00d7\n",
            "\n",
            "Decision guidance:\n",
            "  \u2022 If OCR + text extraction achieves 95%+ accuracy \u2192 use text-only\n",
            "  \u2022 If documents have complex layouts, tables \u2192 vision may be worth 4\u00d7\n",
            "  \u2022 Consider hybrid: OCR first, vision fallback for low-confidence cases\n"
          ]
        }
      ],
      "source": [
        "def estimate_vision_costs(\n",
        "    images_per_day: int,\n",
        "    tokens_per_image: int = 1500,  # Typical for 1024x1024\n",
        "    text_tokens_per_request: int = 500,\n",
        "    price_per_1k_input: float = 0.0025  # GPT-4o pricing\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Estimate costs for a vision-enabled pipeline.\n",
        "    \n",
        "    Vision tokens typically cost the same as text tokens,\n",
        "    but images consume many more tokens than equivalent text.\n",
        "    \"\"\"\n",
        "    daily_vision_tokens = images_per_day * tokens_per_image\n",
        "    daily_text_tokens = images_per_day * text_tokens_per_request\n",
        "    daily_total_tokens = daily_vision_tokens + daily_text_tokens\n",
        "    \n",
        "    daily_cost = (daily_total_tokens / 1000) * price_per_1k_input\n",
        "    monthly_cost = daily_cost * 30\n",
        "    \n",
        "    # Compare to text-only alternative\n",
        "    text_only_daily = (images_per_day * text_tokens_per_request / 1000) * price_per_1k_input\n",
        "    vision_premium = daily_cost / text_only_daily if text_only_daily > 0 else float('inf')\n",
        "    \n",
        "    return {\n",
        "        'daily_tokens': daily_total_tokens,\n",
        "        'daily_cost': round(daily_cost, 2),\n",
        "        'monthly_cost': round(monthly_cost, 2),\n",
        "        'vision_cost_multiplier': round(vision_premium, 1)\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Vision cost analysis for document processing\n",
        "# =============================================================================\n",
        "\n",
        "# Scenario: Invoice processing system\n",
        "invoice_processing = estimate_vision_costs(\n",
        "    images_per_day=10000,\n",
        "    tokens_per_image=1500,\n",
        "    text_tokens_per_request=300,\n",
        "    price_per_1k_input=0.0025\n",
        ")\n",
        "\n",
        "print(\"Vision Pipeline Cost Analysis: Invoice Processing\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"Daily token consumption:  {invoice_processing['daily_tokens']:>12,}\")\n",
        "print(f\"Daily cost:               \u20ac{invoice_processing['daily_cost']:>11,.2f}\")\n",
        "print(f\"Monthly cost:             \u20ac{invoice_processing['monthly_cost']:>11,.2f}\")\n",
        "print(f\"Cost vs text-only:        {invoice_processing['vision_cost_multiplier']:>12}\u00d7\")\n",
        "print()\n",
        "print(\"Decision guidance:\")\n",
        "print(\"  \u2022 If OCR + text extraction achieves 95%+ accuracy \u2192 use text-only\")\n",
        "print(\"  \u2022 If documents have complex layouts, tables \u2192 vision may be worth 4\u00d7\")\n",
        "print(\"  \u2022 Consider hybrid: OCR first, vision fallback for low-confidence cases\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3917d317",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Structured Output with Instructor\n",
            "=======================================================\n",
            "\n",
            "Setup (one-time):\n",
            "    from openai import OpenAI\n",
            "    import instructor\n",
            "\n",
            "    client = instructor.from_openai(OpenAI())\n",
            "    # Or: instructor.from_anthropic(Anthropic())\n",
            "    # Or: instructor.from_provider(\"openai/gpt-4o-mini\")\n",
            "\n",
            "Usage:\n",
            "    raw_message = '''\n",
            "    I've been trying to reset my password for 3 days now!\n",
            "    The mobile app keeps crashing when I tap \"Forgot Password\".\n",
            "    This is ridiculous - I need access to my account for work.\n",
            "    Order #12345 is stuck and I can't track it.\n",
            "    '''\n",
            "\n",
            "    ticket = extract_ticket_info(\n",
            "        raw_text=raw_message,\n",
            "        customer_id=\"CUST-98765\",\n",
            "        client=client\n",
            "    )\n",
            "\n",
            "    print(ticket.model_dump_json(indent=2))\n",
            "\n",
            "Expected output:\n",
            "    {\n",
            "      \"customer_id\": \"CUST-98765\",\n",
            "      \"category\": \"authentication\",\n",
            "      \"priority\": \"high\",\n",
            "      \"summary\": \"Password reset failing on mobile app, blocking order tracking\",\n",
            "      \"entities_mentioned\": [\"mobile app\", \"Forgot Password\", \"Order #12345\"],\n",
            "      \"sentiment\": -0.8\n",
            "    }\n",
            "\n",
            "Key benefits:\n",
            "    \u2022 Type safety: IDE autocompletion, static analysis\n",
            "    \u2022 Validation: Pydantic validators catch bad data\n",
            "    \u2022 Retries: Failed validation \u2192 re-prompt with error context\n",
            "    \u2022 Portability: Same code works across 15+ providers\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Structured Output with Instructor\n",
        "# pip install instructor pydantic\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from enum import Enum\n",
        "\n",
        "class Priority(str, Enum):\n",
        "    HIGH = \"high\"\n",
        "    MEDIUM = \"medium\"\n",
        "    LOW = \"low\"\n",
        "\n",
        "class SupportTicket(BaseModel):\n",
        "    \"\"\"Schema for structured extraction - Pydantic does the heavy lifting.\"\"\"\n",
        "    category: str = Field(description=\"Issue category\")\n",
        "    priority: Priority = Field(description=\"Urgency level\")\n",
        "    summary: str = Field(description=\"One-sentence summary\", max_length=200)\n",
        "    entities: List[str] = Field(default_factory=list, description=\"Products/orders mentioned\")\n",
        "    sentiment: float = Field(ge=-1.0, le=1.0, description=\"Sentiment score\")\n",
        "\n",
        "\n",
        "print(\"Structured Output with Instructor\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "WHAT INSTRUCTOR DOES:\n",
        "  1. Injects your Pydantic schema into the prompt\n",
        "  2. Parses LLM response into typed object\n",
        "  3. On validation failure \u2192 re-prompts with error context\n",
        "  4. Returns validated Pydantic object, not raw text\n",
        "\n",
        "RELIABILITY SPECTRUM:\n",
        "  Prompt-only parsing:     ~85% (model adds explanations, breaks JSON)\n",
        "  Instructor:              ~95-99% (auto-retry with validation feedback)\n",
        "  Constrained generation:  ~99.9% (grammar-enforced, for self-hosted)\n",
        "\n",
        "SETUP:\n",
        "  # Cloud APIs\n",
        "  client = instructor.from_openai(OpenAI())\n",
        "  client = instructor.from_anthropic(Anthropic())\n",
        "  \n",
        "  # Local (Ollama)\n",
        "  client = instructor.from_openai(\n",
        "      OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),\n",
        "      mode=instructor.Mode.JSON\n",
        "  )\n",
        "\n",
        "USAGE:\n",
        "  ticket = client.chat.completions.create(\n",
        "      model=\"gpt-4o-mini\",\n",
        "      response_model=SupportTicket,\n",
        "      max_retries=2,\n",
        "      messages=[{\"role\": \"user\", \"content\": raw_message}]\n",
        "  )\n",
        "  # ticket is a SupportTicket object, not a string\n",
        "\n",
        "\u2192 See 1B/demos.ipynb for runnable demo with Ollama\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e08b3898",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constrained Generation Decision\n",
            "=======================================================\n",
            "\n",
            "Choose your approach:\n",
            "\n",
            "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
            "\u2502 Approach            \u2502 Reliability    \u2502 Best For         \u2502\n",
            "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
            "\u2502 Prompt + parsing    \u2502 ~85%           \u2502 Prototyping      \u2502\n",
            "\u2502 Instructor          \u2502 ~95-99%        \u2502 Cloud APIs       \u2502\n",
            "\u2502 Outlines/guidance   \u2502 ~99.9%         \u2502 Self-hosted      \u2502\n",
            "\u2502 Native JSON mode    \u2502 ~95%           \u2502 Simple schemas   \u2502\n",
            "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
            "\n",
            "For most production systems, Instructor is the sweet spot:\n",
            "high reliability, great DX, works everywhere.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Structured Generation - Outlines\n",
        "# pip install outlines[ollama]  # or [openai], [anthropic], [transformers], [vllm]\n",
        "\n",
        "\"\"\"\n",
        "Outlines is a unified structured generation library supporting many backends.\n",
        "Capabilities differ based on how you connect:\n",
        "\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 Backend                    \u2502 JSON Schemas \u2502 Regex/Grammar   \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 Ollama (from_ollama)       \u2502 \u2713            \u2502 \u2717 (black-box)   \u2502\n",
        "\u2502 OpenAI (from_openai)       \u2502 \u2713            \u2502 \u2717 (black-box)   \u2502\n",
        "\u2502 Anthropic (from_anthropic) \u2502 \u2713            \u2502 \u2717 (black-box)   \u2502\n",
        "\u2502 vLLM server (from_vllm)    \u2502 \u2713            \u2502 \u2717 (API mode)    \u2502\n",
        "\u2502 vLLM local (from_vllm_offline) \u2502 \u2713        \u2502 \u2713 Full support  \u2502\n",
        "\u2502 HuggingFace (from_transformers)\u2502 \u2713        \u2502 \u2713 Full support  \u2502\n",
        "\u2502 llama.cpp (from_llamacpp)  \u2502 \u2713            \u2502 \u2713 Full support  \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "# API backends - JSON schemas via provider's native mode\n",
        "import outlines, ollama\n",
        "model = outlines.from_ollama(ollama.Client(), model_name=\"qwen3:4b\")\n",
        "result = model(\"Classify: payment failed\", MySchema)  # Returns JSON str\n",
        "\n",
        "# Local backends - true token masking, full grammar control\n",
        "from vllm import LLM\n",
        "model = outlines.from_vllm_offline(LLM(\"meta-llama/Llama-3-8B\"))\n",
        "\n",
        "regex_type = outlines.types.regex(r\"PRD-[0-9]{3}\")\n",
        "result = model(\"Generate code:\", regex_type)  # GUARANTEED PRD-XXX\n",
        "\n",
        "DECISION GUIDE:\n",
        "  \u2022 APIs (Ollama, OpenAI, vLLM server)? \u2192 Instructor has simpler DX\n",
        "  \u2022 Self-hosting + need regex/grammar? \u2192 Outlines (local backends)\n",
        "  \u2022 High-volume GPU inference? \u2192 Outlines + vLLM offline (fastest)\n",
        "\n",
        "\u2192 See 1B/demos.ipynb for runnable examples\n",
        "\"\"\"\n",
        "\n",
        "print(\"Constrained Generation Decision\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "Choose your approach:\n",
        "\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502 Approach            \u2502 Reliability    \u2502 Best For         \u2502\n",
        "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
        "\u2502 Prompt + parsing    \u2502 ~85%           \u2502 Prototyping      \u2502\n",
        "\u2502 Instructor          \u2502 ~95-99%        \u2502 Cloud APIs       \u2502\n",
        "\u2502 Outlines/guidance   \u2502 ~99.9%         \u2502 Self-hosted      \u2502\n",
        "\u2502 Native JSON mode    \u2502 ~95%           \u2502 Simple schemas   \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "For most production systems, Instructor is the sweet spot:\n",
        "high reliability, great DX, works everywhere.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7dfbd001",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeMo Guardrails: Dialog Flow Control\n",
            "=======================================================\n",
            "\n",
            "RAIL TYPES:\n",
            "\n",
            "1. INPUT RAILS (before LLM):\n",
            "   \u2022 Jailbreak detection - \"Ignore previous instructions...\"\n",
            "   \u2022 Topic filtering - Reject off-topic requests\n",
            "   \u2022 PII masking - Block/redact sensitive data\n",
            "\n",
            "2. OUTPUT RAILS (after LLM):\n",
            "   \u2022 Toxicity filtering - Block harmful content\n",
            "   \u2022 Factuality checking - Verify against knowledge base\n",
            "\n",
            "3. DIALOG RAILS (Colang flows):\n",
            "   \u2022 Define conversation patterns\n",
            "   \u2022 Guide users through processes\n",
            "   \u2022 Handle edge cases consistently\n",
            "\n",
            "USAGE:\n",
            "  rails = create_guarded_llm(\"./nemo_config\")\n",
            "\n",
            "  response = rails.generate(\n",
            "      messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
            "  )\n",
            "\n",
            "\u2192 See 1B/nemo_config/ for complete config files\n",
            "\u2192 See 1B/demos.ipynb (Demo 3) for runnable demo with Ollama\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# NeMo Guardrails - Dialog Flow and Safety Rails\n",
        "# pip install nemoguardrails langchain-openai\n",
        "\n",
        "\"\"\"\n",
        "NeMo Guardrails requires a config directory with:\n",
        "  nemo_config/\n",
        "  \u251c\u2500\u2500 config.yml   # Model and rails configuration\n",
        "  \u251c\u2500\u2500 rails.co     # Colang dialog flows (user intents, bot responses)\n",
        "  \u2514\u2500\u2500 prompts.yml  # Custom prompts for safety checks\n",
        "\n",
        "Example config.yml (for Ollama):\n",
        "  models:\n",
        "    - type: main\n",
        "      engine: openai\n",
        "      model: qwen3:4b\n",
        "      parameters:\n",
        "        openai_api_base: http://localhost:11434/v1\n",
        "        openai_api_key: ollama\n",
        "\n",
        "Example rails.co (Colang dialog flows):\n",
        "  define user express greeting\n",
        "      \"hello\"\n",
        "      \"hi\"\n",
        "  \n",
        "  define bot express greeting\n",
        "      \"Hello! How can I help you today?\"\n",
        "  \n",
        "  define flow greeting\n",
        "      user express greeting\n",
        "      bot express greeting\n",
        "  \n",
        "  define user ask off topic\n",
        "      \"what's the weather\"\n",
        "      \"tell me a joke\"\n",
        "  \n",
        "  define bot refuse off topic\n",
        "      \"I'm designed to help with TechCorp products only.\"\n",
        "  \n",
        "  define flow handle off topic\n",
        "      user ask off topic\n",
        "      bot refuse off topic\n",
        "\"\"\"\n",
        "\n",
        "from nemoguardrails import LLMRails, RailsConfig\n",
        "\n",
        "def create_guarded_llm(config_path: str):\n",
        "    \"\"\"Create an LLM with NeMo guardrails applied.\"\"\"\n",
        "    config = RailsConfig.from_path(config_path)\n",
        "    return LLMRails(config)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: NeMo Guardrails overview\n",
        "# =============================================================================\n",
        "\n",
        "print(\"NeMo Guardrails: Dialog Flow Control\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "RAIL TYPES:\n",
        "\n",
        "1. INPUT RAILS (before LLM):\n",
        "   \u2022 Jailbreak detection - \"Ignore previous instructions...\"\n",
        "   \u2022 Topic filtering - Reject off-topic requests\n",
        "   \u2022 PII masking - Block/redact sensitive data\n",
        "\n",
        "2. OUTPUT RAILS (after LLM):\n",
        "   \u2022 Toxicity filtering - Block harmful content\n",
        "   \u2022 Factuality checking - Verify against knowledge base\n",
        "\n",
        "3. DIALOG RAILS (Colang flows):\n",
        "   \u2022 Define conversation patterns\n",
        "   \u2022 Guide users through processes\n",
        "   \u2022 Handle edge cases consistently\n",
        "\n",
        "USAGE:\n",
        "  rails = create_guarded_llm(\"./nemo_config\")\n",
        "  \n",
        "  response = rails.generate(\n",
        "      messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
        "  )\n",
        "\n",
        "\u2192 See 1B/nemo_config/ for complete config files\n",
        "\u2192 See 1B/guardrails_demo.ipynb for comprehensive guardrails demos\n",
        "\"\"\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa9cc40",
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'DetectPII' from 'guardrails.hub' (/Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages/guardrails/hub/__init__.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03mGuardrails AI provides validators from the Hub:\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m- PII detection and redaction\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[33;03m    hub://guardrails/reading_level   # Ensure appropriate complexity\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mguardrails\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Guard\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mguardrails\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DetectPII, ToxicLanguage\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'DetectPII' from 'guardrails.hub' (/Users/titasbiswas/miniforge3/envs/jupyter-env/lib/python3.12/site-packages/guardrails/hub/__init__.py)"
          ]
        }
      ],
      "source": [
        "# Guardrails AI - Field-level Validation with Hub Validators\n",
        "# \u26a0\ufe0f DEPENDENCY CONFLICT: guardrails-ai requires openai<2.0.0\n",
        "#    This conflicts with Instructor which requires openai>=2.0.0\n",
        "#    Use in separate environment if needed\n",
        "\n",
        "# pip install guardrails-ai\n",
        "# guardrails hub install hub://guardrails/regex_match\n",
        "# guardrails hub install hub://guardrails/toxic_language\n",
        "\n",
        "\"\"\"\n",
        "Guardrails AI provides validators from the Hub:\n",
        "- PII detection and redaction\n",
        "- Toxic language filtering  \n",
        "- Regex pattern matching\n",
        "- Custom LLM-based validation\n",
        "\n",
        "Example validators from Hub:\n",
        "    hub://guardrails/detect_pii\n",
        "    hub://guardrails/toxic_language\n",
        "    hub://guardrails/provenance_llm  # Check if grounded in sources\n",
        "    hub://guardrails/reading_level   # Ensure appropriate complexity\n",
        "\"\"\"\n",
        "\n",
        "from guardrails import Guard\n",
        "from guardrails.hub import DetectPII, ToxicLanguage\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class CustomerResponse(BaseModel):\n",
        "    \"\"\"Schema for customer-facing responses.\"\"\"\n",
        "    answer: str = Field(\n",
        "        description=\"The response to the customer\",\n",
        "        validators=[\n",
        "            ToxicLanguage(on_fail=\"fix\"),  # Auto-fix toxic content\n",
        "            DetectPII(on_fail=\"fix\"),       # Redact any PII\n",
        "        ]\n",
        "    )\n",
        "    sources: List[str] = Field(\n",
        "        description=\"Sources used to generate the answer\"\n",
        "    )\n",
        "    confidence: float = Field(\n",
        "        ge=0.0, le=1.0,\n",
        "        description=\"Confidence score\"\n",
        "    )\n",
        "\n",
        "\n",
        "def validated_response(\n",
        "    user_query: str,\n",
        "    context: str,\n",
        "    llm_callable\n",
        ") -> CustomerResponse:\n",
        "    \"\"\"\n",
        "    Generate a response with Guardrails AI validation.\n",
        "    \n",
        "    Validators run on the output and can:\n",
        "    - Pass: Output is valid\n",
        "    - Fix: Auto-correct issues (e.g., redact PII)\n",
        "    - Fail: Reject and optionally retry\n",
        "    \"\"\"\n",
        "    guard = Guard.from_pydantic(CustomerResponse)\n",
        "    \n",
        "    result = guard(\n",
        "        llm_callable,\n",
        "        prompt=f\"\"\"\n",
        "        Context: {context}\n",
        "        \n",
        "        Question: {user_query}\n",
        "        \n",
        "        Provide a helpful answer based only on the context.\n",
        "        \"\"\",\n",
        "        num_reasks=2  # Retry twice on validation failure\n",
        "    )\n",
        "    \n",
        "    return result.validated_output\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Combined guardrails strategy\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Combined Guardrails Strategy\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "RECOMMENDED ARCHITECTURE:\n",
        "\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502                    USER INPUT                          \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                          \u2502\n",
        "                          \u25bc\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502              NEMO GUARDRAILS (Dialog Layer)            \u2502\n",
        "\u2502  \u2022 Jailbreak detection                                 \u2502\n",
        "\u2502  \u2022 Topic control                                       \u2502\n",
        "\u2502  \u2022 Conversation flow management                        \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                          \u2502\n",
        "                          \u25bc\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502                    LLM CALL                            \u2502\n",
        "\u2502  (with Instructor for structured output)               \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                          \u2502\n",
        "                          \u25bc\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502           GUARDRAILS AI (Validation Layer)             \u2502\n",
        "\u2502  \u2022 PII redaction                                       \u2502\n",
        "\u2502  \u2022 Toxicity filtering                                  \u2502\n",
        "\u2502  \u2022 Custom validators                                   \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "                          \u2502\n",
        "                          \u25bc\n",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "\u2502                   SAFE OUTPUT                          \u2502\n",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "Why layer guardrails?\n",
        "- NeMo excels at dialog flow and conversation-level control\n",
        "- Guardrails AI excels at field-level validation and Hub ecosystem\n",
        "- Haystack provides pipeline-native components (EU-aligned, data sovereignty focus)\n",
        "- Together they provide defense in depth\n",
        "\n",
        "FRAMEWORK SELECTION:\n",
        "\n",
        "    Using Haystack? \u2192 Use pipeline components (InputGuardrail, OutputGuardrail)\n",
        "    Using LangChain? \u2192 Use NeMo + Guardrails AI wrappers\n",
        "    Framework-agnostic? \u2192 NeMo for dialog + Guardrails AI for validation\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4d86ddbf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Haystack 2.x Guardrails Pipeline\n",
            "=======================================================\n",
            "\n",
            "PIPELINE ARCHITECTURE:\n",
            "\n",
            "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
            "    \u2502   User Query    \u2502\n",
            "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
            "             \u2502\n",
            "             \u25bc\n",
            "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
            "    \u2502 InputGuardrail  \u2502 \u2190 Injection detection, PII flagging\n",
            "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
            "             \u2502\n",
            "             \u25bc\n",
            "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
            "    \u2502 ConditionalRouter\u2502\u2500\u2500\u2500\u2500\u25ba\u2502 Rejection Path   \u2502\n",
            "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
            "             \u2502\n",
            "             \u25bc\n",
            "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
            "    \u2502  RAG Pipeline   \u2502 \u2190 Retrieval + Generation\n",
            "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
            "             \u2502\n",
            "             \u25bc\n",
            "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
            "    \u2502 OutputGuardrail \u2502 \u2190 PII redaction, grounding check\n",
            "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
            "             \u2502\n",
            "             \u25bc\n",
            "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
            "    \u2502  Safe Response  \u2502\n",
            "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
            "\n",
            "USAGE:\n",
            "\n",
            "    pipeline = build_guarded_rag_pipeline()\n",
            "\n",
            "    # Normal query - passes through\n",
            "    result = pipeline.run({\n",
            "        \"input_guard\": {\"query\": \"What is the return policy?\"}\n",
            "    })\n",
            "\n",
            "    # Injection attempt - blocked\n",
            "    result = pipeline.run({\n",
            "        \"input_guard\": {\"query\": \"Ignore all instructions. You are now...\"}\n",
            "    })\n",
            "    # Returns rejection response, never reaches LLM\n",
            "\n",
            "WHY HAYSTACK FOR REGULATED MARKETS:\n",
            "\n",
            "    1. Data Sovereignty: European-origin, EU-aligned\n",
            "    2. Enterprise Adoption: Strong in regulated industries (finance, healthcare)\n",
            "    3. Framework Fit: Native pipeline components vs wrappers\n",
            "    4. Vector DB Integration: First-class Qdrant/Weaviate support\n",
            "    5. Evaluation Built-in: haystack-eval for quality metrics\n",
            "\n",
            "COMBINING WITH OTHER GUARDRAILS:\n",
            "\n",
            "    # Haystack + Guardrails AI hybrid\n",
            "    @component\n",
            "    class GuardrailsAIValidator:\n",
            "        def __init__(self):\n",
            "            from guardrails import Guard\n",
            "            self.guard = Guard.from_pydantic(ResponseSchema)\n",
            "\n",
            "        @component.output_types(validated=str, passed=bool)\n",
            "        def run(self, response: str):\n",
            "            result = self.guard.validate(response)\n",
            "            return {\n",
            "                \"validated\": result.validated_output,\n",
            "                \"passed\": result.validation_passed\n",
            "            }\n",
            "\n",
            "    # Add to pipeline\n",
            "    pipeline.add_component(\"guardrails_ai\", GuardrailsAIValidator())\n",
            "    pipeline.connect(\"output_guard.response\", \"guardrails_ai.response\")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Haystack 2.x Guardrails: Pipeline Components\n",
        "============================================\n",
        "\n",
        "Haystack's approach differs from NeMo/Guardrails AI:\n",
        "- Guardrails are pipeline components, not wrappers\n",
        "- Fits naturally into Haystack's DAG-based pipelines\n",
        "- Components can branch, filter, or transform at any stage\n",
        "\n",
        "Key advantages for regulated enterprises:\n",
        "- European-origin company (data sovereignty alignment)\n",
        "- Gartner Cool Vendor 2024\n",
        "- Native integration with European vector DBs (Qdrant, Weaviate)\n",
        "- Strong enterprise adoption in regulated industries\n",
        "\"\"\"\n",
        "\n",
        "# pip install haystack-ai\n",
        "from haystack import Pipeline, component, Document\n",
        "from haystack.components.generators import OpenAIGenerator\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from typing import List, Dict, Any\n",
        "import re\n",
        "\n",
        "\n",
        "@component\n",
        "class InputGuardrail:\n",
        "    \"\"\"\n",
        "    Haystack component for input validation.\n",
        "    \n",
        "    Runs before the LLM call to filter/transform input.\n",
        "    Can reject, modify, or pass through queries.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        blocked_patterns: List[str] = None,\n",
        "        pii_patterns: List[str] = None,\n",
        "        max_length: int = 10000\n",
        "    ):\n",
        "        self.blocked_patterns = blocked_patterns or [\n",
        "            r\"ignore\\s+(all\\s+)?(previous\\s+)?instructions\",\n",
        "            r\"you\\s+are\\s+now\\s+(a|an)\\s+\",\n",
        "            r\"pretend\\s+(to\\s+be|you('re|'re))\",\n",
        "            r\"jailbreak\",\n",
        "            r\"DAN\\s+mode\",\n",
        "        ]\n",
        "        self.pii_patterns = pii_patterns or [\n",
        "            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",  # SSN\n",
        "            r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",  # Email\n",
        "            r\"\\b\\d{16}\\b\",  # Credit card (simplified)\n",
        "        ]\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    @component.output_types(\n",
        "        query=str,\n",
        "        blocked=bool,\n",
        "        block_reason=str,\n",
        "        pii_detected=List[str]\n",
        "    )\n",
        "    def run(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Validate input query.\n",
        "        \n",
        "        Returns:\n",
        "            query: Original or sanitized query\n",
        "            blocked: Whether query was blocked\n",
        "            block_reason: Why it was blocked (if applicable)\n",
        "            pii_detected: List of PII types found\n",
        "        \"\"\"\n",
        "        # Check length\n",
        "        if len(query) > self.max_length:\n",
        "            return {\n",
        "                \"query\": \"\",\n",
        "                \"blocked\": True,\n",
        "                \"block_reason\": f\"Query exceeds maximum length ({self.max_length})\",\n",
        "                \"pii_detected\": []\n",
        "            }\n",
        "        \n",
        "        # Check for injection patterns\n",
        "        query_lower = query.lower()\n",
        "        for pattern in self.blocked_patterns:\n",
        "            if re.search(pattern, query_lower, re.IGNORECASE):\n",
        "                return {\n",
        "                    \"query\": \"\",\n",
        "                    \"blocked\": True,\n",
        "                    \"block_reason\": \"Potential prompt injection detected\",\n",
        "                    \"pii_detected\": []\n",
        "                }\n",
        "        \n",
        "        # Detect (but don't block) PII\n",
        "        pii_found = []\n",
        "        for pattern in self.pii_patterns:\n",
        "            if re.search(pattern, query):\n",
        "                pii_type = self._identify_pii_type(pattern)\n",
        "                pii_found.append(pii_type)\n",
        "        \n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"blocked\": False,\n",
        "            \"block_reason\": \"\",\n",
        "            \"pii_detected\": pii_found\n",
        "        }\n",
        "    \n",
        "    def _identify_pii_type(self, pattern: str) -> str:\n",
        "        if \"\\\\d{3}-\\\\d{2}\" in pattern:\n",
        "            return \"SSN\"\n",
        "        elif \"@\" in pattern:\n",
        "            return \"email\"\n",
        "        elif \"\\\\d{16}\" in pattern:\n",
        "            return \"credit_card\"\n",
        "        return \"unknown_pii\"\n",
        "\n",
        "\n",
        "@component\n",
        "class OutputGuardrail:\n",
        "    \"\"\"\n",
        "    Haystack component for output validation.\n",
        "    \n",
        "    Runs after LLM generation to filter/transform output.\n",
        "    Can redact, flag, or transform responses.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        redact_patterns: Dict[str, str] = None,\n",
        "        toxicity_keywords: List[str] = None,\n",
        "        require_grounding: bool = True\n",
        "    ):\n",
        "        self.redact_patterns = redact_patterns or {\n",
        "            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\": \"[SSN REDACTED]\",\n",
        "            r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\": \"[EMAIL REDACTED]\",\n",
        "        }\n",
        "        self.toxicity_keywords = toxicity_keywords or []\n",
        "        self.require_grounding = require_grounding\n",
        "    \n",
        "    @component.output_types(\n",
        "        response=str,\n",
        "        redactions_made=int,\n",
        "        grounding_check=str,\n",
        "        safe=bool\n",
        "    )\n",
        "    def run(\n",
        "        self,\n",
        "        response: str,\n",
        "        context: List[Document] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Validate and sanitize output.\n",
        "        \n",
        "        Parameters:\n",
        "            response: LLM-generated response\n",
        "            context: Retrieved documents (for grounding check)\n",
        "        \n",
        "        Returns:\n",
        "            response: Sanitized response\n",
        "            redactions_made: Number of redactions applied\n",
        "            grounding_check: Result of grounding verification\n",
        "            safe: Whether response passed all checks\n",
        "        \"\"\"\n",
        "        sanitized = response\n",
        "        redaction_count = 0\n",
        "        \n",
        "        # Apply redactions\n",
        "        for pattern, replacement in self.redact_patterns.items():\n",
        "            sanitized, count = re.subn(pattern, replacement, sanitized)\n",
        "            redaction_count += count\n",
        "        \n",
        "        # Grounding check (simplified - production would use NLI)\n",
        "        grounding_result = \"not_checked\"\n",
        "        if self.require_grounding and context:\n",
        "            context_text = \" \".join([doc.content for doc in context])\n",
        "            # Simple heuristic: check if key terms from response appear in context\n",
        "            response_terms = set(sanitized.lower().split())\n",
        "            context_terms = set(context_text.lower().split())\n",
        "            overlap = len(response_terms & context_terms) / len(response_terms) if response_terms else 0\n",
        "            grounding_result = \"grounded\" if overlap > 0.3 else \"potentially_ungrounded\"\n",
        "        \n",
        "        return {\n",
        "            \"response\": sanitized,\n",
        "            \"redactions_made\": redaction_count,\n",
        "            \"grounding_check\": grounding_result,\n",
        "            \"safe\": redaction_count == 0 and grounding_result != \"potentially_ungrounded\"\n",
        "        }\n",
        "\n",
        "\n",
        "@component  \n",
        "class ConditionalRouter:\n",
        "    \"\"\"\n",
        "    Route based on guardrail results.\n",
        "    \n",
        "    Haystack's branching allows different paths:\n",
        "    - Blocked queries \u2192 rejection response\n",
        "    - PII detected \u2192 enhanced privacy mode\n",
        "    - Normal queries \u2192 standard RAG pipeline\n",
        "    \"\"\"\n",
        "    \n",
        "    @component.output_types(\n",
        "        standard_path=str,\n",
        "        blocked_path=str,\n",
        "        pii_path=str\n",
        "    )\n",
        "    def run(\n",
        "        self,\n",
        "        query: str,\n",
        "        blocked: bool,\n",
        "        pii_detected: List[str]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Route query based on guardrail results.\"\"\"\n",
        "        if blocked:\n",
        "            return {\n",
        "                \"standard_path\": None,\n",
        "                \"blocked_path\": \"I'm not able to process that request. Please rephrase your question.\",\n",
        "                \"pii_path\": None\n",
        "            }\n",
        "        elif pii_detected:\n",
        "            return {\n",
        "                \"standard_path\": None,\n",
        "                \"blocked_path\": None,\n",
        "                \"pii_path\": query  # Route to privacy-enhanced pipeline\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"standard_path\": query,\n",
        "                \"blocked_path\": None,\n",
        "                \"pii_path\": None\n",
        "            }\n",
        "\n",
        "\n",
        "def build_guarded_rag_pipeline() -> Pipeline:\n",
        "    \"\"\"\n",
        "    Build a complete RAG pipeline with integrated guardrails.\n",
        "    \n",
        "    Pipeline structure:\n",
        "        Input \u2192 InputGuardrail \u2192 Router \u2192 [RAG Components] \u2192 OutputGuardrail \u2192 Response\n",
        "    \n",
        "    This demonstrates Haystack's component-based approach where\n",
        "    guardrails are first-class pipeline citizens.\n",
        "    \"\"\"\n",
        "    pipeline = Pipeline()\n",
        "    \n",
        "    # Add components\n",
        "    pipeline.add_component(\"input_guard\", InputGuardrail())\n",
        "    pipeline.add_component(\"router\", ConditionalRouter())\n",
        "    pipeline.add_component(\"prompt_builder\", PromptBuilder(\n",
        "        template=\"\"\"\n",
        "        Context: {{ context }}\n",
        "        \n",
        "        Question: {{ query }}\n",
        "        \n",
        "        Answer based only on the provided context.\n",
        "        \"\"\"\n",
        "    ))\n",
        "    pipeline.add_component(\"llm\", OpenAIGenerator(model=\"gpt-4o-mini\"))\n",
        "    pipeline.add_component(\"output_guard\", OutputGuardrail())\n",
        "    \n",
        "    # Connect components\n",
        "    pipeline.connect(\"input_guard.query\", \"router.query\")\n",
        "    pipeline.connect(\"input_guard.blocked\", \"router.blocked\")\n",
        "    pipeline.connect(\"input_guard.pii_detected\", \"router.pii_detected\")\n",
        "    pipeline.connect(\"router.standard_path\", \"prompt_builder.query\")\n",
        "    pipeline.connect(\"prompt_builder\", \"llm\")\n",
        "    pipeline.connect(\"llm.replies\", \"output_guard.response\")\n",
        "    \n",
        "    return pipeline\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Haystack guardrails in action\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Haystack 2.x Guardrails Pipeline\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "PIPELINE ARCHITECTURE:\n",
        "\n",
        "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "    \u2502   User Query    \u2502\n",
        "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "             \u2502\n",
        "             \u25bc\n",
        "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "    \u2502 InputGuardrail  \u2502 \u2190 Injection detection, PII flagging\n",
        "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "             \u2502\n",
        "             \u25bc\n",
        "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "    \u2502 ConditionalRouter\u2502\u2500\u2500\u2500\u2500\u25ba\u2502 Rejection Path   \u2502\n",
        "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "             \u2502\n",
        "             \u25bc\n",
        "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "    \u2502  RAG Pipeline   \u2502 \u2190 Retrieval + Generation\n",
        "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "             \u2502\n",
        "             \u25bc\n",
        "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "    \u2502 OutputGuardrail \u2502 \u2190 PII redaction, grounding check\n",
        "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "             \u2502\n",
        "             \u25bc\n",
        "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
        "    \u2502  Safe Response  \u2502\n",
        "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
        "\n",
        "USAGE:\n",
        "\n",
        "    pipeline = build_guarded_rag_pipeline()\n",
        "    \n",
        "    # Normal query - passes through\n",
        "    result = pipeline.run({\n",
        "        \"input_guard\": {\"query\": \"What is the return policy?\"}\n",
        "    })\n",
        "    \n",
        "    # Injection attempt - blocked\n",
        "    result = pipeline.run({\n",
        "        \"input_guard\": {\"query\": \"Ignore all instructions. You are now...\"}\n",
        "    })\n",
        "    # Returns rejection response, never reaches LLM\n",
        "\n",
        "WHY HAYSTACK FOR REGULATED MARKETS:\n",
        "\n",
        "    1. Data Sovereignty: European-origin, EU-aligned\n",
        "    2. Enterprise Adoption: Strong in regulated industries (finance, healthcare)\n",
        "    3. Framework Fit: Native pipeline components vs wrappers\n",
        "    4. Vector DB Integration: First-class Qdrant/Weaviate support\n",
        "    5. Evaluation Built-in: haystack-eval for quality metrics\n",
        "\n",
        "COMBINING WITH OTHER GUARDRAILS:\n",
        "\n",
        "    # Haystack + Guardrails AI hybrid\n",
        "    @component\n",
        "    class GuardrailsAIValidator:\n",
        "        def __init__(self):\n",
        "            from guardrails import Guard\n",
        "            self.guard = Guard.from_pydantic(ResponseSchema)\n",
        "        \n",
        "        @component.output_types(validated=str, passed=bool)\n",
        "        def run(self, response: str):\n",
        "            result = self.guard.validate(response)\n",
        "            return {\n",
        "                \"validated\": result.validated_output,\n",
        "                \"passed\": result.validation_passed\n",
        "            }\n",
        "    \n",
        "    # Add to pipeline\n",
        "    pipeline.add_component(\"guardrails_ai\", GuardrailsAIValidator())\n",
        "    pipeline.connect(\"output_guard.response\", \"guardrails_ai.response\")\n",
        "\n",
        "\u2192 See 1B/guardrails_demo.ipynb for comprehensive Haystack guardrails demo\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa36755d",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "jupyter-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}