{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "38a03d85",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Selection Advisor\n",
            "============================================================\n",
            "\n",
            "MODEL RECOMMENDATION: Support Ticket Classifier\n",
            "============================================================\n",
            "\n",
            "Task Profile:\n",
            "  Complexity:   simple\n",
            "  Sensitivity:  sensitive\n",
            "  Latency:      realtime\n",
            "  Daily Volume: 50,000\n",
            "\n",
            "Recommended: SMALL_OPEN\n",
            "  Examples: Llama 3.1 8B (self-hosted), Mistral 7B\n",
            "\n",
            "Alternatives: small_closed\n",
            "\n",
            "Reasoning:\n",
            "  • SENSITIVE data → prefer self-hosted or regional provider\n",
            "  • Simple task → small open model ideal\n",
            "\n",
            "Warnings:\n",
            "  ⚠ If using cloud API, ensure GDPR-compliant DPA in place\n",
            "\n",
            "Estimated Monthly Cost: €150\n",
            "  (Based on €0.10 per 1K requests)\n",
            "\n",
            "MODEL RECOMMENDATION: Contract Risk Analyzer\n",
            "============================================================\n",
            "\n",
            "Task Profile:\n",
            "  Complexity:   complex\n",
            "  Sensitivity:  restricted\n",
            "  Latency:      batch\n",
            "  Daily Volume: 500\n",
            "\n",
            "Recommended: SELF_HOSTED\n",
            "  Examples: Llama 3.1 70B, Mixtral 8x22B, Qwen 72B\n",
            "\n",
            "Reasoning:\n",
            "  • RESTRICTED data → must self-host (no external APIs)\n",
            "  • Complex task → larger self-hosted model needed\n",
            "\n",
            "Warnings:\n",
            "  ⚠ 70B+ models require significant GPU infrastructure\n",
            "\n",
            "Estimated Monthly Cost: €8\n",
            "  (Based on €0.50 per 1K requests)\n",
            "\n",
            "MODEL RECOMMENDATION: Product Q&A Chatbot\n",
            "============================================================\n",
            "\n",
            "Task Profile:\n",
            "  Complexity:   standard\n",
            "  Sensitivity:  public\n",
            "  Latency:      interactive\n",
            "  Daily Volume: 100,000\n",
            "\n",
            "Recommended: MID_CLOSED\n",
            "  Examples: GPT-4o, Claude Sonnet\n",
            "\n",
            "Alternatives: small_closed\n",
            "\n",
            "Reasoning:\n",
            "  • Standard task → mid-tier model recommended\n",
            "  • High volume (100,000/day) → consider routing\n",
            "\n",
            "Warnings:\n",
            "  ⚠ Route simple queries to smaller model for 50-70% cost reduction\n",
            "\n",
            "Estimated Monthly Cost: €18,000\n",
            "  (Based on €6.00 per 1K requests)\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass, field\n",
        "from enum import Enum\n",
        "from typing import List\n",
        "\n",
        "class TaskComplexity(Enum):\n",
        "    SIMPLE = \"simple\"      # Classification, extraction, formatting\n",
        "    STANDARD = \"standard\"  # Summarization, Q&A, basic generation\n",
        "    COMPLEX = \"complex\"    # Multi-step reasoning, analysis, debugging\n",
        "    AGENTIC = \"agentic\"    # Tool use, planning, self-correction\n",
        "\n",
        "class DataSensitivity(Enum):\n",
        "    PUBLIC = \"public\"           # No restrictions\n",
        "    INTERNAL = \"internal\"       # Business data, contractual API use OK\n",
        "    SENSITIVE = \"sensitive\"     # PII, regulated—regional restrictions apply\n",
        "    RESTRICTED = \"restricted\"   # Cannot leave your infrastructure\n",
        "\n",
        "class LatencyTier(Enum):\n",
        "    REALTIME = \"realtime\"       # < 500ms end-to-end\n",
        "    INTERACTIVE = \"interactive\" # < 2s end-to-end\n",
        "    BATCH = \"batch\"             # Minutes acceptable\n",
        "\n",
        "class ModelClass(Enum):\n",
        "    \"\"\"Model classes representing capability/deployment combinations.\"\"\"\n",
        "    SMALL_OPEN = \"small_open\"           # Llama 8B, Mistral 7B, Phi-3\n",
        "    SMALL_CLOSED = \"small_closed\"       # GPT-4o-mini, Claude Haiku\n",
        "    MID_OPEN = \"mid_open\"               # Llama 70B, Mixtral 8x22B\n",
        "    MID_CLOSED = \"mid_closed\"           # GPT-4o, Claude Sonnet\n",
        "    FRONTIER = \"frontier\"               # Claude Opus, GPT-4.5\n",
        "    SELF_HOSTED = \"self_hosted\"         # Any model, your infrastructure\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TaskProfile:\n",
        "    \"\"\"\n",
        "    Encodes the four dimensions that drive model selection.\n",
        "    \n",
        "    Use this to characterize any LLM task before choosing a model.\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    complexity: TaskComplexity\n",
        "    sensitivity: DataSensitivity\n",
        "    latency: LatencyTier\n",
        "    daily_volume: int\n",
        "    \n",
        "    def requires_self_hosting(self) -> bool:\n",
        "        \"\"\"Restricted data mandates self-hosting.\"\"\"\n",
        "        return self.sensitivity == DataSensitivity.RESTRICTED\n",
        "    \n",
        "    def prefers_self_hosting(self) -> bool:\n",
        "        \"\"\"Sensitive data strongly prefers self-hosting.\"\"\"\n",
        "        return self.sensitivity in (DataSensitivity.SENSITIVE, \n",
        "                                     DataSensitivity.RESTRICTED)\n",
        "    \n",
        "    def is_cost_sensitive(self, threshold: int = 10000) -> bool:\n",
        "        \"\"\"High volume makes per-request cost significant.\"\"\"\n",
        "        return self.daily_volume >= threshold\n",
        "    \n",
        "    def is_latency_constrained(self) -> bool:\n",
        "        \"\"\"Real-time requirements limit model size.\"\"\"\n",
        "        return self.latency == LatencyTier.REALTIME\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelRecommendation:\n",
        "    \"\"\"A model recommendation with reasoning and trade-offs.\"\"\"\n",
        "    primary: ModelClass\n",
        "    primary_examples: List[str]\n",
        "    alternatives: List[ModelClass] = field(default_factory=list)\n",
        "    reasoning: List[str] = field(default_factory=list)\n",
        "    warnings: List[str] = field(default_factory=list)\n",
        "    estimated_cost_per_1k: float = 0.0  # € per 1000 requests (2K tokens avg)\n",
        "\n",
        "\n",
        "def recommend_model(profile: TaskProfile) -> ModelRecommendation:\n",
        "    \"\"\"\n",
        "    Recommend a model class based on task profile.\n",
        "    \n",
        "    Implements the decision logic as executable code.\n",
        "    The reasoning list explains each constraint applied.\n",
        "    \"\"\"\n",
        "    reasoning = []\n",
        "    warnings = []\n",
        "    alternatives = []\n",
        "    \n",
        "    # Hard constraint: restricted data must self-host\n",
        "    if profile.requires_self_hosting():\n",
        "        reasoning.append(\"RESTRICTED data → must self-host (no external APIs)\")\n",
        "        \n",
        "        if profile.complexity in (TaskComplexity.SIMPLE, TaskComplexity.STANDARD):\n",
        "            examples = [\"Llama 3.1 8B\", \"Mistral 7B\", \"Phi-3\"]\n",
        "            reasoning.append(\"Simple/standard task → small model sufficient\")\n",
        "            cost = 0.10  # Rough compute estimate\n",
        "        else:\n",
        "            examples = [\"Llama 3.1 70B\", \"Mixtral 8x22B\", \"Qwen 72B\"]\n",
        "            reasoning.append(\"Complex task → larger self-hosted model needed\")\n",
        "            warnings.append(\"70B+ models require significant GPU infrastructure\")\n",
        "            cost = 0.50\n",
        "        \n",
        "        return ModelRecommendation(\n",
        "            primary=ModelClass.SELF_HOSTED,\n",
        "            primary_examples=examples,\n",
        "            reasoning=reasoning,\n",
        "            warnings=warnings,\n",
        "            estimated_cost_per_1k=cost\n",
        "        )\n",
        "    \n",
        "    # Soft constraint: sensitive data prefers self-hosting\n",
        "    if profile.prefers_self_hosting():\n",
        "        reasoning.append(\"SENSITIVE data → prefer self-hosted or regional provider\")\n",
        "        \n",
        "        if profile.complexity == TaskComplexity.SIMPLE:\n",
        "            return ModelRecommendation(\n",
        "                primary=ModelClass.SMALL_OPEN,\n",
        "                primary_examples=[\"Llama 3.1 8B (self-hosted)\", \"Mistral 7B\"],\n",
        "                alternatives=[ModelClass.SMALL_CLOSED],\n",
        "                reasoning=reasoning + [\"Simple task → small open model ideal\"],\n",
        "                warnings=[\"If using cloud API, ensure GDPR-compliant DPA in place\"],\n",
        "                estimated_cost_per_1k=0.10\n",
        "            )\n",
        "        elif profile.complexity == TaskComplexity.STANDARD:\n",
        "            return ModelRecommendation(\n",
        "                primary=ModelClass.MID_OPEN,\n",
        "                primary_examples=[\"Llama 3.1 70B\", \"Mixtral 8x22B\"],\n",
        "                alternatives=[ModelClass.MID_CLOSED],\n",
        "                reasoning=reasoning + [\"Standard task → mid-tier open model\"],\n",
        "                warnings=[\"Cloud APIs (GPT-4o, Sonnet) viable with proper DPA\"],\n",
        "                estimated_cost_per_1k=0.50\n",
        "            )\n",
        "        else:  # COMPLEX or AGENTIC\n",
        "            reasoning.append(\"Complex task with sensitive data → trade-off required\")\n",
        "            warnings.append(\"Best open models lag frontier by ~6 months on reasoning\")\n",
        "            warnings.append(\"Consider: Can you decompose into sensitive + non-sensitive parts?\")\n",
        "            return ModelRecommendation(\n",
        "                primary=ModelClass.MID_OPEN,\n",
        "                primary_examples=[\"Llama 3.1 70B\", \"Mixtral 8x22B\"],\n",
        "                alternatives=[ModelClass.MID_CLOSED, ModelClass.FRONTIER],\n",
        "                reasoning=reasoning,\n",
        "                warnings=warnings,\n",
        "                estimated_cost_per_1k=0.50\n",
        "            )\n",
        "    \n",
        "    # No sovereignty constraints—optimize for capability and cost\n",
        "    \n",
        "    # Simple tasks: small models suffice\n",
        "    if profile.complexity == TaskComplexity.SIMPLE:\n",
        "        reasoning.append(\"Simple task → small model sufficient\")\n",
        "        \n",
        "        if profile.is_cost_sensitive():\n",
        "            reasoning.append(f\"High volume ({profile.daily_volume:,}/day) → optimize cost\")\n",
        "            return ModelRecommendation(\n",
        "                primary=ModelClass.SMALL_CLOSED,\n",
        "                primary_examples=[\"GPT-4o-mini\", \"Claude Haiku\"],\n",
        "                alternatives=[ModelClass.SMALL_OPEN],\n",
        "                reasoning=reasoning,\n",
        "                estimated_cost_per_1k=0.30\n",
        "            )\n",
        "        else:\n",
        "            return ModelRecommendation(\n",
        "                primary=ModelClass.SMALL_CLOSED,\n",
        "                primary_examples=[\"GPT-4o-mini\", \"Claude Haiku\"],\n",
        "                reasoning=reasoning,\n",
        "                estimated_cost_per_1k=0.30\n",
        "            )\n",
        "    \n",
        "    # Standard tasks: mid-tier models\n",
        "    if profile.complexity == TaskComplexity.STANDARD:\n",
        "        reasoning.append(\"Standard task → mid-tier model recommended\")\n",
        "        \n",
        "        if profile.is_latency_constrained():\n",
        "            reasoning.append(\"Real-time latency → prefer optimized inference\")\n",
        "            warnings.append(\"GPT-4o and Sonnet typically 200-500ms; may need caching\")\n",
        "        \n",
        "        if profile.is_cost_sensitive():\n",
        "            reasoning.append(f\"High volume ({profile.daily_volume:,}/day) → consider routing\")\n",
        "            alternatives.append(ModelClass.SMALL_CLOSED)\n",
        "            warnings.append(\"Route simple queries to smaller model for 50-70% cost reduction\")\n",
        "        \n",
        "        return ModelRecommendation(\n",
        "            primary=ModelClass.MID_CLOSED,\n",
        "            primary_examples=[\"GPT-4o\", \"Claude Sonnet\"],\n",
        "            alternatives=alternatives,\n",
        "            reasoning=reasoning,\n",
        "            warnings=warnings,\n",
        "            estimated_cost_per_1k=6.00\n",
        "        )\n",
        "    \n",
        "    # Complex reasoning: frontier models\n",
        "    if profile.complexity == TaskComplexity.COMPLEX:\n",
        "        reasoning.append(\"Complex reasoning → frontier model recommended\")\n",
        "        \n",
        "        if profile.is_latency_constrained():\n",
        "            warnings.append(\"Frontier models may exceed 500ms on complex prompts\")\n",
        "            warnings.append(\"Consider mid-tier for latency-critical paths\")\n",
        "            alternatives.append(ModelClass.MID_CLOSED)\n",
        "        \n",
        "        if profile.is_cost_sensitive():\n",
        "            warnings.append(f\"At {profile.daily_volume:,}/day, frontier costs add up fast\")\n",
        "            warnings.append(\"Implement routing: frontier for hard queries, mid-tier for rest\")\n",
        "            alternatives.append(ModelClass.MID_CLOSED)\n",
        "        \n",
        "        return ModelRecommendation(\n",
        "            primary=ModelClass.FRONTIER,\n",
        "            primary_examples=[\"Claude Opus\", \"GPT-4.5\", \"Gemini Ultra\"],\n",
        "            alternatives=alternatives,\n",
        "            reasoning=reasoning,\n",
        "            warnings=warnings,\n",
        "            estimated_cost_per_1k=30.00\n",
        "        )\n",
        "    \n",
        "    # Agentic tasks: tool-use optimized models\n",
        "    reasoning.append(\"Agentic task → models optimized for tool use\")\n",
        "    reasoning.append(\"Claude Sonnet and GPT-4o excel at structured tool calling\")\n",
        "    \n",
        "    if profile.is_cost_sensitive():\n",
        "        warnings.append(\"Agentic loops multiply token usage—monitor closely\")\n",
        "    \n",
        "    return ModelRecommendation(\n",
        "        primary=ModelClass.MID_CLOSED,\n",
        "        primary_examples=[\"Claude Sonnet\", \"GPT-4o\"],\n",
        "        alternatives=[ModelClass.FRONTIER],\n",
        "        reasoning=reasoning + [\"Mid-tier often matches frontier on tool use\"],\n",
        "        warnings=warnings,\n",
        "        estimated_cost_per_1k=6.00\n",
        "    )\n",
        "\n",
        "\n",
        "def format_recommendation(profile: TaskProfile, rec: ModelRecommendation) -> str:\n",
        "    \"\"\"Format recommendation as readable output.\"\"\"\n",
        "    lines = [\n",
        "        f\"MODEL RECOMMENDATION: {profile.name}\",\n",
        "        \"=\" * 60,\n",
        "        \"\",\n",
        "        f\"Task Profile:\",\n",
        "        f\"  Complexity:   {profile.complexity.value}\",\n",
        "        f\"  Sensitivity:  {profile.sensitivity.value}\",\n",
        "        f\"  Latency:      {profile.latency.value}\",\n",
        "        f\"  Daily Volume: {profile.daily_volume:,}\",\n",
        "        \"\",\n",
        "        f\"Recommended: {rec.primary.value.upper()}\",\n",
        "        f\"  Examples: {', '.join(rec.primary_examples)}\",\n",
        "        \"\",\n",
        "    ]\n",
        "    \n",
        "    if rec.alternatives:\n",
        "        alt_names = [a.value for a in rec.alternatives]\n",
        "        lines.append(f\"Alternatives: {', '.join(alt_names)}\")\n",
        "        lines.append(\"\")\n",
        "    \n",
        "    lines.append(\"Reasoning:\")\n",
        "    for r in rec.reasoning:\n",
        "        lines.append(f\"  • {r}\")\n",
        "    \n",
        "    if rec.warnings:\n",
        "        lines.append(\"\")\n",
        "        lines.append(\"Warnings:\")\n",
        "        for w in rec.warnings:\n",
        "            lines.append(f\"  ⚠ {w}\")\n",
        "    \n",
        "    lines.append(\"\")\n",
        "    monthly_cost = rec.estimated_cost_per_1k * (profile.daily_volume * 30 / 1000)\n",
        "    lines.append(f\"Estimated Monthly Cost: €{monthly_cost:,.0f}\")\n",
        "    lines.append(f\"  (Based on €{rec.estimated_cost_per_1k:.2f} per 1K requests)\")\n",
        "    \n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Model selection for real scenarios\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Model Selection Advisor\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "\n",
        "# Scenario 1: Support ticket classifier with PII\n",
        "ticket_classifier = TaskProfile(\n",
        "    name=\"Support Ticket Classifier\",\n",
        "    complexity=TaskComplexity.SIMPLE,\n",
        "    sensitivity=DataSensitivity.SENSITIVE,\n",
        "    latency=LatencyTier.REALTIME,\n",
        "    daily_volume=50000\n",
        ")\n",
        "rec1 = recommend_model(ticket_classifier)\n",
        "print(format_recommendation(ticket_classifier, rec1))\n",
        "print()\n",
        "\n",
        "# Scenario 2: Contract analysis for legal team\n",
        "contract_analyzer = TaskProfile(\n",
        "    name=\"Contract Risk Analyzer\", \n",
        "    complexity=TaskComplexity.COMPLEX,\n",
        "    sensitivity=DataSensitivity.RESTRICTED,\n",
        "    latency=LatencyTier.BATCH,\n",
        "    daily_volume=500\n",
        ")\n",
        "rec2 = recommend_model(contract_analyzer)\n",
        "print(format_recommendation(contract_analyzer, rec2))\n",
        "print()\n",
        "\n",
        "# Scenario 3: Customer-facing chatbot\n",
        "chatbot = TaskProfile(\n",
        "    name=\"Product Q&A Chatbot\",\n",
        "    complexity=TaskComplexity.STANDARD,\n",
        "    sensitivity=DataSensitivity.PUBLIC,\n",
        "    latency=LatencyTier.INTERACTIVE,\n",
        "    daily_volume=100000\n",
        ")\n",
        "rec3 = recommend_model(chatbot)\n",
        "print(format_recommendation(chatbot, rec3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0c7ec84a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Model Validation Framework\n",
            "============================================================\n",
            "\n",
            "To validate models on YOUR task:\n",
            "\n",
            "1. BUILD YOUR TEST SET (50-200 examples from production):\n",
            "\n",
            "   test_cases = [\n",
            "       {\"input\": \"Where is my order #12345?\", \"expected\": \"order_status\"},\n",
            "       {\"input\": \"I want a refund\", \"expected\": \"refund_request\"},\n",
            "       {\"input\": \"Your product broke my dishwasher\", \"expected\": \"complaint\"},\n",
            "       # Include edge cases that have caused problems\n",
            "   ]\n",
            "\n",
            "2. DEFINE YOUR EVALUATOR:\n",
            "\n",
            "   # For classification:\n",
            "   def evaluator(actual: str, expected: str) -> float:\n",
            "       return 1.0 if expected.lower() in actual.lower() else 0.0\n",
            "\n",
            "   # For generation (using embedding similarity):\n",
            "   def evaluator(actual: str, expected: str) -> float:\n",
            "       return cosine_similarity(embed(actual), embed(expected))\n",
            "\n",
            "3. DEFINE YOUR REQUIREMENTS:\n",
            "\n",
            "   requirements = {\n",
            "       \"min_accuracy\": 0.92,        # 92% accuracy minimum\n",
            "       \"max_latency_p95_ms\": 500,   # 500ms P95 latency\n",
            "       \"max_cost_per_1k\": 10.0      # €10 per 1000 requests\n",
            "   }\n",
            "\n",
            "4. SET UP MODEL CANDIDATES:\n",
            "\n",
            "   models = {\n",
            "       \"gpt-4o-mini\": (\n",
            "           lambda x: call_openai(x, model=\"gpt-4o-mini\"),\n",
            "           0.00015  # cost per 1K tokens\n",
            "       ),\n",
            "       \"claude-haiku\": (\n",
            "           lambda x: call_anthropic(x, model=\"claude-3-haiku\"),\n",
            "           0.00025\n",
            "       ),\n",
            "       \"llama-8b-local\": (\n",
            "           lambda x: call_local(x, model=\"llama-8b\"),\n",
            "           0.00005  # compute cost estimate\n",
            "       ),\n",
            "   }\n",
            "\n",
            "5. RUN COMPARISON:\n",
            "\n",
            "   results = compare_models(models, test_cases, evaluator, requirements)\n",
            "\n",
            "   for r in results:\n",
            "       status = \"✓\" if r.meets_requirements(**requirements) else \"✗\"\n",
            "       print(f\"{status} {r.model_name}: {r.accuracy:.1%} accuracy, \"\n",
            "             f\"{r.latency_p95_ms:.0f}ms P95, €{r.cost_per_1k_requests:.2f}/1K\")\n",
            "\n",
            "The model that meets all requirements at lowest cost wins.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Dict, Callable, Any\n",
        "from dataclasses import dataclass\n",
        "import time\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkResult:\n",
        "    \"\"\"Results from benchmarking a model on your task.\"\"\"\n",
        "    model_name: str\n",
        "    accuracy: float\n",
        "    latency_p50_ms: float\n",
        "    latency_p95_ms: float\n",
        "    cost_per_1k_requests: float\n",
        "    \n",
        "    def meets_requirements(\n",
        "        self, \n",
        "        min_accuracy: float, \n",
        "        max_latency_p95_ms: float,\n",
        "        max_cost_per_1k: float\n",
        "    ) -> bool:\n",
        "        \"\"\"Check if this model meets all requirements.\"\"\"\n",
        "        return (\n",
        "            self.accuracy >= min_accuracy and\n",
        "            self.latency_p95_ms <= max_latency_p95_ms and\n",
        "            self.cost_per_1k_requests <= max_cost_per_1k\n",
        "        )\n",
        "\n",
        "\n",
        "def benchmark_model(\n",
        "    model_fn: Callable[[str], str],\n",
        "    test_cases: List[Dict[str, str]],\n",
        "    evaluator: Callable[[str, str], float],\n",
        "    cost_per_1k_tokens: float,\n",
        "    avg_tokens_per_request: int = 2000\n",
        ") -> BenchmarkResult:\n",
        "    \"\"\"\n",
        "    Benchmark a single model on your test cases.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model_fn : Callable\n",
        "        Function that takes input string, returns output string\n",
        "    test_cases : List[Dict]\n",
        "        Each dict has 'input' and 'expected' keys\n",
        "    evaluator : Callable\n",
        "        Function(actual, expected) -> score (0.0 to 1.0)\n",
        "    cost_per_1k_tokens : float\n",
        "        Model's price per 1000 tokens\n",
        "    avg_tokens_per_request : int\n",
        "        Expected tokens per request for cost calculation\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    latencies = []\n",
        "    \n",
        "    for case in test_cases:\n",
        "        start = time.perf_counter()\n",
        "        actual = model_fn(case['input'])\n",
        "        latency_ms = (time.perf_counter() - start) * 1000\n",
        "        \n",
        "        score = evaluator(actual, case['expected'])\n",
        "        scores.append(score)\n",
        "        latencies.append(latency_ms)\n",
        "    \n",
        "    latencies.sort()\n",
        "    n = len(latencies)\n",
        "    \n",
        "    return BenchmarkResult(\n",
        "        model_name=\"\",  # Set by caller\n",
        "        accuracy=sum(scores) / len(scores),\n",
        "        latency_p50_ms=latencies[n // 2],\n",
        "        latency_p95_ms=latencies[int(n * 0.95)],\n",
        "        cost_per_1k_requests=(avg_tokens_per_request / 1000) * cost_per_1k_tokens * 1000\n",
        "    )\n",
        "\n",
        "\n",
        "def compare_models(\n",
        "    models: Dict[str, tuple],  # name -> (model_fn, cost_per_1k_tokens)\n",
        "    test_cases: List[Dict[str, str]],\n",
        "    evaluator: Callable[[str, str], float],\n",
        "    requirements: Dict[str, float]  # min_accuracy, max_latency_p95_ms, max_cost_per_1k\n",
        ") -> List[BenchmarkResult]:\n",
        "    \"\"\"\n",
        "    Benchmark multiple models and filter by requirements.\n",
        "    \n",
        "    Returns results sorted by accuracy (highest first),\n",
        "    with models not meeting requirements flagged.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for name, (model_fn, cost) in models.items():\n",
        "        result = benchmark_model(model_fn, test_cases, evaluator, cost)\n",
        "        result.model_name = name\n",
        "        results.append(result)\n",
        "    \n",
        "    # Sort by accuracy descending\n",
        "    results.sort(key=lambda r: r.accuracy, reverse=True)\n",
        "    return results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: How to set up your benchmark\n",
        "# =============================================================================\n",
        "\n",
        "print()\n",
        "print(\"Model Validation Framework\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "To validate models on YOUR task:\n",
        "\n",
        "1. BUILD YOUR TEST SET (50-200 examples from production):\n",
        "\n",
        "   test_cases = [\n",
        "       {\"input\": \"Where is my order #12345?\", \"expected\": \"order_status\"},\n",
        "       {\"input\": \"I want a refund\", \"expected\": \"refund_request\"},\n",
        "       {\"input\": \"Your product broke my dishwasher\", \"expected\": \"complaint\"},\n",
        "       # Include edge cases that have caused problems\n",
        "   ]\n",
        "\n",
        "2. DEFINE YOUR EVALUATOR:\n",
        "\n",
        "   # For classification:\n",
        "   def evaluator(actual: str, expected: str) -> float:\n",
        "       return 1.0 if expected.lower() in actual.lower() else 0.0\n",
        "   \n",
        "   # For generation (using embedding similarity):\n",
        "   def evaluator(actual: str, expected: str) -> float:\n",
        "       return cosine_similarity(embed(actual), embed(expected))\n",
        "\n",
        "3. DEFINE YOUR REQUIREMENTS:\n",
        "\n",
        "   requirements = {\n",
        "       \"min_accuracy\": 0.92,        # 92% accuracy minimum\n",
        "       \"max_latency_p95_ms\": 500,   # 500ms P95 latency\n",
        "       \"max_cost_per_1k\": 10.0      # €10 per 1000 requests\n",
        "   }\n",
        "\n",
        "4. SET UP MODEL CANDIDATES:\n",
        "\n",
        "   models = {\n",
        "       \"gpt-4o-mini\": (\n",
        "           lambda x: call_openai(x, model=\"gpt-4o-mini\"),\n",
        "           0.00015  # cost per 1K tokens\n",
        "       ),\n",
        "       \"claude-haiku\": (\n",
        "           lambda x: call_anthropic(x, model=\"claude-3-haiku\"),\n",
        "           0.00025\n",
        "       ),\n",
        "       \"llama-8b-local\": (\n",
        "           lambda x: call_local(x, model=\"llama-8b\"),\n",
        "           0.00005  # compute cost estimate\n",
        "       ),\n",
        "   }\n",
        "\n",
        "5. RUN COMPARISON:\n",
        "\n",
        "   results = compare_models(models, test_cases, evaluator, requirements)\n",
        "   \n",
        "   for r in results:\n",
        "       status = \"✓\" if r.meets_requirements(**requirements) else \"✗\"\n",
        "       print(f\"{status} {r.model_name}: {r.accuracy:.1%} accuracy, \"\n",
        "             f\"{r.latency_p95_ms:.0f}ms P95, €{r.cost_per_1k_requests:.2f}/1K\")\n",
        "\n",
        "The model that meets all requirements at lowest cost wins.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8209b06d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vision Pipeline Cost Analysis: Invoice Processing\n",
            "=======================================================\n",
            "Daily token consumption:    18,000,000\n",
            "Daily cost:               €      45.00\n",
            "Monthly cost:             €   1,350.00\n",
            "Cost vs text-only:                 6.0×\n",
            "\n",
            "Decision guidance:\n",
            "  • If OCR + text extraction achieves 95%+ accuracy → use text-only\n",
            "  • If documents have complex layouts, tables → vision may be worth 4×\n",
            "  • Consider hybrid: OCR first, vision fallback for low-confidence cases\n"
          ]
        }
      ],
      "source": [
        "def estimate_vision_costs(\n",
        "    images_per_day: int,\n",
        "    tokens_per_image: int = 1500,  # Typical for 1024x1024\n",
        "    text_tokens_per_request: int = 500,\n",
        "    price_per_1k_input: float = 0.0025  # GPT-4o pricing\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Estimate costs for a vision-enabled pipeline.\n",
        "    \n",
        "    Vision tokens typically cost the same as text tokens,\n",
        "    but images consume many more tokens than equivalent text.\n",
        "    \"\"\"\n",
        "    daily_vision_tokens = images_per_day * tokens_per_image\n",
        "    daily_text_tokens = images_per_day * text_tokens_per_request\n",
        "    daily_total_tokens = daily_vision_tokens + daily_text_tokens\n",
        "    \n",
        "    daily_cost = (daily_total_tokens / 1000) * price_per_1k_input\n",
        "    monthly_cost = daily_cost * 30\n",
        "    \n",
        "    # Compare to text-only alternative\n",
        "    text_only_daily = (images_per_day * text_tokens_per_request / 1000) * price_per_1k_input\n",
        "    vision_premium = daily_cost / text_only_daily if text_only_daily > 0 else float('inf')\n",
        "    \n",
        "    return {\n",
        "        'daily_tokens': daily_total_tokens,\n",
        "        'daily_cost': round(daily_cost, 2),\n",
        "        'monthly_cost': round(monthly_cost, 2),\n",
        "        'vision_cost_multiplier': round(vision_premium, 1)\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Vision cost analysis for document processing\n",
        "# =============================================================================\n",
        "\n",
        "# Scenario: Invoice processing system\n",
        "invoice_processing = estimate_vision_costs(\n",
        "    images_per_day=10000,\n",
        "    tokens_per_image=1500,\n",
        "    text_tokens_per_request=300,\n",
        "    price_per_1k_input=0.0025\n",
        ")\n",
        "\n",
        "print(\"Vision Pipeline Cost Analysis: Invoice Processing\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"Daily token consumption:  {invoice_processing['daily_tokens']:>12,}\")\n",
        "print(f\"Daily cost:               €{invoice_processing['daily_cost']:>11,.2f}\")\n",
        "print(f\"Monthly cost:             €{invoice_processing['monthly_cost']:>11,.2f}\")\n",
        "print(f\"Cost vs text-only:        {invoice_processing['vision_cost_multiplier']:>12}×\")\n",
        "print()\n",
        "print(\"Decision guidance:\")\n",
        "print(\"  • If OCR + text extraction achieves 95%+ accuracy → use text-only\")\n",
        "print(\"  • If documents have complex layouts, tables → vision may be worth 4×\")\n",
        "print(\"  • Consider hybrid: OCR first, vision fallback for low-confidence cases\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3917d317",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Structured Output with Instructor\n",
            "=======================================================\n",
            "\n",
            "WHAT INSTRUCTOR DOES:\n",
            "  1. Injects your Pydantic schema into the prompt\n",
            "  2. Parses LLM response into typed object\n",
            "  3. On validation failure → re-prompts with error context\n",
            "  4. Returns validated Pydantic object, not raw text\n",
            "\n",
            "RELIABILITY SPECTRUM:\n",
            "  Prompt-only parsing:     ~85% (model adds explanations, breaks JSON)\n",
            "  Instructor:              ~95-99% (auto-retry with validation feedback)\n",
            "  Constrained generation:  ~99.9% (grammar-enforced, for self-hosted)\n",
            "\n",
            "SETUP:\n",
            "  # Cloud APIs\n",
            "  client = instructor.from_openai(OpenAI())\n",
            "  client = instructor.from_anthropic(Anthropic())\n",
            "\n",
            "  # Local (Ollama)\n",
            "  client = instructor.from_openai(\n",
            "      OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),\n",
            "      mode=instructor.Mode.JSON\n",
            "  )\n",
            "\n",
            "USAGE:\n",
            "  ticket = client.chat.completions.create(\n",
            "      model=\"gpt-4o-mini\",\n",
            "      response_model=SupportTicket,\n",
            "      max_retries=2,\n",
            "      messages=[{\"role\": \"user\", \"content\": raw_message}]\n",
            "  )\n",
            "  # ticket is a SupportTicket object, not a string\n",
            "\n",
            "→ See 1B/demos.ipynb for runnable demo with Ollama\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Structured Output with Instructor\n",
        "# pip install instructor pydantic\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from enum import Enum\n",
        "\n",
        "class Priority(str, Enum):\n",
        "    HIGH = \"high\"\n",
        "    MEDIUM = \"medium\"\n",
        "    LOW = \"low\"\n",
        "\n",
        "class SupportTicket(BaseModel):\n",
        "    \"\"\"Schema for structured extraction - Pydantic does the heavy lifting.\"\"\"\n",
        "    category: str = Field(description=\"Issue category\")\n",
        "    priority: Priority = Field(description=\"Urgency level\")\n",
        "    summary: str = Field(description=\"One-sentence summary\", max_length=200)\n",
        "    entities: List[str] = Field(default_factory=list, description=\"Products/orders mentioned\")\n",
        "    sentiment: float = Field(ge=-1.0, le=1.0, description=\"Sentiment score\")\n",
        "\n",
        "\n",
        "print(\"Structured Output with Instructor\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "WHAT INSTRUCTOR DOES:\n",
        "  1. Injects your Pydantic schema into the prompt\n",
        "  2. Parses LLM response into typed object\n",
        "  3. On validation failure → re-prompts with error context\n",
        "  4. Returns validated Pydantic object, not raw text\n",
        "\n",
        "RELIABILITY SPECTRUM:\n",
        "  Prompt-only parsing:     ~85% (model adds explanations, breaks JSON)\n",
        "  Instructor:              ~95-99% (auto-retry with validation feedback)\n",
        "  Constrained generation:  ~99.9% (grammar-enforced, for self-hosted)\n",
        "\n",
        "SETUP:\n",
        "  # Cloud APIs\n",
        "  client = instructor.from_openai(OpenAI())\n",
        "  client = instructor.from_anthropic(Anthropic())\n",
        "  \n",
        "  # Local (Ollama)\n",
        "  client = instructor.from_openai(\n",
        "      OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\"),\n",
        "      mode=instructor.Mode.JSON\n",
        "  )\n",
        "\n",
        "USAGE:\n",
        "  ticket = client.chat.completions.create(\n",
        "      model=\"gpt-4o-mini\",\n",
        "      response_model=SupportTicket,\n",
        "      max_retries=2,\n",
        "      messages=[{\"role\": \"user\", \"content\": raw_message}]\n",
        "  )\n",
        "  # ticket is a SupportTicket object, not a string\n",
        "\n",
        "→ See 1B/demos.ipynb for runnable demo with Ollama\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e08b3898",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constrained Generation Decision\n",
            "=======================================================\n",
            "\n",
            "Choose your approach:\n",
            "\n",
            "┌─────────────────────┬────────────────┬──────────────────┐\n",
            "│ Approach            │ Reliability    │ Best For         │\n",
            "├─────────────────────┼────────────────┼──────────────────┤\n",
            "│ Prompt + parsing    │ ~85%           │ Prototyping      │\n",
            "│ Instructor          │ ~95-99%        │ Cloud APIs       │\n",
            "│ Outlines/guidance   │ ~99.9%         │ Self-hosted      │\n",
            "│ Native JSON mode    │ ~95%           │ Simple schemas   │\n",
            "└─────────────────────┴────────────────┴──────────────────┘\n",
            "\n",
            "For most production systems, Instructor is the sweet spot:\n",
            "high reliability, great DX, works everywhere.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Structured Generation - Outlines\n",
        "# pip install outlines[ollama]  # or [openai], [anthropic], [transformers], [vllm]\n",
        "\n",
        "\"\"\"\n",
        "Outlines is a unified structured generation library supporting many backends.\n",
        "Capabilities differ based on how you connect:\n",
        "\n",
        "┌────────────────────────────┬──────────────┬─────────────────┐\n",
        "│ Backend                    │ JSON Schemas │ Regex/Grammar   │\n",
        "├────────────────────────────┼──────────────┼─────────────────┤\n",
        "│ Ollama (from_ollama)       │ ✓            │ ✗ (black-box)   │\n",
        "│ OpenAI (from_openai)       │ ✓            │ ✗ (black-box)   │\n",
        "│ Anthropic (from_anthropic) │ ✓            │ ✗ (black-box)   │\n",
        "│ vLLM server (from_vllm)    │ ✓            │ ✗ (API mode)    │\n",
        "│ vLLM local (from_vllm_offline) │ ✓        │ ✓ Full support  │\n",
        "│ HuggingFace (from_transformers)│ ✓        │ ✓ Full support  │\n",
        "│ llama.cpp (from_llamacpp)  │ ✓            │ ✓ Full support  │\n",
        "└────────────────────────────┴──────────────┴─────────────────┘\n",
        "\n",
        "# API backends - JSON schemas via provider's native mode\n",
        "import outlines, ollama\n",
        "model = outlines.from_ollama(ollama.Client(), model_name=\"qwen3:4b\")\n",
        "result = model(\"Classify: payment failed\", MySchema)  # Returns JSON str\n",
        "\n",
        "# Local backends - true token masking, full grammar control\n",
        "from vllm import LLM\n",
        "model = outlines.from_vllm_offline(LLM(\"meta-llama/Llama-3-8B\"))\n",
        "\n",
        "regex_type = outlines.types.regex(r\"PRD-[0-9]{3}\")\n",
        "result = model(\"Generate code:\", regex_type)  # GUARANTEED PRD-XXX\n",
        "\n",
        "DECISION GUIDE:\n",
        "  • APIs (Ollama, OpenAI, vLLM server)? → Instructor has simpler DX\n",
        "  • Self-hosting + need regex/grammar? → Outlines (local backends)\n",
        "  • High-volume GPU inference? → Outlines + vLLM offline (fastest)\n",
        "\n",
        "→ See 1B/demos.ipynb for runnable examples\n",
        "\"\"\"\n",
        "\n",
        "print(\"Constrained Generation Decision\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "Choose your approach:\n",
        "\n",
        "┌─────────────────────┬────────────────┬──────────────────┐\n",
        "│ Approach            │ Reliability    │ Best For         │\n",
        "├─────────────────────┼────────────────┼──────────────────┤\n",
        "│ Prompt + parsing    │ ~85%           │ Prototyping      │\n",
        "│ Instructor          │ ~95-99%        │ Cloud APIs       │\n",
        "│ Outlines/guidance   │ ~99.9%         │ Self-hosted      │\n",
        "│ Native JSON mode    │ ~95%           │ Simple schemas   │\n",
        "└─────────────────────┴────────────────┴──────────────────┘\n",
        "\n",
        "For most production systems, Instructor is the sweet spot:\n",
        "high reliability, great DX, works everywhere.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7dfbd001",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeMo Guardrails: Dialog Flow Control\n",
            "=======================================================\n",
            "\n",
            "RAIL TYPES:\n",
            "\n",
            "1. INPUT RAILS (before LLM):\n",
            "   • Jailbreak detection - \"Ignore previous instructions...\"\n",
            "   • Topic filtering - Reject off-topic requests\n",
            "   • PII masking - Block/redact sensitive data\n",
            "\n",
            "2. OUTPUT RAILS (after LLM):\n",
            "   • Toxicity filtering - Block harmful content\n",
            "   • Factuality checking - Verify against knowledge base\n",
            "\n",
            "3. DIALOG RAILS (Colang flows):\n",
            "   • Define conversation patterns\n",
            "   • Guide users through processes\n",
            "   • Handle edge cases consistently\n",
            "\n",
            "USAGE:\n",
            "  rails = create_guarded_llm(\"./nemo_config\")\n",
            "\n",
            "  response = rails.generate(\n",
            "      messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
            "  )\n",
            "\n",
            "→ See 1B/nemo_config/ for complete config files\n",
            "→ See 1B/guardrails_demo.ipynb for comprehensive guardrails demos\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# NeMo Guardrails - Dialog Flow and Safety Rails\n",
        "# pip install nemoguardrails langchain-openai\n",
        "\n",
        "\"\"\"\n",
        "NeMo Guardrails requires a config directory with:\n",
        "  nemo_config/\n",
        "  ├── config.yml   # Model and rails configuration\n",
        "  ├── rails.co     # Colang dialog flows (user intents, bot responses)\n",
        "  └── prompts.yml  # Custom prompts for safety checks\n",
        "\n",
        "Example config.yml (for Ollama):\n",
        "  models:\n",
        "    - type: main\n",
        "      engine: openai\n",
        "      model: qwen3:4b\n",
        "      parameters:\n",
        "        openai_api_base: http://localhost:11434/v1\n",
        "        openai_api_key: ollama\n",
        "\n",
        "Example rails.co (Colang dialog flows):\n",
        "  define user express greeting\n",
        "      \"hello\"\n",
        "      \"hi\"\n",
        "  \n",
        "  define bot express greeting\n",
        "      \"Hello! How can I help you today?\"\n",
        "  \n",
        "  define flow greeting\n",
        "      user express greeting\n",
        "      bot express greeting\n",
        "  \n",
        "  define user ask off topic\n",
        "      \"what's the weather\"\n",
        "      \"tell me a joke\"\n",
        "  \n",
        "  define bot refuse off topic\n",
        "      \"I'm designed to help with TechCorp products only.\"\n",
        "  \n",
        "  define flow handle off topic\n",
        "      user ask off topic\n",
        "      bot refuse off topic\n",
        "\"\"\"\n",
        "\n",
        "from nemoguardrails import LLMRails, RailsConfig\n",
        "\n",
        "def create_guarded_llm(config_path: str):\n",
        "    \"\"\"Create an LLM with NeMo guardrails applied.\"\"\"\n",
        "    config = RailsConfig.from_path(config_path)\n",
        "    return LLMRails(config)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: NeMo Guardrails overview\n",
        "# =============================================================================\n",
        "\n",
        "print(\"NeMo Guardrails: Dialog Flow Control\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "RAIL TYPES:\n",
        "\n",
        "1. INPUT RAILS (before LLM):\n",
        "   • Jailbreak detection - \"Ignore previous instructions...\"\n",
        "   • Topic filtering - Reject off-topic requests\n",
        "   • PII masking - Block/redact sensitive data\n",
        "\n",
        "2. OUTPUT RAILS (after LLM):\n",
        "   • Toxicity filtering - Block harmful content\n",
        "   • Factuality checking - Verify against knowledge base\n",
        "\n",
        "3. DIALOG RAILS (Colang flows):\n",
        "   • Define conversation patterns\n",
        "   • Guide users through processes\n",
        "   • Handle edge cases consistently\n",
        "\n",
        "USAGE:\n",
        "  rails = create_guarded_llm(\"./nemo_config\")\n",
        "  \n",
        "  response = rails.generate(\n",
        "      messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
        "  )\n",
        "\n",
        "→ See 1B/nemo_config/ for complete config files\n",
        "→ See 1B/guardrails_demo.ipynb for comprehensive guardrails demos\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6fa9cc40",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined Guardrails Strategy\n",
            "=======================================================\n",
            "\n",
            "RECOMMENDED ARCHITECTURE:\n",
            "\n",
            "┌────────────────────────────────────────────────────────┐\n",
            "│                    USER INPUT                          │\n",
            "└────────────────────────────────────────────────────────┘\n",
            "                          │\n",
            "                          ▼\n",
            "┌────────────────────────────────────────────────────────┐\n",
            "│              NEMO GUARDRAILS (Dialog Layer)            │\n",
            "│  • Jailbreak detection                                 │\n",
            "│  • Topic control                                       │\n",
            "│  • Conversation flow management                        │\n",
            "└────────────────────────────────────────────────────────┘\n",
            "                          │\n",
            "                          ▼\n",
            "┌────────────────────────────────────────────────────────┐\n",
            "│                    LLM CALL                            │\n",
            "│  (with Instructor for structured output)               │\n",
            "└────────────────────────────────────────────────────────┘\n",
            "                          │\n",
            "                          ▼\n",
            "┌────────────────────────────────────────────────────────┐\n",
            "│           GUARDRAILS AI (Validation Layer)             │\n",
            "│  • PII redaction                                       │\n",
            "│  • Toxicity filtering                                  │\n",
            "│  • Custom validators                                   │\n",
            "└────────────────────────────────────────────────────────┘\n",
            "                          │\n",
            "                          ▼\n",
            "┌────────────────────────────────────────────────────────┐\n",
            "│                   SAFE OUTPUT                          │\n",
            "└────────────────────────────────────────────────────────┘\n",
            "\n",
            "Why layer guardrails?\n",
            "- NeMo excels at dialog flow and conversation-level control\n",
            "- Guardrails AI excels at field-level validation and Hub ecosystem\n",
            "- Haystack provides pipeline-native components (EU-aligned, data sovereignty focus)\n",
            "- Together they provide defense in depth\n",
            "\n",
            "FRAMEWORK SELECTION:\n",
            "\n",
            "    Using Haystack? → Use pipeline components (InputGuardrail, OutputGuardrail)\n",
            "    Using LangChain? → Use NeMo + Guardrails AI wrappers\n",
            "    Framework-agnostic? → NeMo for dialog + Guardrails AI for validation\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/7r/97vw1q7x18x8btt349jvjpvc0000gn/T/ipykernel_40353/293281384.py:32: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'validators'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  answer: str = Field(\n"
          ]
        }
      ],
      "source": [
        "# Guardrails AI - Field-level Validation with Hub Validators\n",
        "# ⚠️ DEPENDENCY CONFLICT: guardrails-ai requires openai<2.0.0\n",
        "#    This conflicts with Instructor which requires openai>=2.0.0\n",
        "#    Use in separate environment if needed\n",
        "\n",
        "# pip install guardrails-ai\n",
        "# guardrails hub install hub://guardrails/regex_match\n",
        "# guardrails hub install hub://guardrails/toxic_language\n",
        "\n",
        "\"\"\"\n",
        "Guardrails AI provides validators from the Hub:\n",
        "- PII detection and redaction\n",
        "- Toxic language filtering  \n",
        "- Regex pattern matching\n",
        "- Custom LLM-based validation\n",
        "\n",
        "Example validators from Hub:\n",
        "    hub://guardrails/detect_pii\n",
        "    hub://guardrails/toxic_language\n",
        "    hub://guardrails/provenance_llm  # Check if grounded in sources\n",
        "    hub://guardrails/reading_level   # Ensure appropriate complexity\n",
        "\"\"\"\n",
        "\n",
        "from guardrails import Guard\n",
        "from guardrails.hub import DetectPII, ToxicLanguage\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "\n",
        "class CustomerResponse(BaseModel):\n",
        "    \"\"\"Schema for customer-facing responses.\"\"\"\n",
        "    answer: str = Field(\n",
        "        description=\"The response to the customer\",\n",
        "        validators=[\n",
        "            ToxicLanguage(on_fail=\"fix\"),  # Auto-fix toxic content\n",
        "            DetectPII(on_fail=\"fix\"),       # Redact any PII\n",
        "        ]\n",
        "    )\n",
        "    sources: List[str] = Field(\n",
        "        description=\"Sources used to generate the answer\"\n",
        "    )\n",
        "    confidence: float = Field(\n",
        "        ge=0.0, le=1.0,\n",
        "        description=\"Confidence score\"\n",
        "    )\n",
        "\n",
        "\n",
        "def validated_response(\n",
        "    user_query: str,\n",
        "    context: str,\n",
        "    llm_callable\n",
        ") -> CustomerResponse:\n",
        "    \"\"\"\n",
        "    Generate a response with Guardrails AI validation.\n",
        "    \n",
        "    Validators run on the output and can:\n",
        "    - Pass: Output is valid\n",
        "    - Fix: Auto-correct issues (e.g., redact PII)\n",
        "    - Fail: Reject and optionally retry\n",
        "    \"\"\"\n",
        "    guard = Guard.from_pydantic(CustomerResponse)\n",
        "    \n",
        "    result = guard(\n",
        "        llm_callable,\n",
        "        prompt=f\"\"\"\n",
        "        Context: {context}\n",
        "        \n",
        "        Question: {user_query}\n",
        "        \n",
        "        Provide a helpful answer based only on the context.\n",
        "        \"\"\",\n",
        "        num_reasks=2  # Retry twice on validation failure\n",
        "    )\n",
        "    \n",
        "    return result.validated_output\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Combined guardrails strategy\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Combined Guardrails Strategy\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "RECOMMENDED ARCHITECTURE:\n",
        "\n",
        "┌────────────────────────────────────────────────────────┐\n",
        "│                    USER INPUT                          │\n",
        "└────────────────────────────────────────────────────────┘\n",
        "                          │\n",
        "                          ▼\n",
        "┌────────────────────────────────────────────────────────┐\n",
        "│              NEMO GUARDRAILS (Dialog Layer)            │\n",
        "│  • Jailbreak detection                                 │\n",
        "│  • Topic control                                       │\n",
        "│  • Conversation flow management                        │\n",
        "└────────────────────────────────────────────────────────┘\n",
        "                          │\n",
        "                          ▼\n",
        "┌────────────────────────────────────────────────────────┐\n",
        "│                    LLM CALL                            │\n",
        "│  (with Instructor for structured output)               │\n",
        "└────────────────────────────────────────────────────────┘\n",
        "                          │\n",
        "                          ▼\n",
        "┌────────────────────────────────────────────────────────┐\n",
        "│           GUARDRAILS AI (Validation Layer)             │\n",
        "│  • PII redaction                                       │\n",
        "│  • Toxicity filtering                                  │\n",
        "│  • Custom validators                                   │\n",
        "└────────────────────────────────────────────────────────┘\n",
        "                          │\n",
        "                          ▼\n",
        "┌────────────────────────────────────────────────────────┐\n",
        "│                   SAFE OUTPUT                          │\n",
        "└────────────────────────────────────────────────────────┘\n",
        "\n",
        "Why layer guardrails?\n",
        "- NeMo excels at dialog flow and conversation-level control\n",
        "- Guardrails AI excels at field-level validation and Hub ecosystem\n",
        "- Haystack provides pipeline-native components (EU-aligned, data sovereignty focus)\n",
        "- Together they provide defense in depth\n",
        "\n",
        "FRAMEWORK SELECTION:\n",
        "\n",
        "    Using Haystack? → Use pipeline components (InputGuardrail, OutputGuardrail)\n",
        "    Using LangChain? → Use NeMo + Guardrails AI wrappers\n",
        "    Framework-agnostic? → NeMo for dialog + Guardrails AI for validation\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4d86ddbf",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-03 04:15:53 - haystack.tracing.tracer - INFO - tracer.py:199 - auto_enable_tracing() - Auto-enabled tracing for 'OpenTelemetryTracer'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Haystack 2.x Guardrails Pipeline\n",
            "=======================================================\n",
            "\n",
            "PIPELINE ARCHITECTURE:\n",
            "\n",
            "    ┌─────────────────┐\n",
            "    │   User Query    │\n",
            "    └────────┬────────┘\n",
            "             │\n",
            "             ▼\n",
            "    ┌─────────────────┐\n",
            "    │ InputGuardrail  │ ← Injection detection, PII flagging\n",
            "    └────────┬────────┘\n",
            "             │\n",
            "             ▼\n",
            "    ┌─────────────────┐     ┌──────────────────┐\n",
            "    │ ConditionalRouter│────►│ Rejection Path   │\n",
            "    └────────┬────────┘     └──────────────────┘\n",
            "             │\n",
            "             ▼\n",
            "    ┌─────────────────┐\n",
            "    │  RAG Pipeline   │ ← Retrieval + Generation\n",
            "    └────────┬────────┘\n",
            "             │\n",
            "             ▼\n",
            "    ┌─────────────────┐\n",
            "    │ OutputGuardrail │ ← PII redaction, grounding check\n",
            "    └────────┬────────┘\n",
            "             │\n",
            "             ▼\n",
            "    ┌─────────────────┐\n",
            "    │  Safe Response  │\n",
            "    └─────────────────┘\n",
            "\n",
            "USAGE:\n",
            "\n",
            "    pipeline = build_guarded_rag_pipeline()\n",
            "\n",
            "    # Normal query - passes through\n",
            "    result = pipeline.run({\n",
            "        \"input_guard\": {\"query\": \"What is the return policy?\"}\n",
            "    })\n",
            "\n",
            "    # Injection attempt - blocked\n",
            "    result = pipeline.run({\n",
            "        \"input_guard\": {\"query\": \"Ignore all instructions. You are now...\"}\n",
            "    })\n",
            "    # Returns rejection response, never reaches LLM\n",
            "\n",
            "WHY HAYSTACK FOR REGULATED MARKETS:\n",
            "\n",
            "    1. Data Sovereignty: European-origin, EU-aligned\n",
            "    2. Enterprise Adoption: Strong in regulated industries (finance, healthcare)\n",
            "    3. Framework Fit: Native pipeline components vs wrappers\n",
            "    4. Vector DB Integration: First-class Qdrant/Weaviate support\n",
            "    5. Evaluation Built-in: haystack-eval for quality metrics\n",
            "\n",
            "COMBINING WITH OTHER GUARDRAILS:\n",
            "\n",
            "    # Haystack + Guardrails AI hybrid\n",
            "    @component\n",
            "    class GuardrailsAIValidator:\n",
            "        def __init__(self):\n",
            "            from guardrails import Guard\n",
            "            self.guard = Guard.from_pydantic(ResponseSchema)\n",
            "\n",
            "        @component.output_types(validated=str, passed=bool)\n",
            "        def run(self, response: str):\n",
            "            result = self.guard.validate(response)\n",
            "            return {\n",
            "                \"validated\": result.validated_output,\n",
            "                \"passed\": result.validation_passed\n",
            "            }\n",
            "\n",
            "    # Add to pipeline\n",
            "    pipeline.add_component(\"guardrails_ai\", GuardrailsAIValidator())\n",
            "    pipeline.connect(\"output_guard.response\", \"guardrails_ai.response\")\n",
            "\n",
            "→ See 1B/guardrails_demo.ipynb for comprehensive Haystack guardrails demo\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Haystack 2.x Guardrails: Pipeline Components\n",
        "============================================\n",
        "\n",
        "Haystack's approach differs from NeMo/Guardrails AI:\n",
        "- Guardrails are pipeline components, not wrappers\n",
        "- Fits naturally into Haystack's DAG-based pipelines\n",
        "- Components can branch, filter, or transform at any stage\n",
        "\n",
        "Key advantages for regulated enterprises:\n",
        "- European-origin company (data sovereignty alignment)\n",
        "- Gartner Cool Vendor 2024\n",
        "- Native integration with European vector DBs (Qdrant, Weaviate)\n",
        "- Strong enterprise adoption in regulated industries\n",
        "\"\"\"\n",
        "\n",
        "# pip install haystack-ai\n",
        "from haystack import Pipeline, component, Document\n",
        "from haystack.components.generators import OpenAIGenerator\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from typing import List, Dict, Any\n",
        "import re\n",
        "\n",
        "\n",
        "@component\n",
        "class InputGuardrail:\n",
        "    \"\"\"\n",
        "    Haystack component for input validation.\n",
        "    \n",
        "    Runs before the LLM call to filter/transform input.\n",
        "    Can reject, modify, or pass through queries.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        blocked_patterns: List[str] = None,\n",
        "        pii_patterns: List[str] = None,\n",
        "        max_length: int = 10000\n",
        "    ):\n",
        "        self.blocked_patterns = blocked_patterns or [\n",
        "            r\"ignore\\s+(all\\s+)?(previous\\s+)?instructions\",\n",
        "            r\"you\\s+are\\s+now\\s+(a|an)\\s+\",\n",
        "            r\"pretend\\s+(to\\s+be|you('re|'re))\",\n",
        "            r\"jailbreak\",\n",
        "            r\"DAN\\s+mode\",\n",
        "        ]\n",
        "        self.pii_patterns = pii_patterns or [\n",
        "            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\",  # SSN\n",
        "            r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",  # Email\n",
        "            r\"\\b\\d{16}\\b\",  # Credit card (simplified)\n",
        "        ]\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    @component.output_types(\n",
        "        query=str,\n",
        "        blocked=bool,\n",
        "        block_reason=str,\n",
        "        pii_detected=List[str]\n",
        "    )\n",
        "    def run(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Validate input query.\n",
        "        \n",
        "        Returns:\n",
        "            query: Original or sanitized query\n",
        "            blocked: Whether query was blocked\n",
        "            block_reason: Why it was blocked (if applicable)\n",
        "            pii_detected: List of PII types found\n",
        "        \"\"\"\n",
        "        # Check length\n",
        "        if len(query) > self.max_length:\n",
        "            return {\n",
        "                \"query\": \"\",\n",
        "                \"blocked\": True,\n",
        "                \"block_reason\": f\"Query exceeds maximum length ({self.max_length})\",\n",
        "                \"pii_detected\": []\n",
        "            }\n",
        "        \n",
        "        # Check for injection patterns\n",
        "        query_lower = query.lower()\n",
        "        for pattern in self.blocked_patterns:\n",
        "            if re.search(pattern, query_lower, re.IGNORECASE):\n",
        "                return {\n",
        "                    \"query\": \"\",\n",
        "                    \"blocked\": True,\n",
        "                    \"block_reason\": \"Potential prompt injection detected\",\n",
        "                    \"pii_detected\": []\n",
        "                }\n",
        "        \n",
        "        # Detect (but don't block) PII\n",
        "        pii_found = []\n",
        "        for pattern in self.pii_patterns:\n",
        "            if re.search(pattern, query):\n",
        "                pii_type = self._identify_pii_type(pattern)\n",
        "                pii_found.append(pii_type)\n",
        "        \n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"blocked\": False,\n",
        "            \"block_reason\": \"\",\n",
        "            \"pii_detected\": pii_found\n",
        "        }\n",
        "    \n",
        "    def _identify_pii_type(self, pattern: str) -> str:\n",
        "        if \"\\\\d{3}-\\\\d{2}\" in pattern:\n",
        "            return \"SSN\"\n",
        "        elif \"@\" in pattern:\n",
        "            return \"email\"\n",
        "        elif \"\\\\d{16}\" in pattern:\n",
        "            return \"credit_card\"\n",
        "        return \"unknown_pii\"\n",
        "\n",
        "\n",
        "@component\n",
        "class OutputGuardrail:\n",
        "    \"\"\"\n",
        "    Haystack component for output validation.\n",
        "    \n",
        "    Runs after LLM generation to filter/transform output.\n",
        "    Can redact, flag, or transform responses.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        redact_patterns: Dict[str, str] = None,\n",
        "        toxicity_keywords: List[str] = None,\n",
        "        require_grounding: bool = True\n",
        "    ):\n",
        "        self.redact_patterns = redact_patterns or {\n",
        "            r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\": \"[SSN REDACTED]\",\n",
        "            r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\": \"[EMAIL REDACTED]\",\n",
        "        }\n",
        "        self.toxicity_keywords = toxicity_keywords or []\n",
        "        self.require_grounding = require_grounding\n",
        "    \n",
        "    @component.output_types(\n",
        "        response=str,\n",
        "        redactions_made=int,\n",
        "        grounding_check=str,\n",
        "        safe=bool\n",
        "    )\n",
        "    def run(\n",
        "        self,\n",
        "        response: str,\n",
        "        context: List[Document] = None\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Validate and sanitize output.\n",
        "        \n",
        "        Parameters:\n",
        "            response: LLM-generated response\n",
        "            context: Retrieved documents (for grounding check)\n",
        "        \n",
        "        Returns:\n",
        "            response: Sanitized response\n",
        "            redactions_made: Number of redactions applied\n",
        "            grounding_check: Result of grounding verification\n",
        "            safe: Whether response passed all checks\n",
        "        \"\"\"\n",
        "        sanitized = response\n",
        "        redaction_count = 0\n",
        "        \n",
        "        # Apply redactions\n",
        "        for pattern, replacement in self.redact_patterns.items():\n",
        "            sanitized, count = re.subn(pattern, replacement, sanitized)\n",
        "            redaction_count += count\n",
        "        \n",
        "        # Grounding check (simplified - production would use NLI)\n",
        "        grounding_result = \"not_checked\"\n",
        "        if self.require_grounding and context:\n",
        "            context_text = \" \".join([doc.content for doc in context])\n",
        "            # Simple heuristic: check if key terms from response appear in context\n",
        "            response_terms = set(sanitized.lower().split())\n",
        "            context_terms = set(context_text.lower().split())\n",
        "            overlap = len(response_terms & context_terms) / len(response_terms) if response_terms else 0\n",
        "            grounding_result = \"grounded\" if overlap > 0.3 else \"potentially_ungrounded\"\n",
        "        \n",
        "        return {\n",
        "            \"response\": sanitized,\n",
        "            \"redactions_made\": redaction_count,\n",
        "            \"grounding_check\": grounding_result,\n",
        "            \"safe\": redaction_count == 0 and grounding_result != \"potentially_ungrounded\"\n",
        "        }\n",
        "\n",
        "\n",
        "@component  \n",
        "class ConditionalRouter:\n",
        "    \"\"\"\n",
        "    Route based on guardrail results.\n",
        "    \n",
        "    Haystack's branching allows different paths:\n",
        "    - Blocked queries → rejection response\n",
        "    - PII detected → enhanced privacy mode\n",
        "    - Normal queries → standard RAG pipeline\n",
        "    \"\"\"\n",
        "    \n",
        "    @component.output_types(\n",
        "        standard_path=str,\n",
        "        blocked_path=str,\n",
        "        pii_path=str\n",
        "    )\n",
        "    def run(\n",
        "        self,\n",
        "        query: str,\n",
        "        blocked: bool,\n",
        "        pii_detected: List[str]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Route query based on guardrail results.\"\"\"\n",
        "        if blocked:\n",
        "            return {\n",
        "                \"standard_path\": None,\n",
        "                \"blocked_path\": \"I'm not able to process that request. Please rephrase your question.\",\n",
        "                \"pii_path\": None\n",
        "            }\n",
        "        elif pii_detected:\n",
        "            return {\n",
        "                \"standard_path\": None,\n",
        "                \"blocked_path\": None,\n",
        "                \"pii_path\": query  # Route to privacy-enhanced pipeline\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"standard_path\": query,\n",
        "                \"blocked_path\": None,\n",
        "                \"pii_path\": None\n",
        "            }\n",
        "\n",
        "\n",
        "def build_guarded_rag_pipeline() -> Pipeline:\n",
        "    \"\"\"\n",
        "    Build a complete RAG pipeline with integrated guardrails.\n",
        "    \n",
        "    Pipeline structure:\n",
        "        Input → InputGuardrail → Router → [RAG Components] → OutputGuardrail → Response\n",
        "    \n",
        "    This demonstrates Haystack's component-based approach where\n",
        "    guardrails are first-class pipeline citizens.\n",
        "    \"\"\"\n",
        "    pipeline = Pipeline()\n",
        "    \n",
        "    # Add components\n",
        "    pipeline.add_component(\"input_guard\", InputGuardrail())\n",
        "    pipeline.add_component(\"router\", ConditionalRouter())\n",
        "    pipeline.add_component(\"prompt_builder\", PromptBuilder(\n",
        "        template=\"\"\"\n",
        "        Context: {{ context }}\n",
        "        \n",
        "        Question: {{ query }}\n",
        "        \n",
        "        Answer based only on the provided context.\n",
        "        \"\"\"\n",
        "    ))\n",
        "    pipeline.add_component(\"llm\", OpenAIGenerator(model=\"gpt-4o-mini\"))\n",
        "    pipeline.add_component(\"output_guard\", OutputGuardrail())\n",
        "    \n",
        "    # Connect components\n",
        "    pipeline.connect(\"input_guard.query\", \"router.query\")\n",
        "    pipeline.connect(\"input_guard.blocked\", \"router.blocked\")\n",
        "    pipeline.connect(\"input_guard.pii_detected\", \"router.pii_detected\")\n",
        "    pipeline.connect(\"router.standard_path\", \"prompt_builder.query\")\n",
        "    pipeline.connect(\"prompt_builder\", \"llm\")\n",
        "    pipeline.connect(\"llm.replies\", \"output_guard.response\")\n",
        "    \n",
        "    return pipeline\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Haystack guardrails in action\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Haystack 2.x Guardrails Pipeline\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "PIPELINE ARCHITECTURE:\n",
        "\n",
        "    ┌─────────────────┐\n",
        "    │   User Query    │\n",
        "    └────────┬────────┘\n",
        "             │\n",
        "             ▼\n",
        "    ┌─────────────────┐\n",
        "    │ InputGuardrail  │ ← Injection detection, PII flagging\n",
        "    └────────┬────────┘\n",
        "             │\n",
        "             ▼\n",
        "    ┌─────────────────┐     ┌──────────────────┐\n",
        "    │ ConditionalRouter│────►│ Rejection Path   │\n",
        "    └────────┬────────┘     └──────────────────┘\n",
        "             │\n",
        "             ▼\n",
        "    ┌─────────────────┐\n",
        "    │  RAG Pipeline   │ ← Retrieval + Generation\n",
        "    └────────┬────────┘\n",
        "             │\n",
        "             ▼\n",
        "    ┌─────────────────┐\n",
        "    │ OutputGuardrail │ ← PII redaction, grounding check\n",
        "    └────────┬────────┘\n",
        "             │\n",
        "             ▼\n",
        "    ┌─────────────────┐\n",
        "    │  Safe Response  │\n",
        "    └─────────────────┘\n",
        "\n",
        "USAGE:\n",
        "\n",
        "    pipeline = build_guarded_rag_pipeline()\n",
        "    \n",
        "    # Normal query - passes through\n",
        "    result = pipeline.run({\n",
        "        \"input_guard\": {\"query\": \"What is the return policy?\"}\n",
        "    })\n",
        "    \n",
        "    # Injection attempt - blocked\n",
        "    result = pipeline.run({\n",
        "        \"input_guard\": {\"query\": \"Ignore all instructions. You are now...\"}\n",
        "    })\n",
        "    # Returns rejection response, never reaches LLM\n",
        "\n",
        "WHY HAYSTACK FOR REGULATED MARKETS:\n",
        "\n",
        "    1. Data Sovereignty: European-origin, EU-aligned\n",
        "    2. Enterprise Adoption: Strong in regulated industries (finance, healthcare)\n",
        "    3. Framework Fit: Native pipeline components vs wrappers\n",
        "    4. Vector DB Integration: First-class Qdrant/Weaviate support\n",
        "    5. Evaluation Built-in: haystack-eval for quality metrics\n",
        "\n",
        "COMBINING WITH OTHER GUARDRAILS:\n",
        "\n",
        "    # Haystack + Guardrails AI hybrid\n",
        "    @component\n",
        "    class GuardrailsAIValidator:\n",
        "        def __init__(self):\n",
        "            from guardrails import Guard\n",
        "            self.guard = Guard.from_pydantic(ResponseSchema)\n",
        "        \n",
        "        @component.output_types(validated=str, passed=bool)\n",
        "        def run(self, response: str):\n",
        "            result = self.guard.validate(response)\n",
        "            return {\n",
        "                \"validated\": result.validated_output,\n",
        "                \"passed\": result.validation_passed\n",
        "            }\n",
        "    \n",
        "    # Add to pipeline\n",
        "    pipeline.add_component(\"guardrails_ai\", GuardrailsAIValidator())\n",
        "    pipeline.connect(\"output_guard.response\", \"guardrails_ai.response\")\n",
        "\n",
        "→ See 1B/guardrails_demo.ipynb for comprehensive Haystack guardrails demo\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "aa36755d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hallucination Detection Strategies\n",
            "=======================================================\n",
            "\n",
            "DETECTION APPROACHES (by reliability and cost):\n",
            "\n",
            "1. SELF-CONSISTENCY (cheap, moderate reliability)\n",
            "   - Generate multiple responses with temperature > 0\n",
            "   - Check if responses agree on factual claims\n",
            "   - Disagreement suggests uncertainty/hallucination\n",
            "\n",
            "   Use when: High volume, cost-sensitive, can tolerate some misses\n",
            "\n",
            "2. NLI-BASED (moderate cost, good for intrinsic)\n",
            "   - Use NLI model to check: context → response\n",
            "   - Catches contradictions with provided context\n",
            "   - Fast inference (~50ms with small NLI model)\n",
            "\n",
            "   Use when: RAG systems, document Q&A, grounded generation\n",
            "\n",
            "3. LLM-AS-JUDGE (expensive, high reliability)\n",
            "   - Ask GPT-4/Claude to evaluate faithfulness\n",
            "   - Can catch subtle issues NLI misses\n",
            "   - ~80% agreement with human judgment\n",
            "\n",
            "   Use when: High-stakes outputs, quality sampling, evaluation\n",
            "\n",
            "4. TOKEN-LEVEL DETECTION - HaluGate (new, fast)\n",
            "   - ModernBERT-based, runs at inference time\n",
            "   - Flags tokens not supported by context\n",
            "   - No LLM-as-judge latency\n",
            "\n",
            "   Use when: Real-time detection, RAG with tool context\n",
            "\n",
            "RECOMMENDED STACK:\n",
            "┌─────────────────────────────────────────────────────┐\n",
            "│  Real-time: NLI check on all responses (~50ms)     │\n",
            "│  Sampling: LLM-as-judge on 5% of traffic           │\n",
            "│  High-stakes: Human review queue for flagged items │\n",
            "└─────────────────────────────────────────────────────┘\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple\n",
        "from enum import Enum\n",
        "\n",
        "class HallucinationType(Enum):\n",
        "    INTRINSIC = \"intrinsic\"      # Contradicts provided context\n",
        "    EXTRINSIC = \"extrinsic\"      # Fabricated information\n",
        "    FAITHFULNESS = \"faithfulness\" # Diverges from instructions\n",
        "\n",
        "@dataclass\n",
        "class HallucinationCheck:\n",
        "    \"\"\"Result of hallucination detection.\"\"\"\n",
        "    is_hallucinated: bool\n",
        "    hallucination_type: Optional[HallucinationType]\n",
        "    confidence: float  # 0-1, confidence in the detection\n",
        "    problematic_spans: List[Tuple[int, int]]  # Character offsets\n",
        "    explanation: str\n",
        "\n",
        "\n",
        "def check_faithfulness_nli(\n",
        "    response: str,\n",
        "    context: str,\n",
        "    nli_model  # Natural Language Inference model\n",
        ") -> HallucinationCheck:\n",
        "    \"\"\"\n",
        "    Check if response is faithful to context using NLI.\n",
        "    \n",
        "    Natural Language Inference classifies text pairs as:\n",
        "    - Entailment: Response follows from context\n",
        "    - Contradiction: Response contradicts context\n",
        "    - Neutral: Response neither follows nor contradicts\n",
        "    \n",
        "    This catches intrinsic hallucinations where the model\n",
        "    contradicts its provided context.\n",
        "    \"\"\"\n",
        "    # Break response into claims\n",
        "    claims = extract_claims(response)\n",
        "    \n",
        "    contradictions = []\n",
        "    for i, claim in enumerate(claims):\n",
        "        # NLI check: does context entail this claim?\n",
        "        result = nli_model.predict(\n",
        "            premise=context,\n",
        "            hypothesis=claim\n",
        "        )\n",
        "        \n",
        "        if result.label == \"contradiction\":\n",
        "            contradictions.append((claim, result.confidence))\n",
        "    \n",
        "    if contradictions:\n",
        "        return HallucinationCheck(\n",
        "            is_hallucinated=True,\n",
        "            hallucination_type=HallucinationType.INTRINSIC,\n",
        "            confidence=max(c[1] for c in contradictions),\n",
        "            problematic_spans=find_spans(response, [c[0] for c in contradictions]),\n",
        "            explanation=f\"Found {len(contradictions)} claims contradicting context\"\n",
        "        )\n",
        "    \n",
        "    return HallucinationCheck(\n",
        "        is_hallucinated=False,\n",
        "        hallucination_type=None,\n",
        "        confidence=0.95,\n",
        "        problematic_spans=[],\n",
        "        explanation=\"Response appears faithful to context\"\n",
        "    )\n",
        "\n",
        "\n",
        "def extract_claims(text: str) -> List[str]:\n",
        "    \"\"\"Extract atomic claims from text for verification.\"\"\"\n",
        "    # Simplified - production would use a claim extraction model\n",
        "    sentences = text.split('. ')\n",
        "    return [s.strip() for s in sentences if len(s.strip()) > 10]\n",
        "\n",
        "\n",
        "def find_spans(text: str, claims: List[str]) -> List[Tuple[int, int]]:\n",
        "    \"\"\"Find character spans of claims in original text.\"\"\"\n",
        "    spans = []\n",
        "    for claim in claims:\n",
        "        start = text.find(claim)\n",
        "        if start != -1:\n",
        "            spans.append((start, start + len(claim)))\n",
        "    return spans\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Hallucination detection approaches\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Hallucination Detection Strategies\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "DETECTION APPROACHES (by reliability and cost):\n",
        "\n",
        "1. SELF-CONSISTENCY (cheap, moderate reliability)\n",
        "   - Generate multiple responses with temperature > 0\n",
        "   - Check if responses agree on factual claims\n",
        "   - Disagreement suggests uncertainty/hallucination\n",
        "   \n",
        "   Use when: High volume, cost-sensitive, can tolerate some misses\n",
        "\n",
        "2. NLI-BASED (moderate cost, good for intrinsic)\n",
        "   - Use NLI model to check: context → response\n",
        "   - Catches contradictions with provided context\n",
        "   - Fast inference (~50ms with small NLI model)\n",
        "   \n",
        "   Use when: RAG systems, document Q&A, grounded generation\n",
        "\n",
        "3. LLM-AS-JUDGE (expensive, high reliability)\n",
        "   - Ask GPT-4/Claude to evaluate faithfulness\n",
        "   - Can catch subtle issues NLI misses\n",
        "   - ~80% agreement with human judgment\n",
        "   \n",
        "   Use when: High-stakes outputs, quality sampling, evaluation\n",
        "\n",
        "4. TOKEN-LEVEL DETECTION - HaluGate (new, fast)\n",
        "   - ModernBERT-based, runs at inference time\n",
        "   - Flags tokens not supported by context\n",
        "   - No LLM-as-judge latency\n",
        "   \n",
        "   Use when: Real-time detection, RAG with tool context\n",
        "\n",
        "RECOMMENDED STACK:\n",
        "┌─────────────────────────────────────────────────────┐\n",
        "│  Real-time: NLI check on all responses (~50ms)     │\n",
        "│  Sampling: LLM-as-judge on 5% of traffic           │\n",
        "│  High-stakes: Human review queue for flagged items │\n",
        "└─────────────────────────────────────────────────────┘\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "938ed407",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hallucination Mitigation Checklist\n",
            "=======================================================\n",
            "\n",
            "PROMPT-LEVEL MITIGATIONS:\n",
            "☐ Include \"I don't know\" permission explicitly\n",
            "☐ Place context BEFORE the question (recency bias)\n",
            "☐ Require citations/quotes from context\n",
            "☐ Use specific, unambiguous questions\n",
            "☐ Limit scope: \"Based ONLY on the context...\"\n",
            "\n",
            "RETRIEVAL-LEVEL MITIGATIONS:\n",
            "☐ Retrieve more chunks than needed, rerank\n",
            "☐ Include metadata (dates, sources) in context\n",
            "☐ Use hybrid search (dense + sparse) for better recall\n",
            "☐ Chunk at semantic boundaries, not arbitrary lengths\n",
            "\n",
            "GENERATION-LEVEL MITIGATIONS:\n",
            "☐ Lower temperature for factual tasks (0.0-0.3)\n",
            "☐ Use self-consistency for critical outputs\n",
            "☐ Implement confidence scoring\n",
            "☐ Stream with early stopping on uncertainty signals\n",
            "\n",
            "SYSTEM-LEVEL MITIGATIONS:\n",
            "☐ Deploy HaluGate or NLI-based detection\n",
            "☐ Sample outputs for LLM-as-judge evaluation\n",
            "☐ Build feedback loops: user reports → retraining data\n",
            "☐ Maintain \"known facts\" cache for frequent queries\n",
            "\n",
            "COST-EFFECTIVE STACK:\n",
            "    Production traffic → NLI check (all) → HaluGate (RAG)\n",
            "    Quality sampling → LLM-as-judge (5%)\n",
            "    Critical decisions → Human review queue\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def build_grounded_prompt(\n",
        "    query: str,\n",
        "    retrieved_context: str,\n",
        "    instructions: str = \"\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Build a prompt that encourages grounded responses.\n",
        "    \n",
        "    Key techniques:\n",
        "    1. Explicit grounding instruction\n",
        "    2. Context before question (recency bias)\n",
        "    3. \"I don't know\" permission\n",
        "    4. Citation requirement\n",
        "    \"\"\"\n",
        "    return f\"\"\"You are a helpful assistant that answers questions based ONLY on the provided context.\n",
        "\n",
        "RULES:\n",
        "- Answer ONLY based on information in the CONTEXT below\n",
        "- If the context doesn't contain the answer, say \"I don't have information about that in the provided documents\"\n",
        "- Quote or paraphrase directly from the context\n",
        "- Never make up information\n",
        "\n",
        "CONTEXT:\n",
        "{retrieved_context}\n",
        "\n",
        "QUESTION: {query}\n",
        "\n",
        "{instructions}\n",
        "\n",
        "Provide your answer, citing the relevant parts of the context:\"\"\"\n",
        "\n",
        "\n",
        "def implement_self_consistency(\n",
        "    prompt: str,\n",
        "    llm_callable,\n",
        "    num_samples: int = 5,\n",
        "    temperature: float = 0.7\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Generate multiple responses and check consistency.\n",
        "    \n",
        "    Inconsistent responses suggest the model is uncertain\n",
        "    and may be hallucinating.\n",
        "    \n",
        "    Returns the most common response if consistent,\n",
        "    or flags uncertainty if responses diverge.\n",
        "    \"\"\"\n",
        "    responses = []\n",
        "    for _ in range(num_samples):\n",
        "        response = llm_callable(prompt, temperature=temperature)\n",
        "        responses.append(response)\n",
        "    \n",
        "    # Check consistency (simplified - production would use semantic similarity)\n",
        "    unique_responses = len(set(responses))\n",
        "    consistency_score = 1 - (unique_responses - 1) / num_samples\n",
        "    \n",
        "    # Find most common response\n",
        "    from collections import Counter\n",
        "    response_counts = Counter(responses)\n",
        "    most_common = response_counts.most_common(1)[0][0]\n",
        "    \n",
        "    return {\n",
        "        'response': most_common,\n",
        "        'consistency_score': consistency_score,\n",
        "        'is_consistent': consistency_score > 0.6,\n",
        "        'num_unique': unique_responses\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Hallucination mitigation checklist\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Hallucination Mitigation Checklist\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "PROMPT-LEVEL MITIGATIONS:\n",
        "☐ Include \"I don't know\" permission explicitly\n",
        "☐ Place context BEFORE the question (recency bias)\n",
        "☐ Require citations/quotes from context\n",
        "☐ Use specific, unambiguous questions\n",
        "☐ Limit scope: \"Based ONLY on the context...\"\n",
        "\n",
        "RETRIEVAL-LEVEL MITIGATIONS:\n",
        "☐ Retrieve more chunks than needed, rerank\n",
        "☐ Include metadata (dates, sources) in context\n",
        "☐ Use hybrid search (dense + sparse) for better recall\n",
        "☐ Chunk at semantic boundaries, not arbitrary lengths\n",
        "\n",
        "GENERATION-LEVEL MITIGATIONS:\n",
        "☐ Lower temperature for factual tasks (0.0-0.3)\n",
        "☐ Use self-consistency for critical outputs\n",
        "☐ Implement confidence scoring\n",
        "☐ Stream with early stopping on uncertainty signals\n",
        "\n",
        "SYSTEM-LEVEL MITIGATIONS:\n",
        "☐ Deploy HaluGate or NLI-based detection\n",
        "☐ Sample outputs for LLM-as-judge evaluation\n",
        "☐ Build feedback loops: user reports → retraining data\n",
        "☐ Maintain \"known facts\" cache for frequent queries\n",
        "\n",
        "COST-EFFECTIVE STACK:\n",
        "    Production traffic → NLI check (all) → HaluGate (RAG)\n",
        "    Quality sampling → LLM-as-judge (5%)\n",
        "    Critical decisions → Human review queue\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "67ee25cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM Routing Economics: Customer Support\n",
            "=======================================================\n",
            "Daily requests:                  100,000\n",
            "Daily cost (no routing): €      3,000.00\n",
            "Daily cost (with routing):€       441.00\n",
            "Daily savings:           €      2,559.00\n",
            "Monthly savings:         €     76,770.00\n",
            "Savings percentage:                85.3%\n"
          ]
        }
      ],
      "source": [
        "def calculate_routing_savings(\n",
        "    daily_requests: int,\n",
        "    complexity_distribution: dict,  # {\"simple\": 0.7, \"standard\": 0.2, \"complex\": 0.1}\n",
        "    model_costs: dict,  # {\"simple\": 0.0001, \"standard\": 0.001, \"complex\": 0.01}\n",
        "    frontier_cost: float = 0.01,  # Cost if using frontier for everything\n",
        "    tokens_per_request: int = 2000\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Calculate savings from intelligent routing vs. using frontier model for all.\n",
        "    \n",
        "    The key insight: ~70% of production traffic is simple enough for\n",
        "    the smallest capable model.\n",
        "    \"\"\"\n",
        "    # Cost without routing (frontier for everything)\n",
        "    daily_tokens = daily_requests * tokens_per_request\n",
        "    daily_frontier_cost = (daily_tokens / 1000) * frontier_cost\n",
        "    \n",
        "    # Cost with routing\n",
        "    daily_routed_cost = 0\n",
        "    for complexity, fraction in complexity_distribution.items():\n",
        "        tier_requests = daily_requests * fraction\n",
        "        tier_tokens = tier_requests * tokens_per_request\n",
        "        tier_cost = (tier_tokens / 1000) * model_costs[complexity]\n",
        "        daily_routed_cost += tier_cost\n",
        "    \n",
        "    daily_savings = daily_frontier_cost - daily_routed_cost\n",
        "    \n",
        "    return {\n",
        "        'daily_frontier_cost': round(daily_frontier_cost, 2),\n",
        "        'daily_routed_cost': round(daily_routed_cost, 2),\n",
        "        'daily_savings': round(daily_savings, 2),\n",
        "        'monthly_savings': round(daily_savings * 30, 2),\n",
        "        'savings_percent': round((daily_savings / daily_frontier_cost) * 100, 1)\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Routing economics for a support system\n",
        "# =============================================================================\n",
        "\n",
        "# Scenario: Customer support system with 100K daily queries\n",
        "support_routing = calculate_routing_savings(\n",
        "    daily_requests=100000,\n",
        "    complexity_distribution={\n",
        "        \"simple\": 0.70,   # FAQ, status checks, simple questions\n",
        "        \"standard\": 0.20, # Explanations, multi-step answers\n",
        "        \"complex\": 0.10   # Analysis, debugging, complaints\n",
        "    },\n",
        "    model_costs={\n",
        "        \"simple\": 0.00015,   # GPT-4o-mini / Llama 8B\n",
        "        \"standard\": 0.003,   # Claude Sonnet / GPT-4o\n",
        "        \"complex\": 0.015     # Claude Opus\n",
        "    },\n",
        "    frontier_cost=0.015,  # If using Opus for everything\n",
        "    tokens_per_request=2000\n",
        ")\n",
        "\n",
        "print(\"LLM Routing Economics: Customer Support\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"Daily requests:          {100000:>15,}\")\n",
        "print(f\"Daily cost (no routing): €{support_routing['daily_frontier_cost']:>14,.2f}\")\n",
        "print(f\"Daily cost (with routing):€{support_routing['daily_routed_cost']:>13,.2f}\")\n",
        "print(f\"Daily savings:           €{support_routing['daily_savings']:>14,.2f}\")\n",
        "print(f\"Monthly savings:         €{support_routing['monthly_savings']:>14,.2f}\")\n",
        "print(f\"Savings percentage:      {support_routing['savings_percent']:>14}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "2c58907a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Router: Intent-Based Routing\n",
            "============================================================\n",
            "\n",
            "Unlike learned routers (RouteLLM), Semantic Router uses\n",
            "embedding similarity to match queries to predefined routes.\n",
            "\n",
            "HOW IT WORKS:\n",
            "\n",
            "    1. Define routes with example utterances:\n",
            "       billing_route = Route(\n",
            "           name=\"billing\",\n",
            "           utterances=[\"What's my balance?\", \"Pay my bill\"]\n",
            "       )\n",
            "\n",
            "    2. Encode utterances → embedding vectors\n",
            "\n",
            "    3. At runtime: query → embedding → cosine similarity → route\n",
            "\n",
            "┌─────────────────┬───────────────────┬───────────────────┐\n",
            "│ Feature         │ Semantic Router   │ RouteLLM          │\n",
            "├─────────────────┼───────────────────┼───────────────────┤\n",
            "│ Routing logic   │ Defined           │ Learned           │\n",
            "│ Training needed │ No (examples)     │ Yes (preferences) │\n",
            "│ Explainability  │ High              │ Low               │\n",
            "│ Output          │ Category/Intent   │ Model selection   │\n",
            "│ Cold start      │ Works immediately │ Needs data        │\n",
            "└─────────────────┴───────────────────┴───────────────────┘\n",
            "\n",
            "USE CASES:\n",
            "  • Intent classification (billing, technical, sales)\n",
            "  • Guardrails (block certain intents)\n",
            "  • Agent routing (which specialist handles this?)\n",
            "  • Multi-model pipelines (different model per category)\n",
            "\n",
            "COMBINED APPROACH:\n",
            "  Query → [Semantic Router] → Intent category\n",
            "                 │\n",
            "                 ├── billing → [mini model]\n",
            "                 ├── technical → [strong model]  \n",
            "                 └── sales → [persuasive model]\n",
            "\n",
            "→ See 1B/cost_optimization_demo.ipynb for working demos\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Semantic Router: Intent-Based Routing\n",
        "# For demos, see: 1B/cost_optimization_demo.ipynb\n",
        "\n",
        "print(\"Semantic Router: Intent-Based Routing\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "Unlike learned routers (RouteLLM), Semantic Router uses\n",
        "embedding similarity to match queries to predefined routes.\n",
        "\n",
        "HOW IT WORKS:\n",
        "\n",
        "    1. Define routes with example utterances:\n",
        "       billing_route = Route(\n",
        "           name=\"billing\",\n",
        "           utterances=[\"What's my balance?\", \"Pay my bill\"]\n",
        "       )\n",
        "    \n",
        "    2. Encode utterances → embedding vectors\n",
        "    \n",
        "    3. At runtime: query → embedding → cosine similarity → route\n",
        "\n",
        "┌─────────────────┬───────────────────┬───────────────────┐\n",
        "│ Feature         │ Semantic Router   │ RouteLLM          │\n",
        "├─────────────────┼───────────────────┼───────────────────┤\n",
        "│ Routing logic   │ Defined           │ Learned           │\n",
        "│ Training needed │ No (examples)     │ Yes (preferences) │\n",
        "│ Explainability  │ High              │ Low               │\n",
        "│ Output          │ Category/Intent   │ Model selection   │\n",
        "│ Cold start      │ Works immediately │ Needs data        │\n",
        "└─────────────────┴───────────────────┴───────────────────┘\n",
        "\n",
        "USE CASES:\n",
        "  • Intent classification (billing, technical, sales)\n",
        "  • Guardrails (block certain intents)\n",
        "  • Agent routing (which specialist handles this?)\n",
        "  • Multi-model pipelines (different model per category)\n",
        "\n",
        "COMBINED APPROACH:\n",
        "  Query → [Semantic Router] → Intent category\n",
        "                 │\n",
        "                 ├── billing → [mini model]\n",
        "                 ├── technical → [strong model]  \n",
        "                 └── sales → [persuasive model]\n",
        "\n",
        "→ See 1B/cost_optimization_demo.ipynb for working demos\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "44340b6d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GPTCache: Semantic Cache for LLM Applications\n",
            "=============================================\n",
            "\n",
            "GPTCache stores query-response pairs and retrieves them\n",
            "based on semantic similarity using embeddings.\n",
            "\n",
            "Benefits:\n",
            "- 2-10× speedup when cache hits\n",
            "- Direct cost savings (no API call on hit)\n",
            "- Stable latency (no network dependency)\n",
            "- Rate limit buffer (serve from cache during throttling)\n",
            "\n",
            "Components:\n",
            "1. Embedding function: Convert query to vector\n",
            "2. Vector store: Store and search embeddings\n",
            "3. Similarity evaluator: Decide if cached response is usable\n",
            "4. Cache manager: Eviction policies, TTL\n",
            "\n",
            "Semantic Caching with GPTCache\n",
            "=======================================================\n",
            "\n",
            "SETUP:\n",
            "    pip install gptcache\n",
            "\n",
            "    from gptcache import cache\n",
            "    from gptcache.adapter import openai\n",
            "\n",
            "    # Quick start (in-memory, default settings)\n",
            "    cache.init()\n",
            "\n",
            "    # Production setup (persistent, tuned threshold)\n",
            "    setup_semantic_cache(\n",
            "        similarity_threshold=0.8,\n",
            "        cache_dir=\"./cache\"\n",
            "    )\n",
            "\n",
            "USAGE:\n",
            "    # These will share a cache entry:\n",
            "    response1 = cached_completion([\n",
            "        {\"role\": \"user\", \"content\": \"How do I reset my password?\"}\n",
            "    ])\n",
            "\n",
            "    response2 = cached_completion([\n",
            "        {\"role\": \"user\", \"content\": \"I forgot my password, help!\"}\n",
            "    ])  # Returns cached response from query 1\n",
            "\n",
            "TUNING SIMILARITY THRESHOLD:\n",
            "    threshold=0.9 → Very strict, few false positives, lower hit rate\n",
            "    threshold=0.8 → Balanced (recommended starting point)\n",
            "    threshold=0.7 → More aggressive, higher hit rate, some wrong matches\n",
            "\n",
            "EXPECTED HIT RATES BY USE CASE:\n",
            "    FAQ/Support:     30-60% (highly repetitive)\n",
            "    Search:          15-30% (moderate repetition)\n",
            "    Chat:            5-15%  (varied conversations)\n",
            "    Code generation: 10-20% (common patterns)\n",
            "\n",
            "COST SAVINGS FORMULA:\n",
            "    savings = hit_rate × requests × cost_per_request\n",
            "\n",
            "    Example: 30% hit rate, 100K requests/day, €0.002/request\n",
            "    savings = 0.30 × 100,000 × 0.002 = €60/day = €1,800/month\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\"\"\n",
        "GPTCache: Semantic Cache for LLM Applications\n",
        "=============================================\n",
        "\n",
        "GPTCache stores query-response pairs and retrieves them\n",
        "based on semantic similarity using embeddings.\n",
        "\n",
        "Benefits:\n",
        "- 2-10× speedup when cache hits\n",
        "- Direct cost savings (no API call on hit)\n",
        "- Stable latency (no network dependency)\n",
        "- Rate limit buffer (serve from cache during throttling)\n",
        "\n",
        "Components:\n",
        "1. Embedding function: Convert query to vector\n",
        "2. Vector store: Store and search embeddings\n",
        "3. Similarity evaluator: Decide if cached response is usable\n",
        "4. Cache manager: Eviction policies, TTL\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "print(\"Semantic Caching with GPTCache\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "SETUP:\n",
        "    pip install gptcache\n",
        "    \n",
        "    from gptcache import cache\n",
        "    from gptcache.adapter import openai\n",
        "    \n",
        "    # Quick start (in-memory, default settings)\n",
        "    cache.init()\n",
        "    \n",
        "    # Production setup (persistent, tuned threshold)\n",
        "    setup_semantic_cache(\n",
        "        similarity_threshold=0.8,\n",
        "        cache_dir=\"./cache\"\n",
        "    )\n",
        "\n",
        "USAGE:\n",
        "    # These will share a cache entry:\n",
        "    response1 = cached_completion([\n",
        "        {\"role\": \"user\", \"content\": \"How do I reset my password?\"}\n",
        "    ])\n",
        "    \n",
        "    response2 = cached_completion([\n",
        "        {\"role\": \"user\", \"content\": \"I forgot my password, help!\"}\n",
        "    ])  # Returns cached response from query 1\n",
        "\n",
        "TUNING SIMILARITY THRESHOLD:\n",
        "    threshold=0.9 → Very strict, few false positives, lower hit rate\n",
        "    threshold=0.8 → Balanced (recommended starting point)\n",
        "    threshold=0.7 → More aggressive, higher hit rate, some wrong matches\n",
        "\n",
        "EXPECTED HIT RATES BY USE CASE:\n",
        "    FAQ/Support:     30-60% (highly repetitive)\n",
        "    Search:          15-30% (moderate repetition)\n",
        "    Chat:            5-15%  (varied conversations)\n",
        "    Code generation: 10-20% (common patterns)\n",
        "\n",
        "COST SAVINGS FORMULA:\n",
        "    savings = hit_rate × requests × cost_per_request\n",
        "    \n",
        "    Example: 30% hit rate, 100K requests/day, €0.002/request\n",
        "    savings = 0.30 × 100,000 × 0.002 = €60/day = €1,800/month\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3e944000",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Cache Efficiency Analysis\n",
            "=======================================================\n",
            "Hit rate:                   35.0%\n",
            "Latency improvement:        34.3%\n",
            "Cost savings:         €    70.00\n",
            "Efficiency (€/MB):          0.14\n",
            "\n",
            "Optimization recommendations:\n",
            "  • Moderate hit rate: Monitor for patterns\n",
            "  • Consider SISO for better coverage\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "SISO: Next-Generation Semantic Caching\n",
        "======================================\n",
        "\n",
        "SISO (Semantic Index for Serving Optimization) improves on GPTCache:\n",
        "\n",
        "1. Centroid-based caching: Store cluster centroids, not individual queries\n",
        "   - Higher coverage with less memory\n",
        "   - Better generalization to unseen queries\n",
        "\n",
        "2. Locality-aware replacement: Consider query patterns, not just recency\n",
        "   - Keep high-value entries (frequently accessed clusters)\n",
        "   - Evict outliers that won't be hit again\n",
        "\n",
        "3. Dynamic thresholding: Adjust similarity threshold based on load\n",
        "   - Stricter during low traffic (quality focus)\n",
        "   - Looser during high traffic (availability focus)\n",
        "\n",
        "Results: 1.71× higher hit ratio vs GPTCache on diverse datasets.\n",
        "\n",
        "When to upgrade from GPTCache to SISO:\n",
        "- Hit rates plateau below expectations\n",
        "- Memory constrained environments\n",
        "- Variable traffic patterns\n",
        "\"\"\"\n",
        "\n",
        "def calculate_cache_efficiency(\n",
        "    total_requests: int,\n",
        "    cache_hits: int,\n",
        "    cache_memory_mb: int,\n",
        "    avg_latency_hit_ms: float,\n",
        "    avg_latency_miss_ms: float,\n",
        "    cost_per_miss: float\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Calculate comprehensive cache efficiency metrics.\n",
        "    \n",
        "    Use these metrics to tune cache configuration and\n",
        "    justify cache infrastructure investment.\n",
        "    \"\"\"\n",
        "    hit_rate = cache_hits / total_requests if total_requests > 0 else 0\n",
        "    \n",
        "    # Latency improvement\n",
        "    avg_latency_with_cache = (\n",
        "        hit_rate * avg_latency_hit_ms + \n",
        "        (1 - hit_rate) * avg_latency_miss_ms\n",
        "    )\n",
        "    latency_improvement = 1 - (avg_latency_with_cache / avg_latency_miss_ms)\n",
        "    \n",
        "    # Cost savings\n",
        "    cost_without_cache = total_requests * cost_per_miss\n",
        "    cost_with_cache = (total_requests - cache_hits) * cost_per_miss\n",
        "    cost_savings = cost_without_cache - cost_with_cache\n",
        "    \n",
        "    # Efficiency: savings per MB of cache\n",
        "    efficiency = cost_savings / cache_memory_mb if cache_memory_mb > 0 else 0\n",
        "    \n",
        "    return {\n",
        "        'hit_rate': round(hit_rate * 100, 1),\n",
        "        'latency_improvement': round(latency_improvement * 100, 1),\n",
        "        'cost_savings': round(cost_savings, 2),\n",
        "        'efficiency_per_mb': round(efficiency, 2)\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Cache efficiency analysis\n",
        "# =============================================================================\n",
        "\n",
        "# Scenario: Production semantic cache performance\n",
        "metrics = calculate_cache_efficiency(\n",
        "    total_requests=100000,\n",
        "    cache_hits=35000,  # 35% hit rate\n",
        "    cache_memory_mb=512,\n",
        "    avg_latency_hit_ms=15,\n",
        "    avg_latency_miss_ms=800,\n",
        "    cost_per_miss=0.002\n",
        ")\n",
        "\n",
        "print(\"Semantic Cache Efficiency Analysis\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"Hit rate:             {metrics['hit_rate']:>10}%\")\n",
        "print(f\"Latency improvement:  {metrics['latency_improvement']:>10}%\")\n",
        "print(f\"Cost savings:         €{metrics['cost_savings']:>9,.2f}\")\n",
        "print(f\"Efficiency (€/MB):    {metrics['efficiency_per_mb']:>10.2f}\")\n",
        "print()\n",
        "print(\"Optimization recommendations:\")\n",
        "if metrics['hit_rate'] < 20:\n",
        "    print(\"  • Low hit rate: Consider lower similarity threshold\")\n",
        "    print(\"  • Check if queries are too varied for caching\")\n",
        "elif metrics['hit_rate'] > 50:\n",
        "    print(\"  • High hit rate: Good! Consider raising threshold for precision\")\n",
        "    print(\"  • Evaluate if stale responses are a problem\")\n",
        "else:\n",
        "    print(\"  • Moderate hit rate: Monitor for patterns\")\n",
        "    print(\"  • Consider SISO for better coverage\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfc9865b",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "e29f35ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LLM Observability Stack Selection\n",
            "==================================\n",
            "\n",
            "DECISION TREE:\n",
            "\n",
            "1. Are you using LangChain?\n",
            "   YES → Start with LangSmith (zero-config integration)\n",
            "   NO → Continue to #2\n",
            "\n",
            "2. Do you need self-hosting (GDPR, data sovereignty)?\n",
            "   YES → Langfuse (MIT license, well-documented self-host)\n",
            "   NO → Continue to #3\n",
            "\n",
            "3. Do you have existing observability infrastructure?\n",
            "   Datadog → Use Datadog LLM Monitoring (unified stack)\n",
            "   New Relic → Use New Relic AI Monitoring\n",
            "   Neither → Continue to #4\n",
            "\n",
            "4. What's your primary use case?\n",
            "   RAG/Retrieval → Phoenix by Arize (RAG-specific features)\n",
            "   Agents → Langfuse or LangSmith (trace visualization)\n",
            "   Cost tracking → Helicone (fastest setup)\n",
            "   Evaluation focus → Braintrust (eval + observability)\n",
            "\n",
            "TOOL COMPARISON:\n",
            "\n",
            "┌──────────────┬─────────────┬──────────────┬───────────────┐\n",
            "│ Tool         │ Deployment  │ Best For     │ Pricing       │\n",
            "├──────────────┼─────────────┼──────────────┼───────────────┤\n",
            "│ Langfuse     │ Cloud/Self  │ General, OSS │ Free tier     │\n",
            "│ LangSmith    │ Cloud       │ LangChain    │ Free tier     │\n",
            "│ Phoenix      │ Self-host   │ RAG, evals   │ Free (OSS)    │\n",
            "│ Helicone     │ Cloud       │ Cost tracking│ Free tier     │\n",
            "│ Opik         │ Cloud/Self  │ Speed        │ Free tier     │\n",
            "│ Datadog      │ Cloud       │ Enterprise   │ Enterprise $$ │\n",
            "└──────────────┴─────────────┴──────────────┴───────────────┘\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Decision framework for observability tooling\n",
        "\n",
        "OBSERVABILITY_DECISION = \"\"\"\n",
        "LLM Observability Stack Selection\n",
        "==================================\n",
        "\n",
        "DECISION TREE:\n",
        "\n",
        "1. Are you using LangChain?\n",
        "   YES → Start with LangSmith (zero-config integration)\n",
        "   NO → Continue to #2\n",
        "\n",
        "2. Do you need self-hosting (GDPR, data sovereignty)?\n",
        "   YES → Langfuse (MIT license, well-documented self-host)\n",
        "   NO → Continue to #3\n",
        "\n",
        "3. Do you have existing observability infrastructure?\n",
        "   Datadog → Use Datadog LLM Monitoring (unified stack)\n",
        "   New Relic → Use New Relic AI Monitoring\n",
        "   Neither → Continue to #4\n",
        "\n",
        "4. What's your primary use case?\n",
        "   RAG/Retrieval → Phoenix by Arize (RAG-specific features)\n",
        "   Agents → Langfuse or LangSmith (trace visualization)\n",
        "   Cost tracking → Helicone (fastest setup)\n",
        "   Evaluation focus → Braintrust (eval + observability)\n",
        "\n",
        "TOOL COMPARISON:\n",
        "\n",
        "┌──────────────┬─────────────┬──────────────┬───────────────┐\n",
        "│ Tool         │ Deployment  │ Best For     │ Pricing       │\n",
        "├──────────────┼─────────────┼──────────────┼───────────────┤\n",
        "│ Langfuse     │ Cloud/Self  │ General, OSS │ Free tier     │\n",
        "│ LangSmith    │ Cloud       │ LangChain    │ Free tier     │\n",
        "│ Phoenix      │ Self-host   │ RAG, evals   │ Free (OSS)    │\n",
        "│ Helicone     │ Cloud       │ Cost tracking│ Free tier     │\n",
        "│ Opik         │ Cloud/Self  │ Speed        │ Free tier     │\n",
        "│ Datadog      │ Cloud       │ Enterprise   │ Enterprise $$ │\n",
        "└──────────────┴─────────────┴──────────────┴───────────────┘\n",
        "\"\"\"\n",
        "\n",
        "print(OBSERVABILITY_DECISION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "b7703cba",
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langfuse.decorators'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mLangfuse: Open Source LLM Observability\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m=======================================\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[33;03m3. Manual (full control)\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# pip install langfuse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangfuse\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m observe, langfuse_context\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangfuse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Langfuse\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Initialize (reads LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY from env)\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langfuse.decorators'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Langfuse: Open Source LLM Observability\n",
        "=======================================\n",
        "\n",
        "Langfuse is the most popular open-source option (19K+ GitHub stars).\n",
        "Key features:\n",
        "- Tracing with multi-turn conversation support\n",
        "- Prompt versioning and playground\n",
        "- Evaluation (LLM-as-judge, user feedback, custom metrics)\n",
        "- Cost tracking\n",
        "- Self-hosting with extensive documentation\n",
        "\n",
        "Integration approaches:\n",
        "1. Decorator-based (cleanest)\n",
        "2. Context manager (flexible)\n",
        "3. Manual (full control)\n",
        "\"\"\"\n",
        "\n",
        "# pip install langfuse\n",
        "from langfuse.decorators import observe, langfuse_context\n",
        "from langfuse import Langfuse\n",
        "\n",
        "# Initialize (reads LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY from env)\n",
        "langfuse = Langfuse()\n",
        "\n",
        "@observe()  # Automatically traces this function\n",
        "def process_support_ticket(ticket_text: str, customer_id: str) -> dict:\n",
        "    \"\"\"\n",
        "    Process a support ticket with full observability.\n",
        "    \n",
        "    The @observe() decorator:\n",
        "    - Creates a trace for the entire function\n",
        "    - Captures inputs/outputs\n",
        "    - Records latency\n",
        "    - Nests child spans for LLM calls\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieval step (automatically nested in trace)\n",
        "    context = retrieve_relevant_docs(ticket_text)\n",
        "    \n",
        "    # LLM call (nested span with token tracking)\n",
        "    response = generate_response(ticket_text, context)\n",
        "    \n",
        "    # Add custom metadata\n",
        "    langfuse_context.update_current_observation(\n",
        "        metadata={\n",
        "            \"customer_id\": customer_id,\n",
        "            \"context_chunks\": len(context)\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    return response\n",
        "\n",
        "@observe(as_type=\"generation\")  # Marks this as an LLM generation\n",
        "def generate_response(query: str, context: str) -> str:\n",
        "    \"\"\"Generate LLM response with token tracking.\"\"\"\n",
        "    \n",
        "    # Your LLM call here\n",
        "    response = llm.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": f\"Context: {context}\"},\n",
        "            {\"role\": \"user\", \"content\": query}\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    # Langfuse automatically captures:\n",
        "    # - Model name\n",
        "    # - Input/output tokens\n",
        "    # - Latency\n",
        "    # - Cost (if configured)\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "@observe(as_type=\"retrieval\")\n",
        "def retrieve_relevant_docs(query: str) -> str:\n",
        "    \"\"\"Retrieve documents with retrieval-specific tracking.\"\"\"\n",
        "    # Your retrieval logic\n",
        "    pass\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Langfuse setup guide\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Langfuse Setup Guide\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "1. CLOUD SETUP (quickest):\n",
        "   - Sign up at https://cloud.langfuse.com\n",
        "   - Create project, get API keys\n",
        "   - Set environment variables:\n",
        "     \n",
        "     export LANGFUSE_PUBLIC_KEY=\"pk-...\"\n",
        "     export LANGFUSE_SECRET_KEY=\"sk-...\"\n",
        "     export LANGFUSE_HOST=\"https://cloud.langfuse.com\"\n",
        "\n",
        "2. SELF-HOSTED SETUP (data sovereignty):\n",
        "   \n",
        "   # docker-compose.yml\n",
        "   services:\n",
        "     langfuse:\n",
        "       image: langfuse/langfuse:latest\n",
        "       ports:\n",
        "         - \"3000:3000\"\n",
        "       environment:\n",
        "         - DATABASE_URL=postgresql://...\n",
        "         - NEXTAUTH_SECRET=...\n",
        "\n",
        "3. INTEGRATION:\n",
        "   \n",
        "   pip install langfuse\n",
        "   \n",
        "   # Option A: Decorators (cleanest)\n",
        "   from langfuse.decorators import observe\n",
        "   \n",
        "   @observe()\n",
        "   def my_llm_function():\n",
        "       ...\n",
        "   \n",
        "   # Option B: OpenAI wrapper (automatic)\n",
        "   from langfuse.openai import OpenAI\n",
        "   client = OpenAI()  # Drop-in replacement, auto-traces\n",
        "   \n",
        "   # Option C: LangChain integration\n",
        "   from langfuse.callback import CallbackHandler\n",
        "   handler = CallbackHandler()\n",
        "   chain.invoke(..., config={\"callbacks\": [handler]})\n",
        "\n",
        "4. EVALUATION:\n",
        "   \n",
        "   # Score traces (programmatic)\n",
        "   langfuse.score(\n",
        "       trace_id=\"...\",\n",
        "       name=\"quality\",\n",
        "       value=0.9\n",
        "   )\n",
        "   \n",
        "   # LLM-as-judge (automatic)\n",
        "   # Configure in Langfuse dashboard → Evaluation tab\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f1236e8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG Evaluation with DeepEval\n",
            "=======================================================\n",
            "\n",
            "SETUP:\n",
            "    pip install deepeval\n",
            "\n",
            "    # Set evaluator model\n",
            "    export OPENAI_API_KEY=\"sk-...\"\n",
            "\n",
            "CREATING TEST CASES:\n",
            "\n",
            "    test_case = LLMTestCase(\n",
            "        input=\"What is the return policy?\",\n",
            "        actual_output=\"You can return items within 30 days...\",\n",
            "        retrieval_context=[\n",
            "            \"Our return policy allows returns within 30 days...\",\n",
            "            \"Refunds are processed within 5-7 business days...\"\n",
            "        ],\n",
            "        expected_output=\"Items can be returned within 30 days for a full refund.\"\n",
            "    )\n",
            "\n",
            "BUILT-IN METRICS:\n",
            "\n",
            "    Retrieval metrics:\n",
            "    - ContextualPrecisionMetric: Are retrieved docs relevant?\n",
            "    - ContextualRecallMetric: Did we get all relevant docs?\n",
            "\n",
            "    Generation metrics:\n",
            "    - FaithfulnessMetric: Is response grounded in context?\n",
            "    - AnswerRelevancyMetric: Does it answer the question?\n",
            "\n",
            "    End-to-end metrics:\n",
            "    - HallucinationMetric: Did the model make things up?\n",
            "    - ToxicityMetric: Is the response safe?\n",
            "\n",
            "RUNNING EVALUATIONS:\n",
            "\n",
            "    # Single test\n",
            "    metric = FaithfulnessMetric(threshold=0.7)\n",
            "    metric.measure(test_case)\n",
            "    print(f\"Score: {metric.score}, Reason: {metric.reason}\")\n",
            "\n",
            "    # Batch evaluation (with pytest integration)\n",
            "    # test_rag.py\n",
            "    from deepeval import assert_test\n",
            "\n",
            "    def test_faithfulness():\n",
            "        assert_test(test_case, [FaithfulnessMetric(threshold=0.7)])\n",
            "\n",
            "    # Run: deepeval test run test_rag.py\n",
            "\n",
            "CUSTOM METRICS (G-Eval):\n",
            "\n",
            "    professional_tone = GEval(\n",
            "        name=\"Professional Tone\",\n",
            "        criteria=\"Response should be professional and respectful\",\n",
            "        evaluation_steps=[\n",
            "            \"Check if the response uses professional language\",\n",
            "            \"Verify there's no slang or casual expressions\",\n",
            "            \"Ensure the tone is helpful and courteous\"\n",
            "        ]\n",
            "    )\n",
            "\n",
            "CI/CD INTEGRATION:\n",
            "\n",
            "    # Run in pipeline\n",
            "    deepeval test run tests/ --parallel --exit-on-first-failure\n",
            "\n",
            "    # Generate report\n",
            "    deepeval test run tests/ --report\n",
            "\n",
            "LLM-AS-JUDGE BEST PRACTICES:\n",
            "    • Use GPT-3.5 + examples instead of GPT-4 (10× cheaper, similar accuracy)\n",
            "    • Binary/low-precision scales (0-3) work as well as 0-100\n",
            "    • Sample 5-10% of production traffic for ongoing evaluation\n",
            "    • Calibrate against human judgments periodically\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "LLM Evaluation Framework\n",
        "========================\n",
        "\n",
        "Three layers of evaluation:\n",
        "\n",
        "1. COMPONENT METRICS (retrieval, generation)\n",
        "   - Retrieval: Precision, Recall, MRR, NDCG\n",
        "   - Generation: Faithfulness, Relevancy, Coherence\n",
        "\n",
        "2. END-TO-END METRICS (system level)\n",
        "   - Task completion rate\n",
        "   - User satisfaction (CSAT, thumbs up/down)\n",
        "   - Error rate\n",
        "\n",
        "3. SAFETY METRICS (guardrails)\n",
        "   - Hallucination rate\n",
        "   - Toxicity rate\n",
        "   - PII leakage rate\n",
        "\"\"\"\n",
        "\n",
        "# pip install deepeval\n",
        "from deepeval import evaluate\n",
        "from deepeval.metrics import (\n",
        "    FaithfulnessMetric,\n",
        "    AnswerRelevancyMetric,\n",
        "    ContextualPrecisionMetric,\n",
        "    GEval\n",
        ")\n",
        "from deepeval.test_case import LLMTestCase\n",
        "\n",
        "def create_rag_test_case(\n",
        "    query: str,\n",
        "    response: str,\n",
        "    retrieved_context: list,\n",
        "    expected_output: str = None\n",
        ") -> LLMTestCase:\n",
        "    \"\"\"\n",
        "    Create a test case for RAG evaluation.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    query : str\n",
        "        User's question\n",
        "    response : str\n",
        "        Generated response from RAG system\n",
        "    retrieved_context : list\n",
        "        List of retrieved document chunks\n",
        "    expected_output : str, optional\n",
        "        Ground truth answer (if available)\n",
        "    \"\"\"\n",
        "    return LLMTestCase(\n",
        "        input=query,\n",
        "        actual_output=response,\n",
        "        retrieval_context=retrieved_context,\n",
        "        expected_output=expected_output\n",
        "    )\n",
        "\n",
        "def evaluate_rag_quality(test_cases: list) -> dict:\n",
        "    \"\"\"\n",
        "    Evaluate RAG system quality across multiple metrics.\n",
        "    \n",
        "    Metrics explained:\n",
        "    - Faithfulness: Is the response grounded in retrieved context?\n",
        "    - Answer Relevancy: Does the response answer the question?\n",
        "    - Contextual Precision: Are retrieved docs relevant and well-ranked?\n",
        "    \"\"\"\n",
        "    metrics = [\n",
        "        FaithfulnessMetric(\n",
        "            threshold=0.7,\n",
        "            model=\"gpt-4o-mini\"  # Judge model\n",
        "        ),\n",
        "        AnswerRelevancyMetric(\n",
        "            threshold=0.7,\n",
        "            model=\"gpt-4o-mini\"\n",
        "        ),\n",
        "        ContextualPrecisionMetric(\n",
        "            threshold=0.7,\n",
        "            model=\"gpt-4o-mini\"\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    results = evaluate(test_cases, metrics)\n",
        "    \n",
        "    return {\n",
        "        'passed': results.passed,\n",
        "        'failed': results.failed,\n",
        "        'metrics': {\n",
        "            metric.name: {\n",
        "                'avg_score': metric.score,\n",
        "                'threshold': metric.threshold,\n",
        "                'passed': metric.score >= metric.threshold\n",
        "            }\n",
        "            for metric in metrics\n",
        "        }\n",
        "    }\n",
        "\n",
        "def create_custom_eval(\n",
        "    name: str,\n",
        "    criteria: str,\n",
        "    evaluation_steps: list\n",
        ") -> GEval:\n",
        "    \"\"\"\n",
        "    Create a custom evaluation metric using G-Eval.\n",
        "    \n",
        "    G-Eval uses an LLM to evaluate based on your criteria,\n",
        "    achieving ~80% agreement with human judgment.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    name : str\n",
        "        Name for the metric\n",
        "    criteria : str\n",
        "        What you're measuring (e.g., \"professional tone\")\n",
        "    evaluation_steps : list\n",
        "        Step-by-step instructions for the evaluator LLM\n",
        "    \"\"\"\n",
        "    return GEval(\n",
        "        name=name,\n",
        "        criteria=criteria,\n",
        "        evaluation_steps=evaluation_steps,\n",
        "        model=\"gpt-4o-mini\",\n",
        "        threshold=0.7\n",
        "    )\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Evaluation setup for production RAG\n",
        "# =============================================================================\n",
        "\n",
        "print(\"RAG Evaluation with DeepEval\")\n",
        "print(\"=\" * 55)\n",
        "print(\"\"\"\n",
        "SETUP:\n",
        "    pip install deepeval\n",
        "    \n",
        "    # Set evaluator model\n",
        "    export OPENAI_API_KEY=\"sk-...\"\n",
        "\n",
        "CREATING TEST CASES:\n",
        "\n",
        "    test_case = LLMTestCase(\n",
        "        input=\"What is the return policy?\",\n",
        "        actual_output=\"You can return items within 30 days...\",\n",
        "        retrieval_context=[\n",
        "            \"Our return policy allows returns within 30 days...\",\n",
        "            \"Refunds are processed within 5-7 business days...\"\n",
        "        ],\n",
        "        expected_output=\"Items can be returned within 30 days for a full refund.\"\n",
        "    )\n",
        "\n",
        "BUILT-IN METRICS:\n",
        "\n",
        "    Retrieval metrics:\n",
        "    - ContextualPrecisionMetric: Are retrieved docs relevant?\n",
        "    - ContextualRecallMetric: Did we get all relevant docs?\n",
        "    \n",
        "    Generation metrics:\n",
        "    - FaithfulnessMetric: Is response grounded in context?\n",
        "    - AnswerRelevancyMetric: Does it answer the question?\n",
        "    \n",
        "    End-to-end metrics:\n",
        "    - HallucinationMetric: Did the model make things up?\n",
        "    - ToxicityMetric: Is the response safe?\n",
        "\n",
        "RUNNING EVALUATIONS:\n",
        "\n",
        "    # Single test\n",
        "    metric = FaithfulnessMetric(threshold=0.7)\n",
        "    metric.measure(test_case)\n",
        "    print(f\"Score: {metric.score}, Reason: {metric.reason}\")\n",
        "    \n",
        "    # Batch evaluation (with pytest integration)\n",
        "    # test_rag.py\n",
        "    from deepeval import assert_test\n",
        "    \n",
        "    def test_faithfulness():\n",
        "        assert_test(test_case, [FaithfulnessMetric(threshold=0.7)])\n",
        "    \n",
        "    # Run: deepeval test run test_rag.py\n",
        "\n",
        "CUSTOM METRICS (G-Eval):\n",
        "\n",
        "    professional_tone = GEval(\n",
        "        name=\"Professional Tone\",\n",
        "        criteria=\"Response should be professional and respectful\",\n",
        "        evaluation_steps=[\n",
        "            \"Check if the response uses professional language\",\n",
        "            \"Verify there's no slang or casual expressions\",\n",
        "            \"Ensure the tone is helpful and courteous\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "CI/CD INTEGRATION:\n",
        "\n",
        "    # Run in pipeline\n",
        "    deepeval test run tests/ --parallel --exit-on-first-failure\n",
        "    \n",
        "    # Generate report\n",
        "    deepeval test run tests/ --report\n",
        "\n",
        "LLM-AS-JUDGE BEST PRACTICES:\n",
        "    • Use GPT-3.5 + examples instead of GPT-4 (10× cheaper, similar accuracy)\n",
        "    • Binary/low-precision scales (0-3) work as well as 0-100\n",
        "    • Sample 5-10% of production traffic for ongoing evaluation\n",
        "    • Calibrate against human judgments periodically\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c4821e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM Cost Estimation: Document Q&A Feature\n",
            "=======================================================\n",
            "Daily requests:                 50,000\n",
            "Base daily cost:       €        825.00\n",
            "Optimized daily cost:  €        219.19\n",
            "Daily savings:         €        605.81\n",
            "\n",
            "Monthly (base):        €     24,750.00\n",
            "Monthly (optimized):   €      6,575.62\n",
            "Monthly savings:       €     18,174.38\n",
            "Savings percentage:              73.4%\n"
          ]
        }
      ],
      "source": [
        "def estimate_llm_costs(\n",
        "    daily_requests: int,\n",
        "    avg_input_tokens: int,\n",
        "    avg_output_tokens: int,\n",
        "    model_tier: str,  # \"small\", \"medium\", \"large\", \"frontier\"\n",
        "    use_caching: bool = True,\n",
        "    cache_hit_rate: float = 0.25,\n",
        "    use_routing: bool = True,\n",
        "    routing_to_small_rate: float = 0.70\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Comprehensive LLM cost estimation.\n",
        "    \n",
        "    Use this worksheet when planning new LLM features.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Model pricing (per 1K tokens, approximate Dec 2025)\n",
        "    pricing = {\n",
        "        \"small\": {\"input\": 0.00015, \"output\": 0.0006},    # GPT-4o-mini, Haiku\n",
        "        \"medium\": {\"input\": 0.003, \"output\": 0.015},      # Claude Sonnet, GPT-4o\n",
        "        \"large\": {\"input\": 0.015, \"output\": 0.075},       # Claude Opus\n",
        "        \"frontier\": {\"input\": 0.015, \"output\": 0.075}     # Latest frontier\n",
        "    }\n",
        "    \n",
        "    # Base calculation\n",
        "    base_input_cost = (daily_requests * avg_input_tokens / 1000) * pricing[model_tier][\"input\"]\n",
        "    base_output_cost = (daily_requests * avg_output_tokens / 1000) * pricing[model_tier][\"output\"]\n",
        "    base_daily_cost = base_input_cost + base_output_cost\n",
        "    \n",
        "    # Apply caching (reduces requests that hit LLM)\n",
        "    if use_caching:\n",
        "        effective_requests = daily_requests * (1 - cache_hit_rate)\n",
        "    else:\n",
        "        effective_requests = daily_requests\n",
        "    \n",
        "    # Apply routing (routes portion to cheaper model)\n",
        "    if use_routing and model_tier in [\"medium\", \"large\", \"frontier\"]:\n",
        "        # Routed traffic goes to small tier\n",
        "        small_requests = effective_requests * routing_to_small_rate\n",
        "        full_requests = effective_requests * (1 - routing_to_small_rate)\n",
        "        \n",
        "        small_cost = (\n",
        "            (small_requests * avg_input_tokens / 1000) * pricing[\"small\"][\"input\"] +\n",
        "            (small_requests * avg_output_tokens / 1000) * pricing[\"small\"][\"output\"]\n",
        "        )\n",
        "        full_cost = (\n",
        "            (full_requests * avg_input_tokens / 1000) * pricing[model_tier][\"input\"] +\n",
        "            (full_requests * avg_output_tokens / 1000) * pricing[model_tier][\"output\"]\n",
        "        )\n",
        "        optimized_daily_cost = small_cost + full_cost\n",
        "    else:\n",
        "        optimized_daily_cost = (\n",
        "            (effective_requests * avg_input_tokens / 1000) * pricing[model_tier][\"input\"] +\n",
        "            (effective_requests * avg_output_tokens / 1000) * pricing[model_tier][\"output\"]\n",
        "        )\n",
        "    \n",
        "    return {\n",
        "        'daily_requests': daily_requests,\n",
        "        'base_daily_cost': round(base_daily_cost, 2),\n",
        "        'optimized_daily_cost': round(optimized_daily_cost, 2),\n",
        "        'daily_savings': round(base_daily_cost - optimized_daily_cost, 2),\n",
        "        'monthly_base': round(base_daily_cost * 30, 2),\n",
        "        'monthly_optimized': round(optimized_daily_cost * 30, 2),\n",
        "        'monthly_savings': round((base_daily_cost - optimized_daily_cost) * 30, 2),\n",
        "        'savings_percent': round((1 - optimized_daily_cost / base_daily_cost) * 100, 1)\n",
        "    }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Driver: Cost planning for a new feature\n",
        "# =============================================================================\n",
        "\n",
        "# Scenario: Planning a document Q&A feature\n",
        "qa_feature = estimate_llm_costs(\n",
        "    daily_requests=50000,\n",
        "    avg_input_tokens=3000,  # Context + query\n",
        "    avg_output_tokens=500,  # Response\n",
        "    model_tier=\"medium\",    # Claude Sonnet\n",
        "    use_caching=True,\n",
        "    cache_hit_rate=0.30,    # FAQ-heavy domain\n",
        "    use_routing=True,\n",
        "    routing_to_small_rate=0.65  # Most queries are simple\n",
        ")\n",
        "\n",
        "print(\"LLM Cost Estimation: Document Q&A Feature\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"Daily requests:        {qa_feature['daily_requests']:>15,}\")\n",
        "print(f\"Base daily cost:       €{qa_feature['base_daily_cost']:>14,.2f}\")\n",
        "print(f\"Optimized daily cost:  €{qa_feature['optimized_daily_cost']:>14,.2f}\")\n",
        "print(f\"Daily savings:         €{qa_feature['daily_savings']:>14,.2f}\")\n",
        "print()\n",
        "print(f\"Monthly (base):        €{qa_feature['monthly_base']:>14,.2f}\")\n",
        "print(f\"Monthly (optimized):   €{qa_feature['monthly_optimized']:>14,.2f}\")\n",
        "print(f\"Monthly savings:       €{qa_feature['monthly_savings']:>14,.2f}\")\n",
        "print(f\"Savings percentage:    {qa_feature['savings_percent']:>14}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31b1fece",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LLM System Failure Mode Checklist\n",
            "==================================\n",
            "\n",
            "PRE-DEPLOYMENT:\n",
            "☐ Model validated on YOUR data (not just public benchmarks)\n",
            "☐ Structured output tested with edge cases\n",
            "☐ Guardrails configured and tested (jailbreak, PII, toxicity)\n",
            "☐ Hallucination baseline measured\n",
            "☐ Cost projections validated with realistic traffic estimates\n",
            "☐ Latency tested under load\n",
            "\n",
            "MONITORING (Day 1):\n",
            "☐ Observability deployed (traces, tokens, costs)\n",
            "☐ Alerts configured (error rate, latency P95, cost spikes)\n",
            "☐ Evaluation pipeline running (5% sample with LLM-as-judge)\n",
            "☐ User feedback collection enabled\n",
            "\n",
            "ONGOING:\n",
            "☐ Weekly: Review quality scores, cost trends\n",
            "☐ Monthly: Re-evaluate model selection (new models may be better/cheaper)\n",
            "☐ Quarterly: Refresh evaluation dataset with production examples\n",
            "☐ Ad-hoc: Investigate quality degradation signals\n",
            "\n",
            "COMMON FAILURE MODES TO WATCH:\n",
            "\n",
            "1. PROMPT DRIFT\n",
            "   Symptom: Quality degrades over time without code changes\n",
            "   Cause: Model updates by provider, data distribution shift\n",
            "   Fix: Pin model versions, monitor quality metrics\n",
            "\n",
            "2. CONTEXT OVERFLOW\n",
            "   Symptom: Responses ignore important context\n",
            "   Cause: Exceeded context window, \"lost in the middle\"\n",
            "   Fix: Better chunking, reranking, hierarchical summarization\n",
            "\n",
            "3. COST EXPLOSION\n",
            "   Symptom: Bills much higher than projected\n",
            "   Cause: Verbose prompts, chatty responses, missing caching\n",
            "   Fix: Audit token usage, implement output length limits\n",
            "\n",
            "4. HALLUCINATION SPIKE\n",
            "   Symptom: Users report factually wrong answers\n",
            "   Cause: Poor retrieval quality, model uncertainty\n",
            "   Fix: Improve retrieval, add confidence thresholds\n",
            "\n",
            "5. LATENCY REGRESSION\n",
            "   Symptom: Response times increase\n",
            "   Cause: Larger context, provider issues, cold starts\n",
            "   Fix: Monitor TTFT separately, implement timeouts\n",
            "\n",
            "6. GUARDRAIL BYPASS\n",
            "   Symptom: Harmful/off-topic responses get through\n",
            "   Cause: New attack patterns, incomplete rules\n",
            "   Fix: Red team regularly, update guardrails\n",
            "\n"
          ]
        }
      ],
      "source": [
        "FAILURE_CHECKLIST = \"\"\"\n",
        "LLM System Failure Mode Checklist\n",
        "==================================\n",
        "\n",
        "PRE-DEPLOYMENT:\n",
        "☐ Model validated on YOUR data (not just public benchmarks)\n",
        "☐ Structured output tested with edge cases\n",
        "☐ Guardrails configured and tested (jailbreak, PII, toxicity)\n",
        "☐ Hallucination baseline measured\n",
        "☐ Cost projections validated with realistic traffic estimates\n",
        "☐ Latency tested under load\n",
        "\n",
        "MONITORING (Day 1):\n",
        "☐ Observability deployed (traces, tokens, costs)\n",
        "☐ Alerts configured (error rate, latency P95, cost spikes)\n",
        "☐ Evaluation pipeline running (5% sample with LLM-as-judge)\n",
        "☐ User feedback collection enabled\n",
        "\n",
        "ONGOING:\n",
        "☐ Weekly: Review quality scores, cost trends\n",
        "☐ Monthly: Re-evaluate model selection (new models may be better/cheaper)\n",
        "☐ Quarterly: Refresh evaluation dataset with production examples\n",
        "☐ Ad-hoc: Investigate quality degradation signals\n",
        "\n",
        "COMMON FAILURE MODES TO WATCH:\n",
        "\n",
        "1. PROMPT DRIFT\n",
        "   Symptom: Quality degrades over time without code changes\n",
        "   Cause: Model updates by provider, data distribution shift\n",
        "   Fix: Pin model versions, monitor quality metrics\n",
        "\n",
        "2. CONTEXT OVERFLOW\n",
        "   Symptom: Responses ignore important context\n",
        "   Cause: Exceeded context window, \"lost in the middle\"\n",
        "   Fix: Better chunking, reranking, hierarchical summarization\n",
        "\n",
        "3. COST EXPLOSION\n",
        "   Symptom: Bills much higher than projected\n",
        "   Cause: Verbose prompts, chatty responses, missing caching\n",
        "   Fix: Audit token usage, implement output length limits\n",
        "\n",
        "4. HALLUCINATION SPIKE\n",
        "   Symptom: Users report factually wrong answers\n",
        "   Cause: Poor retrieval quality, model uncertainty\n",
        "   Fix: Improve retrieval, add confidence thresholds\n",
        "\n",
        "5. LATENCY REGRESSION\n",
        "   Symptom: Response times increase\n",
        "   Cause: Larger context, provider issues, cold starts\n",
        "   Fix: Monitor TTFT separately, implement timeouts\n",
        "\n",
        "6. GUARDRAIL BYPASS\n",
        "   Symptom: Harmful/off-topic responses get through\n",
        "   Cause: New attack patterns, incomplete rules\n",
        "   Fix: Red team regularly, update guardrails\n",
        "\"\"\"\n",
        "\n",
        "print(FAILURE_CHECKLIST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "491cc626",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "guardrails-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
