{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Production Operations Demo\n",
        "\n",
        "Demos aligned with Part 1B Section 4: Production Operations\n",
        "\n",
        "1. **Observability Stack Decision** - Choosing the right monitoring tools\n",
        "2. **Phoenix (Arize)** - Open source, self-hosted observability with RAG focus\n",
        "3. **Langfuse Simulation** - Open source LLM tracing patterns (local demo)\n",
        "4. **DeepEval** - LLM evaluation with pytest integration\n",
        "5. **RAGAS** - RAG-specific evaluation metrics\n",
        "6. **LLM-as-Judge** - Custom evaluation with local models\n",
        "7. **Production Failure Modes** - Checklist and detection patterns\n",
        "\n",
        "## Key Insight\n",
        "\n",
        "Traditional APM tells you if your service is *up*. LLM observability tells you if your service is *good*.\n",
        "\n",
        "## Architecture\n",
        "\n",
        "```\n",
        "Production LLM System\n",
        "        │\n",
        "        ▼\n",
        "┌───────────────────┐     ┌───────────────────┐\n",
        "│   Observability   │     │    Evaluation     │\n",
        "│  (What happened)  │     │  (Was it good?)   │\n",
        "├───────────────────┤     ├───────────────────┤\n",
        "│ • Traces          │     │ • Faithfulness    │\n",
        "│ • Token costs     │     │ • Relevancy       │\n",
        "│ • Latency         │     │ • Hallucination   │\n",
        "│ • User feedback   │     │ • LLM-as-Judge    │\n",
        "└───────────────────┘     └───────────────────┘\n",
        "         │                         │\n",
        "         └──────────┬──────────────┘\n",
        "                    ▼\n",
        "            Quality Monitoring\n",
        "```\n",
        "\n",
        "## Setup\n",
        "\n",
        "```bash\n",
        "pip install arize-phoenix deepeval ragas sentence-transformers openinference-instrumentation\n",
        "```\n",
        "\n",
        "**Ollama:** `ollama pull qwen3:4b && ollama pull llama3.2:1b`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m[INFO]\u001b[0m Ollama is running\n",
            "\u001b[92m[INFO]\u001b[0m Helper functions ready\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Available models:\n",
            "NAME           ID              SIZE      MODIFIED   \n",
            "qwen3:4b       359d7dd4bcda    2.5 GB    2 days ago    \n",
            "llama3.2:1b    baf6a787fdff    1.3 GB    2 days ago    \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Setup: Environment, logging, and Ollama client\n",
        "import subprocess\n",
        "import logging\n",
        "import requests\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime\n",
        "\n",
        "# Color-coded logging\n",
        "class ColoredFormatter(logging.Formatter):\n",
        "    COLORS = {'DEBUG': '\\033[90m', 'INFO': '\\033[92m', 'WARNING': '\\033[93m', 'ERROR': '\\033[91m', 'RESET': '\\033[0m'}\n",
        "    def format(self, record):\n",
        "        color = self.COLORS.get(record.levelname, self.COLORS['RESET'])\n",
        "        record.msg = f\"{color}[{record.levelname}]{self.COLORS['RESET']} {record.msg}\"\n",
        "        return super().format(record)\n",
        "\n",
        "logger = logging.getLogger(\"production_ops_demo\")\n",
        "logger.setLevel(logging.DEBUG)\n",
        "if not logger.handlers:\n",
        "    handler = logging.StreamHandler()\n",
        "    handler.setFormatter(ColoredFormatter('%(message)s'))\n",
        "    logger.addHandler(handler)\n",
        "\n",
        "# Check Ollama\n",
        "def check_ollama():\n",
        "    try:\n",
        "        result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True, timeout=5)\n",
        "        if result.returncode == 0:\n",
        "            logger.info(\"Ollama is running\")\n",
        "            print(f\"\\nAvailable models:\\n{result.stdout}\")\n",
        "            return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ollama check failed: {e}\")\n",
        "    return False\n",
        "\n",
        "ollama_ready = check_ollama()\n",
        "OLLAMA_URL = \"http://localhost:11434\"\n",
        "EVAL_MODEL = \"qwen3:4b\"    # For LLM-as-judge\n",
        "WEAK_MODEL = \"llama3.2:1b\" # For basic generation\n",
        "\n",
        "# Ollama helpers\n",
        "def ollama_generate(prompt: str, model: str, temperature: float = 0.7) -> Tuple[str, float]:\n",
        "    start = time.perf_counter()\n",
        "    response = requests.post(\n",
        "        f\"{OLLAMA_URL}/api/generate\",\n",
        "        json={\"model\": model, \"prompt\": prompt, \"stream\": False, \"options\": {\"temperature\": temperature}},\n",
        "        timeout=120\n",
        "    )\n",
        "    latency_ms = (time.perf_counter() - start) * 1000\n",
        "    return response.json().get(\"response\", \"\"), latency_ms\n",
        "\n",
        "def clean_response(text: str) -> str:\n",
        "    return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
        "\n",
        "logger.info(\"Helper functions ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Demo 1: Observability Stack Decision Framework\n",
        "\n",
        "LLM observability differs from traditional APM. Key dimensions:\n",
        "\n",
        "| Dimension | Traditional APM | LLM Observability |\n",
        "|-----------|-----------------|-------------------|\n",
        "| Latency | Response time | TTFT, generation time, tool calls |\n",
        "| Errors | HTTP codes | Hallucinations, refusals, toxicity |\n",
        "| Costs | Compute/storage | Token economics (input/output) |\n",
        "| Quality | N/A | User feedback, LLM-as-judge scores |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "OBSERVABILITY: Stack Selection Decision Framework\n",
            "=================================================================\n",
            "\n",
            "LLM Observability Stack Selection\n",
            "==================================\n",
            "\n",
            "DECISION TREE:\n",
            "\n",
            "1. Are you using LangChain?\n",
            "   YES → Start with LangSmith (zero-config integration)\n",
            "   NO → Continue to #2\n",
            "\n",
            "2. Do you need self-hosting (GDPR, data sovereignty)?\n",
            "   YES → Langfuse (MIT license) or Phoenix (Apache 2.0)\n",
            "   NO → Continue to #3\n",
            "\n",
            "3. Do you have existing observability infrastructure?\n",
            "   Datadog → Use Datadog LLM Monitoring (unified stack)\n",
            "   New Relic → Use New Relic AI Monitoring\n",
            "   Neither → Continue to #4\n",
            "\n",
            "4. What's your primary use case?\n",
            "   RAG/Retrieval → Phoenix by Arize (RAG-specific features)\n",
            "   Agents → Langfuse or LangSmith (trace visualization)\n",
            "   Cost tracking → Helicone (fastest setup)\n",
            "   Evaluation focus → Braintrust (eval + observability)\n",
            "\n",
            "\n",
            "TOOL COMPARISON:\n",
            "───────────────────────────────────────────────────────────────────────────\n",
            "Tool         Deployment      Best For       Pricing      EU OK\n",
            "───────────────────────────────────────────────────────────────────────────\n",
            "Langfuse     Cloud/Self-host General OSS    Free tier    ✓\n",
            "Phoenix      Self-host only  RAG, evals     Free (OSS)   ✓\n",
            "LangSmith    Cloud           LangChain      Free tier    ~\n",
            "Helicone     Cloud           Cost tracking  Free tier    ~\n",
            "Opik         Cloud/Self-host Speed          Free tier    ✓\n",
            "Datadog      Cloud           Enterprise     Enterprise   ✓\n",
            "\n",
            "═════════════════════════════════════════════════════════════════\n",
            "EU/GDPR CONSIDERATIONS\n",
            "═════════════════════════════════════════════════════════════════\n",
            "\n",
            "For EU data sovereignty:\n",
            "\n",
            "1. SELF-HOSTED (Full control):\n",
            "   • Langfuse - MIT license, Docker/K8s, PostgreSQL backend\n",
            "   • Phoenix - Apache 2.0, runs locally, no external deps\n",
            "   • Opik - Apache 2.0, self-host option\n",
            "\n",
            "2. EU-HOSTED CLOUD:\n",
            "   • Langfuse Cloud EU (eu.cloud.langfuse.com)\n",
            "   • Datadog EU (datadoghq.eu)\n",
            "\n",
            "3. KEY REQUIREMENTS:\n",
            "   • Data stays in EU region\n",
            "   • No transatlantic data transfers\n",
            "   • Audit logs for compliance\n",
            "   • PII masking before logging\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Demo 1: Observability Decision Framework\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"OBSERVABILITY: Stack Selection Decision Framework\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "OBSERVABILITY_DECISION = \"\"\"\n",
        "LLM Observability Stack Selection\n",
        "==================================\n",
        "\n",
        "DECISION TREE:\n",
        "\n",
        "1. Are you using LangChain?\n",
        "   YES → Start with LangSmith (zero-config integration)\n",
        "   NO → Continue to #2\n",
        "\n",
        "2. Do you need self-hosting (GDPR, data sovereignty)?\n",
        "   YES → Langfuse (MIT license) or Phoenix (Apache 2.0)\n",
        "   NO → Continue to #3\n",
        "\n",
        "3. Do you have existing observability infrastructure?\n",
        "   Datadog → Use Datadog LLM Monitoring (unified stack)\n",
        "   New Relic → Use New Relic AI Monitoring\n",
        "   Neither → Continue to #4\n",
        "\n",
        "4. What's your primary use case?\n",
        "   RAG/Retrieval → Phoenix by Arize (RAG-specific features)\n",
        "   Agents → Langfuse or LangSmith (trace visualization)\n",
        "   Cost tracking → Helicone (fastest setup)\n",
        "   Evaluation focus → Braintrust (eval + observability)\n",
        "\"\"\"\n",
        "\n",
        "print(OBSERVABILITY_DECISION)\n",
        "\n",
        "# Tool comparison matrix\n",
        "tools = [\n",
        "    {\"name\": \"Langfuse\", \"deployment\": \"Cloud/Self-host\", \"best_for\": \"General OSS\", \"pricing\": \"Free tier\", \"license\": \"MIT\", \"eu_friendly\": \"✓\"},\n",
        "    {\"name\": \"Phoenix\", \"deployment\": \"Self-host only\", \"best_for\": \"RAG, evals\", \"pricing\": \"Free (OSS)\", \"license\": \"Apache 2.0\", \"eu_friendly\": \"✓\"},\n",
        "    {\"name\": \"LangSmith\", \"deployment\": \"Cloud\", \"best_for\": \"LangChain\", \"pricing\": \"Free tier\", \"license\": \"Proprietary\", \"eu_friendly\": \"~\"},\n",
        "    {\"name\": \"Helicone\", \"deployment\": \"Cloud\", \"best_for\": \"Cost tracking\", \"pricing\": \"Free tier\", \"license\": \"Proprietary\", \"eu_friendly\": \"~\"},\n",
        "    {\"name\": \"Opik\", \"deployment\": \"Cloud/Self-host\", \"best_for\": \"Speed\", \"pricing\": \"Free tier\", \"license\": \"Apache 2.0\", \"eu_friendly\": \"✓\"},\n",
        "    {\"name\": \"Datadog\", \"deployment\": \"Cloud\", \"best_for\": \"Enterprise\", \"pricing\": \"Enterprise\", \"license\": \"Proprietary\", \"eu_friendly\": \"✓\"},\n",
        "]\n",
        "\n",
        "print(\"\\nTOOL COMPARISON:\")\n",
        "print(\"─\"*75)\n",
        "print(f\"{'Tool':<12} {'Deployment':<15} {'Best For':<14} {'Pricing':<12} {'EU OK'}\")\n",
        "print(\"─\"*75)\n",
        "for t in tools:\n",
        "    print(f\"{t['name']:<12} {t['deployment']:<15} {t['best_for']:<14} {t['pricing']:<12} {t['eu_friendly']}\")\n",
        "\n",
        "print(\"\\n\" + \"═\"*65)\n",
        "print(\"EU/GDPR CONSIDERATIONS\")\n",
        "print(\"═\"*65)\n",
        "print(\"\"\"\n",
        "For EU data sovereignty:\n",
        "\n",
        "1. SELF-HOSTED (Full control):\n",
        "   • Langfuse - MIT license, Docker/K8s, PostgreSQL backend\n",
        "   • Phoenix - Apache 2.0, runs locally, no external deps\n",
        "   • Opik - Apache 2.0, self-host option\n",
        "\n",
        "2. EU-HOSTED CLOUD:\n",
        "   • Langfuse Cloud EU (eu.cloud.langfuse.com)\n",
        "   • Datadog EU (datadoghq.eu)\n",
        "\n",
        "3. KEY REQUIREMENTS:\n",
        "   • Data stays in EU region\n",
        "   • No transatlantic data transfers\n",
        "   • Audit logs for compliance\n",
        "   • PII masking before logging\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Demo 2: Phoenix - Self-Hosted RAG Observability\n",
        "\n",
        "Phoenix by Arize is an open-source (Apache 2.0) observability tool with:\n",
        "- **Local execution** - No data leaves your machine\n",
        "- **RAG-specific** - Retrieval quality metrics built-in\n",
        "- **Trace visualization** - Multi-step LLM flow debugging\n",
        "- **Evaluation** - LLM-as-judge integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "PHOENIX: Open Source LLM Observability\n",
            "=================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/titasbiswas/miniforge3/envs/guardrails-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✓ Phoenix installed\n",
            "\n",
            "PHOENIX ARCHITECTURE:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "  ┌──────────────┐     ┌──────────────────────────────────────┐\n",
            "  │  Your App    │────▶│         Phoenix Server               │\n",
            "  │  (LLM calls) │     │  (localhost:6006)                    │\n",
            "  └──────────────┘     ├──────────────────────────────────────┤\n",
            "         │             │  • Trace Collection                  │\n",
            "         │             │  • Span Visualization                │\n",
            "    OpenTelemetry      │  • RAG Metrics (MRR, NDCG)          │\n",
            "    Instrumentation    │  • LLM-as-Judge Evals               │\n",
            "         │             │  • Prompt Playground                 │\n",
            "         ▼             └──────────────────────────────────────┘\n",
            "  ┌──────────────┐\n",
            "  │   Traces     │     DATA STAYS LOCAL (GDPR-friendly)\n",
            "  │   (OTLP)     │\n",
            "  └──────────────┘\n",
            "\n",
            "KEY FEATURES:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "  1. TRACING\n",
            "     • Auto-instrumentation for OpenAI, Anthropic, LangChain\n",
            "     • Multi-step flow visualization\n",
            "     • Latency breakdown per span\n",
            "\n",
            "  2. RAG ANALYSIS\n",
            "     • Retrieval precision/recall\n",
            "     • Context relevance scoring\n",
            "     • Query-document embeddings visualization\n",
            "\n",
            "  3. EVALUATION\n",
            "     • Built-in LLM-as-judge templates\n",
            "     • Custom evaluation criteria\n",
            "     • Experiment tracking\n",
            "\n",
            "  4. PROMPT ENGINEERING\n",
            "     • Prompt playground\n",
            "     • A/B testing prompts\n",
            "     • Version tracking\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Demo 2: Phoenix Setup and Core Concepts\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"PHOENIX: Open Source LLM Observability\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "phoenix_available = False\n",
        "try:\n",
        "    import phoenix as px\n",
        "    phoenix_available = True\n",
        "    print(\"\\n✓ Phoenix installed\")\n",
        "except ImportError:\n",
        "    print(\"\\n⚠ Phoenix not installed\")\n",
        "    print(\"  pip install arize-phoenix openinference-instrumentation\")\n",
        "\n",
        "print(\"\"\"\n",
        "PHOENIX ARCHITECTURE:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "  ┌──────────────┐     ┌──────────────────────────────────────┐\n",
        "  │  Your App    │────▶│         Phoenix Server               │\n",
        "  │  (LLM calls) │     │  (localhost:6006)                    │\n",
        "  └──────────────┘     ├──────────────────────────────────────┤\n",
        "         │             │  • Trace Collection                  │\n",
        "         │             │  • Span Visualization                │\n",
        "    OpenTelemetry      │  • RAG Metrics (MRR, NDCG)          │\n",
        "    Instrumentation    │  • LLM-as-Judge Evals               │\n",
        "         │             │  • Prompt Playground                 │\n",
        "         ▼             └──────────────────────────────────────┘\n",
        "  ┌──────────────┐\n",
        "  │   Traces     │     DATA STAYS LOCAL (GDPR-friendly)\n",
        "  │   (OTLP)     │\n",
        "  └──────────────┘\n",
        "\n",
        "KEY FEATURES:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "  1. TRACING\n",
        "     • Auto-instrumentation for OpenAI, Anthropic, LangChain\n",
        "     • Multi-step flow visualization\n",
        "     • Latency breakdown per span\n",
        "\n",
        "  2. RAG ANALYSIS\n",
        "     • Retrieval precision/recall\n",
        "     • Context relevance scoring\n",
        "     • Query-document embeddings visualization\n",
        "\n",
        "  3. EVALUATION\n",
        "     • Built-in LLM-as-judge templates\n",
        "     • Custom evaluation criteria\n",
        "     • Experiment tracking\n",
        "\n",
        "  4. PROMPT ENGINEERING\n",
        "     • Prompt playground\n",
        "     • A/B testing prompts\n",
        "     • Version tracking\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "PHOENIX: Tracing Pattern Demo (Local Simulation)\n",
            "=================================================================\n",
            "\n",
            "[Simulated RAG Trace]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "  Query: What is Python?\n",
            "  Response: Python is a high-level, interpreted programming language known for its simplicity, readability, and ...\n",
            "\n",
            "  Trace Summary:\n",
            "    trace_id: trace_1767393196344\n",
            "    total_spans: 3\n",
            "    total_duration_ms: 1855.29420900275\n",
            "    span_types: ['embedding', 'retrieval', 'llm']\n",
            "\n",
            "  Span Details:\n",
            "    [embedding ] embed_query: 12.5ms\n",
            "    [retrieval ] retrieve_docs: 25.0ms\n",
            "    [llm       ] llm_generate: 1817.8ms\n"
          ]
        }
      ],
      "source": [
        "# Demo 2b: Phoenix Tracing Simulation (without full server)\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"PHOENIX: Tracing Pattern Demo (Local Simulation)\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "# Simulate what Phoenix traces would look like\n",
        "# In production, you'd use OpenTelemetry instrumentation\n",
        "\n",
        "@dataclass\n",
        "class Span:\n",
        "    \"\"\"Simulated OpenTelemetry span for LLM observability.\"\"\"\n",
        "    name: str\n",
        "    span_type: str  # \"llm\", \"retrieval\", \"tool\", \"chain\"\n",
        "    start_time: float\n",
        "    end_time: float = 0\n",
        "    attributes: Dict[str, Any] = field(default_factory=dict)\n",
        "    events: List[Dict] = field(default_factory=list)\n",
        "    parent_id: str = None\n",
        "    span_id: str = field(default_factory=lambda: f\"span_{int(time.time()*1000)}\")\n",
        "    \n",
        "    @property\n",
        "    def duration_ms(self) -> float:\n",
        "        return (self.end_time - self.start_time) * 1000\n",
        "\n",
        "@dataclass\n",
        "class Trace:\n",
        "    \"\"\"Collection of spans representing an LLM operation.\"\"\"\n",
        "    trace_id: str\n",
        "    spans: List[Span] = field(default_factory=list)\n",
        "    \n",
        "    def add_span(self, span: Span):\n",
        "        self.spans.append(span)\n",
        "    \n",
        "    def summary(self) -> Dict:\n",
        "        return {\n",
        "            \"trace_id\": self.trace_id,\n",
        "            \"total_spans\": len(self.spans),\n",
        "            \"total_duration_ms\": sum(s.duration_ms for s in self.spans),\n",
        "            \"span_types\": list({s.span_type for s in self.spans})\n",
        "        }\n",
        "\n",
        "class LocalTracer:\n",
        "    \"\"\"Simulated tracer demonstrating Phoenix patterns.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.traces: List[Trace] = []\n",
        "        self.current_trace: Optional[Trace] = None\n",
        "    \n",
        "    def start_trace(self, name: str) -> Trace:\n",
        "        trace = Trace(trace_id=f\"trace_{int(time.time()*1000)}\")\n",
        "        self.current_trace = trace\n",
        "        self.traces.append(trace)\n",
        "        return trace\n",
        "    \n",
        "    def start_span(self, name: str, span_type: str, attributes: Dict = None) -> Span:\n",
        "        span = Span(\n",
        "            name=name,\n",
        "            span_type=span_type,\n",
        "            start_time=time.perf_counter(),\n",
        "            attributes=attributes or {}\n",
        "        )\n",
        "        if self.current_trace:\n",
        "            self.current_trace.add_span(span)\n",
        "        return span\n",
        "    \n",
        "    def end_span(self, span: Span, output: Any = None):\n",
        "        span.end_time = time.perf_counter()\n",
        "        if output:\n",
        "            span.attributes[\"output\"] = str(output)[:200]\n",
        "\n",
        "# Demo: Trace a RAG query\n",
        "print(\"\\n[Simulated RAG Trace]\")\n",
        "print(\"─\"*65)\n",
        "\n",
        "tracer = LocalTracer()\n",
        "\n",
        "def traced_rag_query(query: str) -> Dict:\n",
        "    \"\"\"Simulate a RAG query with full tracing.\"\"\"\n",
        "    \n",
        "    # Start trace\n",
        "    trace = tracer.start_trace(\"rag_query\")\n",
        "    \n",
        "    # Span 1: Embedding\n",
        "    embed_span = tracer.start_span(\"embed_query\", \"embedding\", {\n",
        "        \"model\": \"all-MiniLM-L6-v2\",\n",
        "        \"input_text\": query\n",
        "    })\n",
        "    time.sleep(0.01)  # Simulate embedding\n",
        "    tracer.end_span(embed_span, output=\"[0.1, 0.2, ...384 dims]\")\n",
        "    \n",
        "    # Span 2: Retrieval\n",
        "    retrieve_span = tracer.start_span(\"retrieve_docs\", \"retrieval\", {\n",
        "        \"top_k\": 3,\n",
        "        \"index\": \"knowledge_base\"\n",
        "    })\n",
        "    time.sleep(0.02)  # Simulate retrieval\n",
        "    retrieved_docs = [\"Doc 1: Python basics...\", \"Doc 2: Data types...\"]\n",
        "    retrieve_span.attributes[\"num_docs\"] = len(retrieved_docs)\n",
        "    tracer.end_span(retrieve_span, output=retrieved_docs)\n",
        "    \n",
        "    # Span 3: LLM Generation\n",
        "    llm_span = tracer.start_span(\"llm_generate\", \"llm\", {\n",
        "        \"model\": WEAK_MODEL,\n",
        "        \"temperature\": 0.7,\n",
        "        \"input_tokens\": 150,\n",
        "        \"context_chunks\": len(retrieved_docs)\n",
        "    })\n",
        "    \n",
        "    if ollama_ready:\n",
        "        context = \"\\n\".join(retrieved_docs)\n",
        "        prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer briefly:\"\n",
        "        response, latency = ollama_generate(prompt, WEAK_MODEL)\n",
        "        response = clean_response(response)\n",
        "        llm_span.attributes[\"output_tokens\"] = int(len(response.split()) * 1.3)\n",
        "        llm_span.attributes[\"latency_ms\"] = latency\n",
        "    else:\n",
        "        response = \"[LLM response would appear here]\"\n",
        "        time.sleep(0.1)\n",
        "    \n",
        "    tracer.end_span(llm_span, output=response)\n",
        "    \n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"response\": response,\n",
        "        \"trace\": trace.summary()\n",
        "    }\n",
        "\n",
        "# Execute traced query\n",
        "result = traced_rag_query(\"What is Python?\")\n",
        "\n",
        "print(f\"  Query: {result['query']}\")\n",
        "print(f\"  Response: {result['response'][:100]}...\")\n",
        "print(f\"\\n  Trace Summary:\")\n",
        "for k, v in result['trace'].items():\n",
        "    print(f\"    {k}: {v}\")\n",
        "\n",
        "print(\"\\n  Span Details:\")\n",
        "for span in tracer.traces[-1].spans:\n",
        "    print(f\"    [{span.span_type:10}] {span.name}: {span.duration_ms:.1f}ms\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "PHOENIX: Setup Guide\n",
            "=================================================================\n",
            "\n",
            "1. INSTALLATION:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "   pip install arize-phoenix openinference-instrumentation-openai\n",
            "\n",
            "2. START SERVER:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "   # Terminal 1: Start Phoenix\n",
            "   python -m phoenix.server.main serve\n",
            "\n",
            "   # Opens at http://localhost:6006\n",
            "\n",
            "3. INSTRUMENT YOUR APP:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "   from phoenix.otel import register\n",
            "   from openinference.instrumentation.openai import OpenAIInstrumentor\n",
            "\n",
            "   # Connect to Phoenix\n",
            "   tracer_provider = register(\n",
            "       project_name=\"my-llm-app\",\n",
            "       endpoint=\"http://localhost:6006/v1/traces\"\n",
            "   )\n",
            "\n",
            "   # Auto-instrument OpenAI calls\n",
            "   OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
            "\n",
            "   # Now all OpenAI calls are automatically traced!\n",
            "   from openai import OpenAI\n",
            "   client = OpenAI()\n",
            "   response = client.chat.completions.create(\n",
            "       model=\"gpt-4o-mini\",\n",
            "       messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
            "   )\n",
            "\n",
            "4. VIEW TRACES:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "   Open http://localhost:6006 to see:\n",
            "   • All traces with latency\n",
            "   • Token usage per request\n",
            "   • Span hierarchy (retrieval → LLM → etc.)\n",
            "   • Error rates and types\n",
            "\n",
            "5. RUN EVALUATIONS:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "   from phoenix.evals import (\n",
            "       HallucinationEvaluator,\n",
            "       RelevanceEvaluator,\n",
            "       run_evals\n",
            "   )\n",
            "\n",
            "   # Evaluate traces\n",
            "   hallucination_eval = HallucinationEvaluator(model=eval_model)\n",
            "   relevance_eval = RelevanceEvaluator(model=eval_model)\n",
            "\n",
            "   results = run_evals(\n",
            "       dataframe=traces_df,\n",
            "       evaluators=[hallucination_eval, relevance_eval]\n",
            "   )\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Demo 2c: Phoenix Setup Guide\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"PHOENIX: Setup Guide\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "print(\"\"\"\n",
        "1. INSTALLATION:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "   pip install arize-phoenix openinference-instrumentation-openai\n",
        "\n",
        "2. START SERVER:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "   # Terminal 1: Start Phoenix\n",
        "   python -m phoenix.server.main serve\n",
        "   \n",
        "   # Opens at http://localhost:6006\n",
        "\n",
        "3. INSTRUMENT YOUR APP:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "   from phoenix.otel import register\n",
        "   from openinference.instrumentation.openai import OpenAIInstrumentor\n",
        "   \n",
        "   # Connect to Phoenix\n",
        "   tracer_provider = register(\n",
        "       project_name=\"my-llm-app\",\n",
        "       endpoint=\"http://localhost:6006/v1/traces\"\n",
        "   )\n",
        "   \n",
        "   # Auto-instrument OpenAI calls\n",
        "   OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
        "   \n",
        "   # Now all OpenAI calls are automatically traced!\n",
        "   from openai import OpenAI\n",
        "   client = OpenAI()\n",
        "   response = client.chat.completions.create(\n",
        "       model=\"gpt-4o-mini\",\n",
        "       messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
        "   )\n",
        "\n",
        "4. VIEW TRACES:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "   Open http://localhost:6006 to see:\n",
        "   • All traces with latency\n",
        "   • Token usage per request\n",
        "   • Span hierarchy (retrieval → LLM → etc.)\n",
        "   • Error rates and types\n",
        "\n",
        "5. RUN EVALUATIONS:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "   from phoenix.evals import (\n",
        "       HallucinationEvaluator,\n",
        "       RelevanceEvaluator,\n",
        "       run_evals\n",
        "   )\n",
        "   \n",
        "   # Evaluate traces\n",
        "   hallucination_eval = HallucinationEvaluator(model=eval_model)\n",
        "   relevance_eval = RelevanceEvaluator(model=eval_model)\n",
        "   \n",
        "   results = run_evals(\n",
        "       dataframe=traces_df,\n",
        "       evaluators=[hallucination_eval, relevance_eval]\n",
        "   )\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Demo 3: Langfuse Patterns (Local Simulation)\n",
        "\n",
        "Langfuse is the most popular open-source LLM observability platform:\n",
        "- **19K+ GitHub stars**\n",
        "- **MIT license** - Full control\n",
        "- **EU-hosted cloud** option (eu.cloud.langfuse.com)\n",
        "- **Self-hosting** with Docker\n",
        "\n",
        "We'll demonstrate the patterns locally without requiring cloud setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "LANGFUSE: LLM Observability Patterns\n",
            "=================================================================\n",
            "\n",
            "[Traced Support Ticket Processing]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "  Category: i'd classify the support ticket as \"technical\". th\n",
            "  Trace ID: trace_040318231536\n",
            "\n",
            "  Trace Data (what Langfuse stores):\n",
            "    Name: support_ticket\n",
            "    User ID: cust_12345\n",
            "    Observations: 1\n",
            "    Scores: classification_confidence = 0.85\n"
          ]
        }
      ],
      "source": [
        "# Demo 3: Langfuse Pattern Simulation\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"LANGFUSE: LLM Observability Patterns\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "# Simulate Langfuse's decorator-based tracing pattern\n",
        "# In production, you'd use: from langfuse.decorators import observe\n",
        "\n",
        "class LangfuseSimulator:\n",
        "    \"\"\"Simulates Langfuse tracing to demonstrate patterns.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.traces = []\n",
        "        self.current_trace_id = None\n",
        "        self.observations = []\n",
        "    \n",
        "    def trace(self, name: str, user_id: str = None, metadata: Dict = None):\n",
        "        trace_id = f\"trace_{datetime.now().strftime('%H%M%S%f')}\"\n",
        "        self.current_trace_id = trace_id\n",
        "        trace_data = {\n",
        "            \"id\": trace_id,\n",
        "            \"name\": name,\n",
        "            \"user_id\": user_id,\n",
        "            \"metadata\": metadata or {},\n",
        "            \"start_time\": datetime.now().isoformat(),\n",
        "            \"observations\": []\n",
        "        }\n",
        "        self.traces.append(trace_data)\n",
        "        return trace_data\n",
        "    \n",
        "    def generation(self, name: str, model: str, input_text: str, \n",
        "                   output: str = None, usage: Dict = None):\n",
        "        observation = {\n",
        "            \"type\": \"generation\",\n",
        "            \"name\": name,\n",
        "            \"model\": model,\n",
        "            \"input\": input_text[:500],\n",
        "            \"output\": output[:500] if output else None,\n",
        "            \"usage\": usage or {},\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        if self.traces:\n",
        "            self.traces[-1][\"observations\"].append(observation)\n",
        "        return observation\n",
        "    \n",
        "    def score(self, trace_id: str, name: str, value: float, comment: str = None):\n",
        "        \"\"\"Add evaluation score to a trace.\"\"\"\n",
        "        score_data = {\n",
        "            \"trace_id\": trace_id,\n",
        "            \"name\": name,\n",
        "            \"value\": value,\n",
        "            \"comment\": comment\n",
        "        }\n",
        "        for trace in self.traces:\n",
        "            if trace[\"id\"] == trace_id:\n",
        "                if \"scores\" not in trace:\n",
        "                    trace[\"scores\"] = []\n",
        "                trace[\"scores\"].append(score_data)\n",
        "        return score_data\n",
        "\n",
        "# Demo the patterns\n",
        "lf = LangfuseSimulator()\n",
        "\n",
        "print(\"\\n[Traced Support Ticket Processing]\")\n",
        "print(\"─\"*65)\n",
        "\n",
        "def process_support_ticket(ticket: str, customer_id: str):\n",
        "    \"\"\"Process support ticket with Langfuse-style tracing.\"\"\"\n",
        "    \n",
        "    # Start trace\n",
        "    trace = lf.trace(\n",
        "        name=\"support_ticket\",\n",
        "        user_id=customer_id,\n",
        "        metadata={\"channel\": \"web\", \"priority\": \"normal\"}\n",
        "    )\n",
        "    \n",
        "    # Simulate LLM generation\n",
        "    if ollama_ready:\n",
        "        prompt = f\"Classify this support ticket into: billing, technical, general.\\n\\nTicket: {ticket}\\n\\nCategory:\"\n",
        "        response, latency = ollama_generate(prompt, WEAK_MODEL, temperature=0.3)\n",
        "        response = clean_response(response)\n",
        "    else:\n",
        "        response = \"technical\"\n",
        "        latency = 50\n",
        "    \n",
        "    # Log generation\n",
        "    lf.generation(\n",
        "        name=\"classify_ticket\",\n",
        "        model=WEAK_MODEL,\n",
        "        input_text=ticket,\n",
        "        output=response,\n",
        "        usage={\"input_tokens\": len(ticket.split()), \"output_tokens\": len(response.split()), \"latency_ms\": latency}\n",
        "    )\n",
        "    \n",
        "    # Add quality score\n",
        "    lf.score(\n",
        "        trace_id=trace[\"id\"],\n",
        "        name=\"classification_confidence\",\n",
        "        value=0.85,\n",
        "        comment=\"High confidence classification\"\n",
        "    )\n",
        "    \n",
        "    return {\"category\": response.strip().lower(), \"trace_id\": trace[\"id\"]}\n",
        "\n",
        "# Process a ticket\n",
        "result = process_support_ticket(\n",
        "    \"The app keeps crashing when I try to upload files\",\n",
        "    customer_id=\"cust_12345\"\n",
        ")\n",
        "\n",
        "print(f\"  Category: {result['category'][:50]}\")\n",
        "print(f\"  Trace ID: {result['trace_id']}\")\n",
        "\n",
        "# Show trace data\n",
        "print(\"\\n  Trace Data (what Langfuse stores):\")\n",
        "trace_data = lf.traces[-1]\n",
        "print(f\"    Name: {trace_data['name']}\")\n",
        "print(f\"    User ID: {trace_data['user_id']}\")\n",
        "print(f\"    Observations: {len(trace_data['observations'])}\")\n",
        "if trace_data.get('scores'):\n",
        "    print(f\"    Scores: {trace_data['scores'][0]['name']} = {trace_data['scores'][0]['value']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "LANGFUSE: Setup Guide\n",
            "=================================================================\n",
            "\n",
            "1. CLOUD SETUP (Quickest):\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "   # Sign up at https://cloud.langfuse.com (or eu.cloud.langfuse.com for EU)\n",
            "   # Get API keys from project settings\n",
            "\n",
            "   export LANGFUSE_PUBLIC_KEY=\"pk-...\"\n",
            "   export LANGFUSE_SECRET_KEY=\"sk-...\"\n",
            "   export LANGFUSE_HOST=\"https://cloud.langfuse.com\"  # or eu.cloud...\n",
            "\n",
            "2. SELF-HOSTED (Data Sovereignty):\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "   # docker-compose.yml\n",
            "   services:\n",
            "     langfuse:\n",
            "       image: langfuse/langfuse:latest\n",
            "       ports:\n",
            "         - \"3000:3000\"\n",
            "       environment:\n",
            "         - DATABASE_URL=postgresql://...\n",
            "         - NEXTAUTH_SECRET=your-secret\n",
            "         - SALT=your-salt\n",
            "\n",
            "   # Then: docker-compose up -d\n",
            "\n",
            "3. INTEGRATION OPTIONS:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "   pip install langfuse\n",
            "\n",
            "   # Option A: Decorators (cleanest)\n",
            "   from langfuse.decorators import observe\n",
            "\n",
            "   @observe()\n",
            "   def my_llm_function(query: str):\n",
            "       # Automatically traces inputs, outputs, latency\n",
            "       return llm.complete(query)\n",
            "\n",
            "   # Option B: OpenAI wrapper (drop-in replacement)\n",
            "   from langfuse.openai import OpenAI\n",
            "   client = OpenAI()  # Auto-traces all calls\n",
            "\n",
            "   # Option C: LangChain callback\n",
            "   from langfuse.callback import CallbackHandler\n",
            "   handler = CallbackHandler()\n",
            "   chain.invoke(..., config={\"callbacks\": [handler]})\n",
            "\n",
            "4. ADDING SCORES:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "   from langfuse import Langfuse\n",
            "\n",
            "   langfuse = Langfuse()\n",
            "\n",
            "   # Score a trace programmatically\n",
            "   langfuse.score(\n",
            "       trace_id=\"trace-xxx\",\n",
            "       name=\"quality\",\n",
            "       value=0.9,\n",
            "       comment=\"User thumbs up\"\n",
            "   )\n",
            "\n",
            "   # Or use LLM-as-judge in dashboard → Evaluation tab\n",
            "\n",
            "5. KEY METRICS TO TRACK:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "   • Latency (P50, P95, P99)\n",
            "   • Token usage and costs\n",
            "   • Error rates by type\n",
            "   • Quality scores over time\n",
            "   • User satisfaction (thumbs up/down)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Demo 3b: Langfuse Setup Guide\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"LANGFUSE: Setup Guide\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "print(\"\"\"\n",
        "1. CLOUD SETUP (Quickest):\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "   # Sign up at https://cloud.langfuse.com (or eu.cloud.langfuse.com for EU)\n",
        "   # Get API keys from project settings\n",
        "   \n",
        "   export LANGFUSE_PUBLIC_KEY=\"pk-...\"\n",
        "   export LANGFUSE_SECRET_KEY=\"sk-...\"\n",
        "   export LANGFUSE_HOST=\"https://cloud.langfuse.com\"  # or eu.cloud...\n",
        "\n",
        "2. SELF-HOSTED (Data Sovereignty):\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "   # docker-compose.yml\n",
        "   services:\n",
        "     langfuse:\n",
        "       image: langfuse/langfuse:latest\n",
        "       ports:\n",
        "         - \"3000:3000\"\n",
        "       environment:\n",
        "         - DATABASE_URL=postgresql://...\n",
        "         - NEXTAUTH_SECRET=your-secret\n",
        "         - SALT=your-salt\n",
        "   \n",
        "   # Then: docker-compose up -d\n",
        "\n",
        "3. INTEGRATION OPTIONS:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "   pip install langfuse\n",
        "   \n",
        "   # Option A: Decorators (cleanest)\n",
        "   from langfuse.decorators import observe\n",
        "   \n",
        "   @observe()\n",
        "   def my_llm_function(query: str):\n",
        "       # Automatically traces inputs, outputs, latency\n",
        "       return llm.complete(query)\n",
        "   \n",
        "   # Option B: OpenAI wrapper (drop-in replacement)\n",
        "   from langfuse.openai import OpenAI\n",
        "   client = OpenAI()  # Auto-traces all calls\n",
        "   \n",
        "   # Option C: LangChain callback\n",
        "   from langfuse.callback import CallbackHandler\n",
        "   handler = CallbackHandler()\n",
        "   chain.invoke(..., config={\"callbacks\": [handler]})\n",
        "\n",
        "4. ADDING SCORES:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "   from langfuse import Langfuse\n",
        "   \n",
        "   langfuse = Langfuse()\n",
        "   \n",
        "   # Score a trace programmatically\n",
        "   langfuse.score(\n",
        "       trace_id=\"trace-xxx\",\n",
        "       name=\"quality\",\n",
        "       value=0.9,\n",
        "       comment=\"User thumbs up\"\n",
        "   )\n",
        "   \n",
        "   # Or use LLM-as-judge in dashboard → Evaluation tab\n",
        "\n",
        "5. KEY METRICS TO TRACK:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "   • Latency (P50, P95, P99)\n",
        "   • Token usage and costs\n",
        "   • Error rates by type\n",
        "   • Quality scores over time\n",
        "   • User satisfaction (thumbs up/down)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Demo 4: DeepEval - LLM Evaluation Framework\n",
        "\n",
        "DeepEval is an open-source evaluation framework with:\n",
        "- **Pytest integration** - Run evals in CI/CD\n",
        "- **Built-in metrics** - Faithfulness, relevancy, hallucination\n",
        "- **Custom metrics** - G-Eval for any criteria\n",
        "- **~80% agreement** with human judgment\n",
        "\n",
        "Key metrics:\n",
        "- **Faithfulness**: Is response grounded in context?\n",
        "- **Answer Relevancy**: Does it answer the question?\n",
        "- **Contextual Precision**: Are retrieved docs relevant?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "DEEPEVAL: LLM Evaluation Framework\n",
            "=================================================================\n",
            "\n",
            "✓ DeepEval installed\n",
            "\n",
            "DEEPEVAL ARCHITECTURE:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "  ┌─────────────────┐\n",
            "  │   Test Case     │\n",
            "  │  ─────────────  │\n",
            "  │  • input        │     ┌──────────────┐\n",
            "  │  • actual_output│────▶│   Metrics    │────▶ Pass/Fail\n",
            "  │  • context      │     │  (LLM-judge) │\n",
            "  │  • expected     │     └──────────────┘\n",
            "  └─────────────────┘\n",
            "\n",
            "BUILT-IN METRICS:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "  RETRIEVAL METRICS:\n",
            "    • ContextualPrecisionMetric  - Are retrieved docs relevant?\n",
            "    • ContextualRecallMetric     - Did we get all relevant docs?\n",
            "    • ContextualRelevancyMetric  - How relevant is the context?\n",
            "\n",
            "  GENERATION METRICS:\n",
            "    • FaithfulnessMetric    - Is response grounded in context?\n",
            "    • AnswerRelevancyMetric - Does it answer the question?\n",
            "    • HallucinationMetric   - Did the model make things up?\n",
            "\n",
            "  SAFETY METRICS:\n",
            "    • ToxicityMetric  - Is the response toxic?\n",
            "    • BiasMetric      - Does it show bias?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Demo 4: DeepEval Overview and Local Simulation\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"DEEPEVAL: LLM Evaluation Framework\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "deepeval_available = False\n",
        "try:\n",
        "    import deepeval\n",
        "    deepeval_available = True\n",
        "    print(\"\\n✓ DeepEval installed\")\n",
        "except ImportError:\n",
        "    print(\"\\n⚠ DeepEval not installed\")\n",
        "    print(\"  pip install deepeval\")\n",
        "\n",
        "print(\"\"\"\n",
        "DEEPEVAL ARCHITECTURE:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "  ┌─────────────────┐\n",
        "  │   Test Case     │\n",
        "  │  ─────────────  │\n",
        "  │  • input        │     ┌──────────────┐\n",
        "  │  • actual_output│────▶│   Metrics    │────▶ Pass/Fail\n",
        "  │  • context      │     │  (LLM-judge) │\n",
        "  │  • expected     │     └──────────────┘\n",
        "  └─────────────────┘\n",
        "\n",
        "BUILT-IN METRICS:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "  RETRIEVAL METRICS:\n",
        "    • ContextualPrecisionMetric  - Are retrieved docs relevant?\n",
        "    • ContextualRecallMetric     - Did we get all relevant docs?\n",
        "    • ContextualRelevancyMetric  - How relevant is the context?\n",
        "    \n",
        "  GENERATION METRICS:\n",
        "    • FaithfulnessMetric    - Is response grounded in context?\n",
        "    • AnswerRelevancyMetric - Does it answer the question?\n",
        "    • HallucinationMetric   - Did the model make things up?\n",
        "    \n",
        "  SAFETY METRICS:\n",
        "    • ToxicityMetric  - Is the response toxic?\n",
        "    • BiasMetric      - Does it show bias?\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "DEEPEVAL: Local Evaluation Demo (Ollama as Judge)\n",
            "=================================================================\n",
            "\n",
            "[Creating Test Cases]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "  Test Case 1:\n",
            "    Question: What is the return policy?\n",
            "    Response: You can return items within 30 days for a full refund. Items...\n",
            "    Context chunks: 3\n",
            "\n",
            "  Test Case 2:\n",
            "    Question: How do I contact support?\n",
            "    Response: You can reach our support team 24/7 via email at support@com...\n",
            "    Context chunks: 2\n"
          ]
        }
      ],
      "source": [
        "# Demo 4b: DeepEval Pattern Simulation with Local LLM\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"DEEPEVAL: Local Evaluation Demo (Ollama as Judge)\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "# Simulate DeepEval's evaluation pattern using Ollama as the judge\n",
        "\n",
        "@dataclass\n",
        "class EvalTestCase:\n",
        "    \"\"\"Simulates DeepEval's LLMTestCase.\"\"\"\n",
        "    input: str                        # User query\n",
        "    actual_output: str                # Generated response\n",
        "    retrieval_context: List[str]      # Retrieved documents\n",
        "    expected_output: str = None       # Ground truth (optional)\n",
        "\n",
        "@dataclass\n",
        "class EvalResult:\n",
        "    \"\"\"Result of a metric evaluation.\"\"\"\n",
        "    metric_name: str\n",
        "    score: float\n",
        "    passed: bool\n",
        "    reason: str\n",
        "\n",
        "class LocalEvaluator:\n",
        "    \"\"\"Simulates DeepEval metrics using local Ollama models.\"\"\"\n",
        "    \n",
        "    def __init__(self, model: str = EVAL_MODEL, threshold: float = 0.7):\n",
        "        self.model = model\n",
        "        self.threshold = threshold\n",
        "    \n",
        "    def evaluate_faithfulness(self, test_case: EvalTestCase) -> EvalResult:\n",
        "        \"\"\"\n",
        "        Faithfulness: Is the response grounded in the provided context?\n",
        "        Score 0-1, higher = more faithful to source.\n",
        "        \"\"\"\n",
        "        context_str = \"\\n\".join(test_case.retrieval_context)\n",
        "        \n",
        "        prompt = f\"\"\"You are an evaluation judge. Rate the faithfulness of the response.\n",
        "Faithfulness = Is every claim in the response supported by the context?\n",
        "\n",
        "CONTEXT:\n",
        "{context_str}\n",
        "\n",
        "RESPONSE TO EVALUATE:\n",
        "{test_case.actual_output}\n",
        "\n",
        "Rate faithfulness from 0.0 (completely unfaithful/hallucinated) to 1.0 (fully grounded).\n",
        "Output ONLY a JSON: {{\"score\": <number>, \"reason\": \"<brief explanation>\"}}\"\"\"\n",
        "        \n",
        "        if ollama_ready:\n",
        "            raw_response, _ = ollama_generate(prompt, self.model, temperature=0.1)\n",
        "            raw_response = clean_response(raw_response)\n",
        "            try:\n",
        "                # Extract JSON from response\n",
        "                import json\n",
        "                json_match = re.search(r'\\{[^}]+\\}', raw_response)\n",
        "                if json_match:\n",
        "                    result = json.loads(json_match.group())\n",
        "                    score = float(result.get(\"score\", 0.5))\n",
        "                    reason = result.get(\"reason\", \"No reason provided\")\n",
        "                else:\n",
        "                    score = 0.5\n",
        "                    reason = \"Could not parse response\"\n",
        "            except:\n",
        "                score = 0.5\n",
        "                reason = \"Parse error\"\n",
        "        else:\n",
        "            score = 0.8\n",
        "            reason = \"Simulated evaluation\"\n",
        "        \n",
        "        return EvalResult(\n",
        "            metric_name=\"faithfulness\",\n",
        "            score=score,\n",
        "            passed=score >= self.threshold,\n",
        "            reason=reason\n",
        "        )\n",
        "    \n",
        "    def evaluate_relevancy(self, test_case: EvalTestCase) -> EvalResult:\n",
        "        \"\"\"\n",
        "        Answer Relevancy: Does the response answer the question?\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"You are an evaluation judge. Rate how well the response answers the question.\n",
        "\n",
        "QUESTION:\n",
        "{test_case.input}\n",
        "\n",
        "RESPONSE:\n",
        "{test_case.actual_output}\n",
        "\n",
        "Rate relevancy from 0.0 (completely irrelevant) to 1.0 (perfectly answers the question).\n",
        "Output ONLY a JSON: {{\"score\": <number>, \"reason\": \"<brief explanation>\"}}\"\"\"\n",
        "        \n",
        "        if ollama_ready:\n",
        "            raw_response, _ = ollama_generate(prompt, self.model, temperature=0.1)\n",
        "            raw_response = clean_response(raw_response)\n",
        "            try:\n",
        "                json_match = re.search(r'\\{[^}]+\\}', raw_response)\n",
        "                if json_match:\n",
        "                    result = json.loads(json_match.group())\n",
        "                    score = float(result.get(\"score\", 0.5))\n",
        "                    reason = result.get(\"reason\", \"No reason provided\")\n",
        "                else:\n",
        "                    score = 0.5\n",
        "                    reason = \"Could not parse response\"\n",
        "            except:\n",
        "                score = 0.5\n",
        "                reason = \"Parse error\"\n",
        "        else:\n",
        "            score = 0.9\n",
        "            reason = \"Simulated evaluation\"\n",
        "        \n",
        "        return EvalResult(\n",
        "            metric_name=\"answer_relevancy\",\n",
        "            score=score,\n",
        "            passed=score >= self.threshold,\n",
        "            reason=reason\n",
        "        )\n",
        "\n",
        "# Create test cases\n",
        "print(\"\\n[Creating Test Cases]\")\n",
        "print(\"─\"*65)\n",
        "\n",
        "test_cases = [\n",
        "    EvalTestCase(\n",
        "        input=\"What is the return policy?\",\n",
        "        actual_output=\"You can return items within 30 days for a full refund. Items must be unused and in original packaging.\",\n",
        "        retrieval_context=[\n",
        "            \"Return Policy: Items may be returned within 30 days of purchase.\",\n",
        "            \"Refunds: Full refunds are issued for unused items in original packaging.\",\n",
        "            \"Shipping: Free shipping on orders over $50.\"\n",
        "        ]\n",
        "    ),\n",
        "    EvalTestCase(\n",
        "        input=\"How do I contact support?\",\n",
        "        actual_output=\"You can reach our support team 24/7 via email at support@company.com or call 1-800-SUPPORT.\",\n",
        "        retrieval_context=[\n",
        "            \"Contact: Email us at help@company.com\",\n",
        "            \"Hours: Monday-Friday 9am-5pm\",\n",
        "        ]\n",
        "    ),\n",
        "]\n",
        "\n",
        "for i, tc in enumerate(test_cases):\n",
        "    print(f\"\\n  Test Case {i+1}:\")\n",
        "    print(f\"    Question: {tc.input}\")\n",
        "    print(f\"    Response: {tc.actual_output[:60]}...\")\n",
        "    print(f\"    Context chunks: {len(tc.retrieval_context)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "DEEPEVAL: Running Evaluations\n",
            "=================================================================\n",
            "\n",
            "[Evaluation Results]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "  Test Case 1: What is the return policy?\n",
            "    Faithfulness:  1.00 [✓ PASS]\n",
            "      Reason: Response matches context exactly...\n",
            "    Relevancy:     1.00 [✓ PASS]\n",
            "      Reason: The response directly and completely answers the question by...\n",
            "\n",
            "  Test Case 2: How do I contact support?\n",
            "    Faithfulness:  0.00 [✗ FAIL]\n",
            "      Reason: Response claims 24/7 availability, email support@company.com...\n",
            "    Relevancy:     1.00 [✓ PASS]\n",
            "      Reason: Directly provides clear contact methods (email and phone) fo...\n",
            "\n",
            "═════════════════════════════════════════════════════════════════\n",
            "EVALUATION SUMMARY\n",
            "═════════════════════════════════════════════════════════════════\n",
            "\n",
            "  Passed: 3/4 (75%)\n",
            "  Threshold: 0.7\n",
            "\n",
            "  Average Scores:\n",
            "    faithfulness: 0.50\n",
            "    answer_relevancy: 1.00\n"
          ]
        }
      ],
      "source": [
        "# Demo 4c: Run Evaluations\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"DEEPEVAL: Running Evaluations\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "evaluator = LocalEvaluator(model=EVAL_MODEL, threshold=0.7)\n",
        "\n",
        "print(\"\\n[Evaluation Results]\")\n",
        "print(\"─\"*65)\n",
        "\n",
        "all_results = []\n",
        "for i, tc in enumerate(test_cases):\n",
        "    print(f\"\\n  Test Case {i+1}: {tc.input}\")\n",
        "    \n",
        "    # Faithfulness\n",
        "    faith_result = evaluator.evaluate_faithfulness(tc)\n",
        "    all_results.append(faith_result)\n",
        "    status = \"✓ PASS\" if faith_result.passed else \"✗ FAIL\"\n",
        "    print(f\"    Faithfulness:  {faith_result.score:.2f} [{status}]\")\n",
        "    print(f\"      Reason: {faith_result.reason[:60]}...\")\n",
        "    \n",
        "    # Relevancy\n",
        "    rel_result = evaluator.evaluate_relevancy(tc)\n",
        "    all_results.append(rel_result)\n",
        "    status = \"✓ PASS\" if rel_result.passed else \"✗ FAIL\"\n",
        "    print(f\"    Relevancy:     {rel_result.score:.2f} [{status}]\")\n",
        "    print(f\"      Reason: {rel_result.reason[:60]}...\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"═\"*65)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"═\"*65)\n",
        "passed = sum(1 for r in all_results if r.passed)\n",
        "total = len(all_results)\n",
        "print(f\"\\n  Passed: {passed}/{total} ({passed/total*100:.0f}%)\")\n",
        "print(f\"  Threshold: {evaluator.threshold}\")\n",
        "\n",
        "# Aggregate by metric\n",
        "from collections import defaultdict\n",
        "by_metric = defaultdict(list)\n",
        "for r in all_results:\n",
        "    by_metric[r.metric_name].append(r.score)\n",
        "\n",
        "print(\"\\n  Average Scores:\")\n",
        "for metric, scores in by_metric.items():\n",
        "    avg = sum(scores) / len(scores)\n",
        "    print(f\"    {metric}: {avg:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Demo 5: RAGAS - RAG-Specific Evaluation\n",
        "\n",
        "RAGAS (RAG Assessment) is designed specifically for evaluating RAG pipelines:\n",
        "- **Component-level** - Retriever and generator metrics\n",
        "- **End-to-end** - Full pipeline quality\n",
        "- **Reference-free** - Works without ground truth\n",
        "\n",
        "Core Metrics:\n",
        "| Metric | Measures | Range |\n",
        "|--------|----------|-------|\n",
        "| Context Precision | Are retrieved docs relevant to question? | 0-1 |\n",
        "| Context Recall | Did we retrieve all needed info? | 0-1 |\n",
        "| Faithfulness | Is answer grounded in context? | 0-1 |\n",
        "| Answer Relevancy | Does answer address the question? | 0-1 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "RAGAS: RAG Assessment Framework\n",
            "=================================================================\n",
            "\n",
            "✓ RAGAS installed\n",
            "\n",
            "RAGAS ARCHITECTURE:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "  RAG Pipeline Output\n",
            "         │\n",
            "         ▼\n",
            "  ┌─────────────────────────────────────────────────────┐\n",
            "  │                    RAGAS Evaluation                  │\n",
            "  ├─────────────────────────────────────────────────────┤\n",
            "  │                                                      │\n",
            "  │  RETRIEVER METRICS           GENERATOR METRICS       │\n",
            "  │  ─────────────────           ─────────────────       │\n",
            "  │  • Context Precision         • Faithfulness          │\n",
            "  │  • Context Recall            • Answer Relevancy      │\n",
            "  │  • Context Relevancy         • Answer Correctness    │\n",
            "  │                                                      │\n",
            "  │                 END-TO-END                           │\n",
            "  │                 ──────────                           │\n",
            "  │                 • Answer Similarity                  │\n",
            "  │                 • Answer Semantic Similarity         │\n",
            "  │                                                      │\n",
            "  └─────────────────────────────────────────────────────┘\n",
            "                         │\n",
            "                         ▼\n",
            "                  Quality Scores (0-1)\n",
            "\n",
            "RAGAS vs DEEPEVAL:\n",
            "─────────────────────────────────────────────────────────────────\n",
            "\n",
            "  ┌─────────────────┬────────────────────┬────────────────────┐\n",
            "  │ Feature         │ RAGAS              │ DeepEval           │\n",
            "  ├─────────────────┼────────────────────┼────────────────────┤\n",
            "  │ Focus           │ RAG-specific       │ General LLM        │\n",
            "  │ Metrics         │ RAG-optimized      │ Broader coverage   │\n",
            "  │ Integration     │ LangChain native   │ Pytest native      │\n",
            "  │ CI/CD           │ Manual             │ Built-in           │\n",
            "  │ Custom metrics  │ Limited            │ G-Eval support     │\n",
            "  └─────────────────┴────────────────────┴────────────────────┘\n",
            "\n",
            "WHEN TO USE RAGAS:\n",
            "  ✓ Evaluating RAG pipelines specifically\n",
            "  ✓ Need retrieval quality metrics\n",
            "  ✓ LangChain-based applications\n",
            "  ✓ Quick RAG quality assessment\n",
            "\n",
            "WHEN TO USE DEEPEVAL:\n",
            "  ✓ General LLM evaluation\n",
            "  ✓ CI/CD integration needed\n",
            "  ✓ Custom evaluation criteria\n",
            "  ✓ Non-RAG use cases\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Demo 5: RAGAS Overview\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"RAGAS: RAG Assessment Framework\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "ragas_available = False\n",
        "try:\n",
        "    import ragas\n",
        "    ragas_available = True\n",
        "    print(\"\\n✓ RAGAS installed\")\n",
        "except ImportError:\n",
        "    print(\"\\n⚠ RAGAS not installed\")\n",
        "    print(\"  pip install ragas\")\n",
        "\n",
        "print(\"\"\"\n",
        "RAGAS ARCHITECTURE:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "  RAG Pipeline Output\n",
        "         │\n",
        "         ▼\n",
        "  ┌─────────────────────────────────────────────────────┐\n",
        "  │                    RAGAS Evaluation                  │\n",
        "  ├─────────────────────────────────────────────────────┤\n",
        "  │                                                      │\n",
        "  │  RETRIEVER METRICS           GENERATOR METRICS       │\n",
        "  │  ─────────────────           ─────────────────       │\n",
        "  │  • Context Precision         • Faithfulness          │\n",
        "  │  • Context Recall            • Answer Relevancy      │\n",
        "  │  • Context Relevancy         • Answer Correctness    │\n",
        "  │                                                      │\n",
        "  │                 END-TO-END                           │\n",
        "  │                 ──────────                           │\n",
        "  │                 • Answer Similarity                  │\n",
        "  │                 • Answer Semantic Similarity         │\n",
        "  │                                                      │\n",
        "  └─────────────────────────────────────────────────────┘\n",
        "                         │\n",
        "                         ▼\n",
        "                  Quality Scores (0-1)\n",
        "\n",
        "RAGAS vs DEEPEVAL:\n",
        "─────────────────────────────────────────────────────────────────\n",
        "\n",
        "  ┌─────────────────┬────────────────────┬────────────────────┐\n",
        "  │ Feature         │ RAGAS              │ DeepEval           │\n",
        "  ├─────────────────┼────────────────────┼────────────────────┤\n",
        "  │ Focus           │ RAG-specific       │ General LLM        │\n",
        "  │ Metrics         │ RAG-optimized      │ Broader coverage   │\n",
        "  │ Integration     │ LangChain native   │ Pytest native      │\n",
        "  │ CI/CD           │ Manual             │ Built-in           │\n",
        "  │ Custom metrics  │ Limited            │ G-Eval support     │\n",
        "  └─────────────────┴────────────────────┴────────────────────┘\n",
        "\n",
        "WHEN TO USE RAGAS:\n",
        "  ✓ Evaluating RAG pipelines specifically\n",
        "  ✓ Need retrieval quality metrics\n",
        "  ✓ LangChain-based applications\n",
        "  ✓ Quick RAG quality assessment\n",
        "\n",
        "WHEN TO USE DEEPEVAL:\n",
        "  ✓ General LLM evaluation\n",
        "  ✓ CI/CD integration needed\n",
        "  ✓ Custom evaluation criteria\n",
        "  ✓ Non-RAG use cases\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "RAGAS: Local Metric Simulation\n",
            "=================================================================\n",
            "\n",
            "[RAGAS Evaluation Demo]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "  Question: What programming languages does the company use?\n",
            "  Answer: The company primarily uses Python for backend development and TypeScri...\n",
            "  Contexts: 4 documents\n",
            "\n",
            "  Running RAGAS metrics...\n",
            "\n",
            "  Results:\n",
            "    context_precision   : 0.50 [██████████░░░░░░░░░░]\n",
            "    faithfulness        : 1.00 [████████████████████]\n",
            "    answer_relevancy    : 1.00 [████████████████████]\n"
          ]
        }
      ],
      "source": [
        "# Demo 5b: RAGAS Metric Simulation with Local LLM\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"RAGAS: Local Metric Simulation\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "class LocalRAGASEvaluator:\n",
        "    \"\"\"Simulates RAGAS metrics using local Ollama models.\"\"\"\n",
        "    \n",
        "    def __init__(self, model: str = EVAL_MODEL):\n",
        "        self.model = model\n",
        "    \n",
        "    def context_precision(self, question: str, contexts: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Context Precision: What fraction of retrieved contexts are relevant?\n",
        "        Measures retriever precision.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Evaluate if each context is relevant to answering the question.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "CONTEXTS:\n",
        "{chr(10).join(f'{i+1}. {c}' for i, c in enumerate(contexts))}\n",
        "\n",
        "For each context, respond with 1 if relevant, 0 if not.\n",
        "Output ONLY a JSON: {{\"relevance\": [1, 0, 1, ...]}}\"\"\"\n",
        "        \n",
        "        if ollama_ready:\n",
        "            raw, _ = ollama_generate(prompt, self.model, temperature=0.1)\n",
        "            raw = clean_response(raw)\n",
        "            try:\n",
        "                match = re.search(r'\\{[^}]+\\}', raw)\n",
        "                if match:\n",
        "                    result = json.loads(match.group())\n",
        "                    relevance = result.get(\"relevance\", [])\n",
        "                    if relevance:\n",
        "                        return sum(relevance) / len(relevance)\n",
        "            except:\n",
        "                pass\n",
        "        return 0.7  # Default\n",
        "    \n",
        "    def faithfulness(self, answer: str, contexts: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Faithfulness: Is the answer grounded in the contexts?\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Check if the answer is supported by the contexts.\n",
        "\n",
        "CONTEXTS:\n",
        "{chr(10).join(contexts)}\n",
        "\n",
        "ANSWER: {answer}\n",
        "\n",
        "Rate from 0.0 (no support) to 1.0 (fully supported).\n",
        "Output ONLY: {{\"score\": <number>}}\"\"\"\n",
        "        \n",
        "        if ollama_ready:\n",
        "            raw, _ = ollama_generate(prompt, self.model, temperature=0.1)\n",
        "            raw = clean_response(raw)\n",
        "            try:\n",
        "                match = re.search(r'\\{[^}]+\\}', raw)\n",
        "                if match:\n",
        "                    return float(json.loads(match.group()).get(\"score\", 0.5))\n",
        "            except:\n",
        "                pass\n",
        "        return 0.8\n",
        "    \n",
        "    def answer_relevancy(self, question: str, answer: str) -> float:\n",
        "        \"\"\"\n",
        "        Answer Relevancy: Does the answer address the question?\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Does this answer address the question?\n",
        "\n",
        "QUESTION: {question}\n",
        "ANSWER: {answer}\n",
        "\n",
        "Rate from 0.0 (irrelevant) to 1.0 (perfectly relevant).\n",
        "Output ONLY: {{\"score\": <number>}}\"\"\"\n",
        "        \n",
        "        if ollama_ready:\n",
        "            raw, _ = ollama_generate(prompt, self.model, temperature=0.1)\n",
        "            raw = clean_response(raw)\n",
        "            try:\n",
        "                match = re.search(r'\\{[^}]+\\}', raw)\n",
        "                if match:\n",
        "                    return float(json.loads(match.group()).get(\"score\", 0.5))\n",
        "            except:\n",
        "                pass\n",
        "        return 0.85\n",
        "    \n",
        "    def evaluate_rag(self, question: str, answer: str, contexts: List[str]) -> Dict[str, float]:\n",
        "        \"\"\"Run all RAGAS metrics on a single sample.\"\"\"\n",
        "        return {\n",
        "            \"context_precision\": self.context_precision(question, contexts),\n",
        "            \"faithfulness\": self.faithfulness(answer, contexts),\n",
        "            \"answer_relevancy\": self.answer_relevancy(question, answer)\n",
        "        }\n",
        "\n",
        "# Demo\n",
        "print(\"\\n[RAGAS Evaluation Demo]\")\n",
        "print(\"─\"*65)\n",
        "\n",
        "ragas_eval = LocalRAGASEvaluator()\n",
        "\n",
        "# Sample RAG output\n",
        "sample = {\n",
        "    \"question\": \"What programming languages does the company use?\",\n",
        "    \"answer\": \"The company primarily uses Python for backend development and TypeScript for frontend. They also use Go for microservices.\",\n",
        "    \"contexts\": [\n",
        "        \"Tech Stack: Backend is built with Python (FastAPI), frontend uses React with TypeScript.\",\n",
        "        \"Our microservices are written in Go for performance-critical components.\",\n",
        "        \"The database layer uses PostgreSQL with Redis for caching.\",\n",
        "        \"Team uses Git for version control and GitHub Actions for CI/CD.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "print(f\"  Question: {sample['question']}\")\n",
        "print(f\"  Answer: {sample['answer'][:70]}...\")\n",
        "print(f\"  Contexts: {len(sample['contexts'])} documents\")\n",
        "\n",
        "print(\"\\n  Running RAGAS metrics...\")\n",
        "scores = ragas_eval.evaluate_rag(\n",
        "    sample[\"question\"],\n",
        "    sample[\"answer\"],\n",
        "    sample[\"contexts\"]\n",
        ")\n",
        "\n",
        "print(\"\\n  Results:\")\n",
        "for metric, score in scores.items():\n",
        "    bar = \"█\" * int(score * 20) + \"░\" * (20 - int(score * 20))\n",
        "    print(f\"    {metric:20}: {score:.2f} [{bar}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Demo 6: LLM-as-Judge Patterns\n",
        "\n",
        "LLM-as-Judge achieves ~80% agreement with human evaluators. Key patterns:\n",
        "\n",
        "| Pattern | Use Case | Cost |\n",
        "|---------|----------|------|\n",
        "| G-Eval | Custom criteria | Medium |\n",
        "| Pairwise | A/B comparison | Higher |\n",
        "| Reference-based | Ground truth available | Medium |\n",
        "| Reference-free | No ground truth | Lower |\n",
        "\n",
        "**Best Practices:**\n",
        "- Use GPT-3.5 + examples instead of GPT-4 (10× cheaper, similar accuracy)\n",
        "- Binary/low-precision scales (0-3) work as well as 0-100\n",
        "- Sample 5-10% of production traffic for ongoing evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "LLM-AS-JUDGE: Evaluation Patterns\n",
            "=================================================================\n",
            "\n",
            "[1. G-Eval: Custom Criteria]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "  Score: 5/5 (100%)\n",
            "  Reasoning: Acknowledges customer's frustration, provides specific delivery timeline (shippe...\n"
          ]
        }
      ],
      "source": [
        "# Demo 6: LLM-as-Judge Patterns\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"LLM-AS-JUDGE: Evaluation Patterns\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "class LLMJudge:\n",
        "    \"\"\"Implements various LLM-as-Judge patterns using local Ollama.\"\"\"\n",
        "    \n",
        "    def __init__(self, model: str = EVAL_MODEL):\n",
        "        self.model = model\n",
        "    \n",
        "    def geval(self, response: str, criteria: str, evaluation_steps: List[str]) -> Dict:\n",
        "        \"\"\"\n",
        "        G-Eval: Custom evaluation with explicit criteria.\n",
        "        From: Liu et al., \"G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment\"\n",
        "        \"\"\"\n",
        "        steps_text = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(evaluation_steps))\n",
        "        \n",
        "        prompt = f\"\"\"You are an evaluation judge. Evaluate the response based on the given criteria.\n",
        "\n",
        "CRITERIA: {criteria}\n",
        "\n",
        "EVALUATION STEPS:\n",
        "{steps_text}\n",
        "\n",
        "RESPONSE TO EVALUATE:\n",
        "{response}\n",
        "\n",
        "Based on the criteria and steps, rate from 1 (poor) to 5 (excellent).\n",
        "Output ONLY: {{\"score\": <1-5>, \"reasoning\": \"<brief explanation>\"}}\"\"\"\n",
        "        \n",
        "        if ollama_ready:\n",
        "            raw, _ = ollama_generate(prompt, self.model, temperature=0.1)\n",
        "            raw = clean_response(raw)\n",
        "            try:\n",
        "                match = re.search(r'\\{[^}]+\\}', raw)\n",
        "                if match:\n",
        "                    result = json.loads(match.group())\n",
        "                    return {\n",
        "                        \"score\": result.get(\"score\", 3),\n",
        "                        \"normalized_score\": result.get(\"score\", 3) / 5,\n",
        "                        \"reasoning\": result.get(\"reasoning\", \"\")\n",
        "                    }\n",
        "            except:\n",
        "                pass\n",
        "        return {\"score\": 3, \"normalized_score\": 0.6, \"reasoning\": \"Default\"}\n",
        "    \n",
        "    def pairwise(self, query: str, response_a: str, response_b: str, criteria: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Pairwise comparison: Which response is better?\n",
        "        More reliable than absolute scoring.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Compare two responses and decide which is better.\n",
        "\n",
        "QUERY: {query}\n",
        "\n",
        "CRITERIA: {criteria}\n",
        "\n",
        "RESPONSE A:\n",
        "{response_a}\n",
        "\n",
        "RESPONSE B:\n",
        "{response_b}\n",
        "\n",
        "Which response is better based on the criteria?\n",
        "Output ONLY: {{\"winner\": \"A\" or \"B\", \"reasoning\": \"<brief explanation>\"}}\"\"\"\n",
        "        \n",
        "        if ollama_ready:\n",
        "            raw, _ = ollama_generate(prompt, self.model, temperature=0.1)\n",
        "            raw = clean_response(raw)\n",
        "            try:\n",
        "                match = re.search(r'\\{[^}]+\\}', raw)\n",
        "                if match:\n",
        "                    result = json.loads(match.group())\n",
        "                    return {\n",
        "                        \"winner\": result.get(\"winner\", \"A\"),\n",
        "                        \"reasoning\": result.get(\"reasoning\", \"\")\n",
        "                    }\n",
        "            except:\n",
        "                pass\n",
        "        return {\"winner\": \"A\", \"reasoning\": \"Default\"}\n",
        "    \n",
        "    def reference_based(self, response: str, reference: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Reference-based: Compare response to ground truth.\n",
        "        \"\"\"\n",
        "        prompt = f\"\"\"Compare the response to the reference answer.\n",
        "\n",
        "REFERENCE (Ground Truth):\n",
        "{reference}\n",
        "\n",
        "RESPONSE TO EVALUATE:\n",
        "{response}\n",
        "\n",
        "How well does the response match the reference in terms of correctness and completeness?\n",
        "Rate from 0.0 (completely wrong) to 1.0 (matches perfectly).\n",
        "Output ONLY: {{\"score\": <number>, \"issues\": \"<any issues found>\"}}\"\"\"\n",
        "        \n",
        "        if ollama_ready:\n",
        "            raw, _ = ollama_generate(prompt, self.model, temperature=0.1)\n",
        "            raw = clean_response(raw)\n",
        "            try:\n",
        "                match = re.search(r'\\{[^}]+\\}', raw)\n",
        "                if match:\n",
        "                    result = json.loads(match.group())\n",
        "                    return {\n",
        "                        \"score\": float(result.get(\"score\", 0.5)),\n",
        "                        \"issues\": result.get(\"issues\", \"\")\n",
        "                    }\n",
        "            except:\n",
        "                pass\n",
        "        return {\"score\": 0.75, \"issues\": \"Default\"}\n",
        "\n",
        "# Demo the patterns\n",
        "print(\"\\n[1. G-Eval: Custom Criteria]\")\n",
        "print(\"─\"*65)\n",
        "\n",
        "judge = LLMJudge()\n",
        "\n",
        "geval_result = judge.geval(\n",
        "    response=\"Thank you for contacting us! I understand your frustration with the delayed order. I've checked our system and see that your package was shipped yesterday. You should receive it within 2-3 business days. Is there anything else I can help with?\",\n",
        "    criteria=\"Professional and empathetic customer service response\",\n",
        "    evaluation_steps=[\n",
        "        \"Check if the response acknowledges the customer's concern\",\n",
        "        \"Verify the response provides a concrete solution or next steps\",\n",
        "        \"Assess the tone for professionalism and empathy\",\n",
        "        \"Check for unnecessary jargon or overly formal language\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"  Score: {geval_result['score']}/5 ({geval_result['normalized_score']:.0%})\")\n",
        "print(f\"  Reasoning: {geval_result['reasoning'][:80]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "LLM-AS-JUDGE: Pairwise and Reference-Based\n",
            "=================================================================\n",
            "\n",
            "[2. Pairwise Comparison]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "  Query: How do I reset my password?\n",
            "  Response A: Go to settings and click reset password.\n",
            "  Response B: To reset your password: 1) Click your profile icon...\n",
            "\n",
            "  Winner: Response B\n",
            "  Reasoning: Response B provides a detailed, step-by-step guide with specific actions and cri...\n",
            "\n",
            "[3. Reference-Based Evaluation]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "  Reference: Python was created by Guido van Rossum and first released in 1991...\n",
            "  Response: Python was created by Guido van Rossum in 1991...\n",
            "\n",
            "  Score: 0.70\n",
            "  Issues: Missing 'interpreted' and the known characteristics (readability and versatility...\n",
            "\n",
            "═════════════════════════════════════════════════════════════════\n",
            "LLM-AS-JUDGE BEST PRACTICES\n",
            "═════════════════════════════════════════════════════════════════\n",
            "\n",
            "  1. COST OPTIMIZATION:\n",
            "     • Use smaller models (GPT-3.5, local Llama) with few-shot examples\n",
            "     • Binary/low-precision scales reduce token cost\n",
            "     • Cache judge responses for similar inputs\n",
            "\n",
            "  2. RELIABILITY:\n",
            "     • Pairwise comparison > absolute scoring\n",
            "     • Use chain-of-thought for complex criteria\n",
            "     • Sample multiple judgments and average\n",
            "\n",
            "  3. PRODUCTION INTEGRATION:\n",
            "     • Sample 5-10% of traffic for evaluation\n",
            "     • Track judge agreement over time\n",
            "     • Calibrate against human labels periodically\n",
            "\n",
            "  4. COMMON METRICS:\n",
            "     • Helpfulness (1-5)\n",
            "     • Harmlessness (pass/fail)\n",
            "     • Honesty (0-1)\n",
            "     • Task completion (pass/fail)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Demo 6b: More LLM-as-Judge Patterns\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"LLM-AS-JUDGE: Pairwise and Reference-Based\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "# Pairwise comparison\n",
        "print(\"\\n[2. Pairwise Comparison]\")\n",
        "print(\"─\"*65)\n",
        "\n",
        "pairwise_result = judge.pairwise(\n",
        "    query=\"How do I reset my password?\",\n",
        "    response_a=\"Go to settings and click reset password.\",\n",
        "    response_b=\"To reset your password: 1) Click your profile icon, 2) Select 'Settings', 3) Click 'Security', 4) Click 'Reset Password', 5) Check your email for the reset link. The link expires in 24 hours.\",\n",
        "    criteria=\"Completeness and helpfulness\"\n",
        ")\n",
        "\n",
        "print(f\"  Query: How do I reset my password?\")\n",
        "print(f\"  Response A: Go to settings and click reset password.\")\n",
        "print(f\"  Response B: To reset your password: 1) Click your profile icon...\")\n",
        "print(f\"\\n  Winner: Response {pairwise_result['winner']}\")\n",
        "print(f\"  Reasoning: {pairwise_result['reasoning'][:80]}...\")\n",
        "\n",
        "# Reference-based comparison\n",
        "print(\"\\n[3. Reference-Based Evaluation]\")\n",
        "print(\"─\"*65)\n",
        "\n",
        "ref_result = judge.reference_based(\n",
        "    response=\"Python was created by Guido van Rossum in 1991. It's a high-level programming language.\",\n",
        "    reference=\"Python was created by Guido van Rossum and first released in 1991. It is a high-level, interpreted programming language known for its readability and versatility.\"\n",
        ")\n",
        "\n",
        "print(f\"  Reference: Python was created by Guido van Rossum and first released in 1991...\")\n",
        "print(f\"  Response: Python was created by Guido van Rossum in 1991...\")\n",
        "print(f\"\\n  Score: {ref_result['score']:.2f}\")\n",
        "print(f\"  Issues: {ref_result['issues'][:80]}...\" if ref_result['issues'] else \"  Issues: None\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"═\"*65)\n",
        "print(\"LLM-AS-JUDGE BEST PRACTICES\")\n",
        "print(\"═\"*65)\n",
        "print(\"\"\"\n",
        "  1. COST OPTIMIZATION:\n",
        "     • Use smaller models (GPT-3.5, local Llama) with few-shot examples\n",
        "     • Binary/low-precision scales reduce token cost\n",
        "     • Cache judge responses for similar inputs\n",
        "  \n",
        "  2. RELIABILITY:\n",
        "     • Pairwise comparison > absolute scoring\n",
        "     • Use chain-of-thought for complex criteria\n",
        "     • Sample multiple judgments and average\n",
        "  \n",
        "  3. PRODUCTION INTEGRATION:\n",
        "     • Sample 5-10% of traffic for evaluation\n",
        "     • Track judge agreement over time\n",
        "     • Calibrate against human labels periodically\n",
        "  \n",
        "  4. COMMON METRICS:\n",
        "     • Helpfulness (1-5)\n",
        "     • Harmlessness (pass/fail)\n",
        "     • Honesty (0-1)\n",
        "     • Task completion (pass/fail)\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Demo 7: Production Failure Mode Checklist\n",
        "\n",
        "Common LLM failure modes and how to detect/mitigate them:\n",
        "\n",
        "| Failure Mode | Symptom | Detection | Fix |\n",
        "|-------------|---------|-----------|-----|\n",
        "| Prompt Drift | Quality degrades over time | Monitor quality scores | Pin model versions |\n",
        "| Context Overflow | Ignores important context | Track context length | Better chunking |\n",
        "| Cost Explosion | Bills higher than expected | Budget alerts | Token limits |\n",
        "| Hallucination Spike | Wrong answers | Faithfulness metric | Improve retrieval |\n",
        "| Latency Regression | Slow responses | P95 monitoring | Timeouts, caching |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "PRODUCTION: Failure Mode Checklist\n",
            "=================================================================\n",
            "\n",
            "LLM System Failure Mode Checklist\n",
            "==================================\n",
            "\n",
            "PRE-DEPLOYMENT:\n",
            "☐ Model validated on YOUR data (not just public benchmarks)\n",
            "☐ Structured output tested with edge cases\n",
            "☐ Guardrails configured and tested (jailbreak, PII, toxicity)\n",
            "☐ Hallucination baseline measured\n",
            "☐ Cost projections validated with realistic traffic estimates\n",
            "☐ Latency tested under load\n",
            "\n",
            "MONITORING (Day 1):\n",
            "☐ Observability deployed (traces, tokens, costs)\n",
            "☐ Alerts configured (error rate, latency P95, cost spikes)\n",
            "☐ Evaluation pipeline running (5% sample with LLM-as-judge)\n",
            "☐ User feedback collection enabled\n",
            "\n",
            "ONGOING:\n",
            "☐ Weekly: Review quality scores, cost trends\n",
            "☐ Monthly: Re-evaluate model selection (new models may be better/cheaper)\n",
            "☐ Quarterly: Refresh evaluation dataset with production examples\n",
            "☐ Ad-hoc: Investigate quality degradation signals\n",
            "\n",
            "═════════════════════════════════════════════════════════════════\n",
            "COMMON FAILURE MODES\n",
            "═════════════════════════════════════════════════════════════════\n",
            "\n",
            "PROMPT DRIFT\n",
            "────────────────────────────────────────\n",
            "  Symptom:   Quality degrades over time without code changes\n",
            "  Cause:     Model updates by provider, data distribution shift\n",
            "  Detection: Track quality scores weekly, compare to baseline\n",
            "  Fix:       Pin model versions, monitor quality metrics, A/B test updates\n",
            "\n",
            "CONTEXT OVERFLOW\n",
            "────────────────────────────────────────\n",
            "  Symptom:   Responses ignore important context\n",
            "  Cause:     Exceeded context window, 'lost in the middle' effect\n",
            "  Detection: Track avg context length, test with long inputs\n",
            "  Fix:       Better chunking, reranking, hierarchical summarization\n",
            "\n",
            "COST EXPLOSION\n",
            "────────────────────────────────────────\n",
            "  Symptom:   Bills much higher than projected\n",
            "  Cause:     Verbose prompts, chatty responses, missing caching\n",
            "  Detection: Daily cost monitoring, per-request cost tracking\n",
            "  Fix:       Audit token usage, implement output length limits, add routing\n",
            "\n",
            "HALLUCINATION SPIKE\n",
            "────────────────────────────────────────\n",
            "  Symptom:   Users report factually wrong answers\n",
            "  Cause:     Poor retrieval quality, model uncertainty\n",
            "  Detection: Faithfulness metric, user feedback analysis\n",
            "  Fix:       Improve retrieval, add confidence thresholds, HaluGate\n",
            "\n",
            "LATENCY REGRESSION\n",
            "────────────────────────────────────────\n",
            "  Symptom:   Response times increase\n",
            "  Cause:     Larger context, provider issues, cold starts\n",
            "  Detection: P95 latency monitoring, TTFT tracking\n",
            "  Fix:       Implement timeouts, add caching, optimize context\n",
            "\n",
            "GUARDRAIL BYPASS\n",
            "────────────────────────────────────────\n",
            "  Symptom:   Harmful/off-topic responses get through\n",
            "  Cause:     New attack patterns, incomplete rules\n",
            "  Detection: Safety metric sampling, user reports\n",
            "  Fix:       Red team regularly, update guardrails, add NeMo\n"
          ]
        }
      ],
      "source": [
        "# Demo 7: Production Failure Mode Checklist\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"PRODUCTION: Failure Mode Checklist\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "FAILURE_CHECKLIST = \"\"\"\n",
        "LLM System Failure Mode Checklist\n",
        "==================================\n",
        "\n",
        "PRE-DEPLOYMENT:\n",
        "☐ Model validated on YOUR data (not just public benchmarks)\n",
        "☐ Structured output tested with edge cases\n",
        "☐ Guardrails configured and tested (jailbreak, PII, toxicity)\n",
        "☐ Hallucination baseline measured\n",
        "☐ Cost projections validated with realistic traffic estimates\n",
        "☐ Latency tested under load\n",
        "\n",
        "MONITORING (Day 1):\n",
        "☐ Observability deployed (traces, tokens, costs)\n",
        "☐ Alerts configured (error rate, latency P95, cost spikes)\n",
        "☐ Evaluation pipeline running (5% sample with LLM-as-judge)\n",
        "☐ User feedback collection enabled\n",
        "\n",
        "ONGOING:\n",
        "☐ Weekly: Review quality scores, cost trends\n",
        "☐ Monthly: Re-evaluate model selection (new models may be better/cheaper)\n",
        "☐ Quarterly: Refresh evaluation dataset with production examples\n",
        "☐ Ad-hoc: Investigate quality degradation signals\n",
        "\"\"\"\n",
        "\n",
        "print(FAILURE_CHECKLIST)\n",
        "\n",
        "# Common failure modes with detection\n",
        "print(\"═\"*65)\n",
        "print(\"COMMON FAILURE MODES\")\n",
        "print(\"═\"*65)\n",
        "\n",
        "failure_modes = [\n",
        "    {\n",
        "        \"name\": \"PROMPT DRIFT\",\n",
        "        \"symptom\": \"Quality degrades over time without code changes\",\n",
        "        \"cause\": \"Model updates by provider, data distribution shift\",\n",
        "        \"detection\": \"Track quality scores weekly, compare to baseline\",\n",
        "        \"fix\": \"Pin model versions, monitor quality metrics, A/B test updates\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"CONTEXT OVERFLOW\",\n",
        "        \"symptom\": \"Responses ignore important context\",\n",
        "        \"cause\": \"Exceeded context window, 'lost in the middle' effect\",\n",
        "        \"detection\": \"Track avg context length, test with long inputs\",\n",
        "        \"fix\": \"Better chunking, reranking, hierarchical summarization\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"COST EXPLOSION\",\n",
        "        \"symptom\": \"Bills much higher than projected\",\n",
        "        \"cause\": \"Verbose prompts, chatty responses, missing caching\",\n",
        "        \"detection\": \"Daily cost monitoring, per-request cost tracking\",\n",
        "        \"fix\": \"Audit token usage, implement output length limits, add routing\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"HALLUCINATION SPIKE\",\n",
        "        \"symptom\": \"Users report factually wrong answers\",\n",
        "        \"cause\": \"Poor retrieval quality, model uncertainty\",\n",
        "        \"detection\": \"Faithfulness metric, user feedback analysis\",\n",
        "        \"fix\": \"Improve retrieval, add confidence thresholds, HaluGate\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"LATENCY REGRESSION\",\n",
        "        \"symptom\": \"Response times increase\",\n",
        "        \"cause\": \"Larger context, provider issues, cold starts\",\n",
        "        \"detection\": \"P95 latency monitoring, TTFT tracking\",\n",
        "        \"fix\": \"Implement timeouts, add caching, optimize context\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"GUARDRAIL BYPASS\",\n",
        "        \"symptom\": \"Harmful/off-topic responses get through\",\n",
        "        \"cause\": \"New attack patterns, incomplete rules\",\n",
        "        \"detection\": \"Safety metric sampling, user reports\",\n",
        "        \"fix\": \"Red team regularly, update guardrails, add NeMo\"\n",
        "    },\n",
        "]\n",
        "\n",
        "for fm in failure_modes:\n",
        "    print(f\"\\n{fm['name']}\")\n",
        "    print(\"─\"*40)\n",
        "    print(f\"  Symptom:   {fm['symptom']}\")\n",
        "    print(f\"  Cause:     {fm['cause']}\")\n",
        "    print(f\"  Detection: {fm['detection']}\")\n",
        "    print(f\"  Fix:       {fm['fix']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "PRODUCTION: Automated Failure Detection\n",
            "=================================================================\n",
            "\n",
            "[Baseline Metrics]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "  Latency P95:    400ms\n",
            "  Daily cost:     €50.00\n",
            "  Avg tokens in:  800\n",
            "  Faithfulness:   0.85\n",
            "\n",
            "[Current Metrics]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "  Latency P95:    650ms\n",
            "  Daily cost:     €72.00\n",
            "  Avg tokens in:  1100\n",
            "  Faithfulness:   0.82\n",
            "\n",
            "[Failure Detection Results]\n",
            "─────────────────────────────────────────────────────────────────\n",
            "  ⚠ LATENCY REGRESSION: P95 650ms vs baseline 400ms\n",
            "  ⚠ COST SPIKE: €72.00/day vs baseline €50.00/day\n",
            "\n",
            "═════════════════════════════════════════════════════════════════\n",
            "PRODUCTION OPERATIONS: COMPLETE SUMMARY\n",
            "═════════════════════════════════════════════════════════════════\n",
            "\n",
            "  OBSERVABILITY STACK:\n",
            "    • Phoenix - Self-hosted, RAG-focused, Apache 2.0\n",
            "    • Langfuse - Open source standard, MIT, EU cloud option\n",
            "    • LangSmith - Best for LangChain users\n",
            "\n",
            "  EVALUATION STACK:\n",
            "    • DeepEval - Pytest integration, CI/CD ready\n",
            "    • RAGAS - RAG-specific metrics\n",
            "    • LLM-as-Judge - Custom criteria, 80% human agreement\n",
            "\n",
            "  MONITORING CADENCE:\n",
            "    • Real-time: Latency, errors, costs\n",
            "    • Daily: Quality scores, cost trends\n",
            "    • Weekly: Evaluation pipeline review\n",
            "    • Monthly: Model re-evaluation\n",
            "\n",
            "  KEY METRICS:\n",
            "    • Faithfulness, Relevancy, Context Precision\n",
            "    • Latency (P50, P95, TTFT)\n",
            "    • Token costs, error rates\n",
            "    • User satisfaction (thumbs up/down)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Demo 7b: Automated Failure Detection System\n",
        "\n",
        "print(\"=\"*65)\n",
        "print(\"PRODUCTION: Automated Failure Detection\")\n",
        "print(\"=\"*65)\n",
        "\n",
        "@dataclass\n",
        "class HealthMetrics:\n",
        "    \"\"\"Production health metrics for LLM system.\"\"\"\n",
        "    latency_p50_ms: float\n",
        "    latency_p95_ms: float\n",
        "    error_rate: float\n",
        "    avg_input_tokens: int\n",
        "    avg_output_tokens: int\n",
        "    daily_cost: float\n",
        "    faithfulness_score: float\n",
        "    relevancy_score: float\n",
        "    user_satisfaction: float  # 0-1\n",
        "\n",
        "class FailureDetector:\n",
        "    \"\"\"Detects common LLM failure modes from metrics.\"\"\"\n",
        "    \n",
        "    def __init__(self, baseline: HealthMetrics):\n",
        "        self.baseline = baseline\n",
        "        self.alerts = []\n",
        "    \n",
        "    def check_latency_regression(self, current: HealthMetrics) -> Optional[str]:\n",
        "        if current.latency_p95_ms > self.baseline.latency_p95_ms * 1.5:\n",
        "            return f\"⚠ LATENCY REGRESSION: P95 {current.latency_p95_ms:.0f}ms vs baseline {self.baseline.latency_p95_ms:.0f}ms\"\n",
        "        return None\n",
        "    \n",
        "    def check_cost_explosion(self, current: HealthMetrics) -> Optional[str]:\n",
        "        if current.daily_cost > self.baseline.daily_cost * 1.3:\n",
        "            return f\"⚠ COST SPIKE: €{current.daily_cost:.2f}/day vs baseline €{self.baseline.daily_cost:.2f}/day\"\n",
        "        return None\n",
        "    \n",
        "    def check_quality_degradation(self, current: HealthMetrics) -> Optional[str]:\n",
        "        if current.faithfulness_score < self.baseline.faithfulness_score * 0.9:\n",
        "            return f\"⚠ QUALITY DROP: Faithfulness {current.faithfulness_score:.2f} vs baseline {self.baseline.faithfulness_score:.2f}\"\n",
        "        return None\n",
        "    \n",
        "    def check_context_bloat(self, current: HealthMetrics) -> Optional[str]:\n",
        "        if current.avg_input_tokens > self.baseline.avg_input_tokens * 1.4:\n",
        "            return f\"⚠ CONTEXT BLOAT: {current.avg_input_tokens} tokens vs baseline {self.baseline.avg_input_tokens}\"\n",
        "        return None\n",
        "    \n",
        "    def check_all(self, current: HealthMetrics) -> List[str]:\n",
        "        alerts = []\n",
        "        checks = [\n",
        "            self.check_latency_regression,\n",
        "            self.check_cost_explosion,\n",
        "            self.check_quality_degradation,\n",
        "            self.check_context_bloat,\n",
        "        ]\n",
        "        for check in checks:\n",
        "            result = check(current)\n",
        "            if result:\n",
        "                alerts.append(result)\n",
        "        return alerts\n",
        "\n",
        "# Simulate baseline and current metrics\n",
        "baseline = HealthMetrics(\n",
        "    latency_p50_ms=150,\n",
        "    latency_p95_ms=400,\n",
        "    error_rate=0.02,\n",
        "    avg_input_tokens=800,\n",
        "    avg_output_tokens=200,\n",
        "    daily_cost=50.0,\n",
        "    faithfulness_score=0.85,\n",
        "    relevancy_score=0.90,\n",
        "    user_satisfaction=0.78\n",
        ")\n",
        "\n",
        "current = HealthMetrics(\n",
        "    latency_p50_ms=220,      # Increased\n",
        "    latency_p95_ms=650,      # Regression!\n",
        "    error_rate=0.025,\n",
        "    avg_input_tokens=1100,   # Bloated\n",
        "    avg_output_tokens=250,\n",
        "    daily_cost=72.0,         # Spike!\n",
        "    faithfulness_score=0.82,\n",
        "    relevancy_score=0.88,\n",
        "    user_satisfaction=0.72\n",
        ")\n",
        "\n",
        "print(\"\\n[Baseline Metrics]\")\n",
        "print(\"─\"*65)\n",
        "print(f\"  Latency P95:    {baseline.latency_p95_ms:.0f}ms\")\n",
        "print(f\"  Daily cost:     €{baseline.daily_cost:.2f}\")\n",
        "print(f\"  Avg tokens in:  {baseline.avg_input_tokens}\")\n",
        "print(f\"  Faithfulness:   {baseline.faithfulness_score:.2f}\")\n",
        "\n",
        "print(\"\\n[Current Metrics]\")\n",
        "print(\"─\"*65)\n",
        "print(f\"  Latency P95:    {current.latency_p95_ms:.0f}ms\")\n",
        "print(f\"  Daily cost:     €{current.daily_cost:.2f}\")\n",
        "print(f\"  Avg tokens in:  {current.avg_input_tokens}\")\n",
        "print(f\"  Faithfulness:   {current.faithfulness_score:.2f}\")\n",
        "\n",
        "# Run detection\n",
        "detector = FailureDetector(baseline)\n",
        "alerts = detector.check_all(current)\n",
        "\n",
        "print(\"\\n[Failure Detection Results]\")\n",
        "print(\"─\"*65)\n",
        "if alerts:\n",
        "    for alert in alerts:\n",
        "        print(f\"  {alert}\")\n",
        "else:\n",
        "    print(\"  ✓ All systems nominal\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\n\" + \"═\"*65)\n",
        "print(\"PRODUCTION OPERATIONS: COMPLETE SUMMARY\")\n",
        "print(\"═\"*65)\n",
        "print(\"\"\"\n",
        "  OBSERVABILITY STACK:\n",
        "    • Phoenix - Self-hosted, RAG-focused, Apache 2.0\n",
        "    • Langfuse - Open source standard, MIT, EU cloud option\n",
        "    • LangSmith - Best for LangChain users\n",
        "    \n",
        "  EVALUATION STACK:\n",
        "    • DeepEval - Pytest integration, CI/CD ready\n",
        "    • RAGAS - RAG-specific metrics\n",
        "    • LLM-as-Judge - Custom criteria, 80% human agreement\n",
        "    \n",
        "  MONITORING CADENCE:\n",
        "    • Real-time: Latency, errors, costs\n",
        "    • Daily: Quality scores, cost trends\n",
        "    • Weekly: Evaluation pipeline review\n",
        "    • Monthly: Model re-evaluation\n",
        "    \n",
        "  KEY METRICS:\n",
        "    • Faithfulness, Relevancy, Context Precision\n",
        "    • Latency (P50, P95, TTFT)\n",
        "    • Token costs, error rates\n",
        "    • User satisfaction (thumbs up/down)\n",
        "\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "guardrails-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
